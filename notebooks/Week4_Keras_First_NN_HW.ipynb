{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, precision_recall_curve, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, K\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "data_path = ['data']\n",
    "file_path = os.sep.join(data_path+['pima-indians-diabetes.csv'])\n",
    "names = [\"times_pregnant\",\"glucose_concentration\", \"blood_pressure\", \"skin_foldness\", \"insulin\", \"bmi\", \"pedigree\",\n",
    "         \"age\", \"has_diabetes\"]\n",
    "\n",
    "diabetes_df = pd.read_csv(file_path, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_concentration</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_foldness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>115</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.944</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>75</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>3</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>105</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.107</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>84</td>\n",
       "      <td>41</td>\n",
       "      <td>210</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.395</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.317</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_concentration  blood_pressure  skin_foldness  \\\n",
       "396               3                     96              56             34   \n",
       "145               0                    102              75             23   \n",
       "527               3                    116              74             15   \n",
       "195               5                    158              84             41   \n",
       "720               4                     83              86             19   \n",
       "\n",
       "     insulin   bmi  pedigree  age  has_diabetes  \n",
       "396      115  24.7     0.944   39             0  \n",
       "145        0   0.0     0.572   21             0  \n",
       "527      105  26.3     0.107   24             0  \n",
       "195      210  39.4     0.395   29             1  \n",
       "720        0  29.3     0.317   34             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.776\n",
      "roc-auc is 0.829\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "Yes, it makes sense. For the first layer, we have (12x8) inputs and 12 outputs, which is 108 and for the final layer, we have (12 inputs + 1 output).\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 360us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.5098 - val_acc: 0.7396\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.5098 - val_acc: 0.7396\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.5098 - val_acc: 0.7396\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4425 - acc: 0.7795 - val_loss: 0.5098 - val_acc: 0.7396\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4423 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4423 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4422 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4422 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4422 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4422 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4421 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4421 - acc: 0.7778 - val_loss: 0.5097 - val_acc: 0.7396\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4420 - acc: 0.7795 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4420 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4419 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4419 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4419 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4418 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4418 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4418 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4417 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4417 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4416 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4416 - acc: 0.7778 - val_loss: 0.5096 - val_acc: 0.7396\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4416 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4415 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4415 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4415 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4414 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4414 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4414 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4413 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4413 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4413 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4413 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7448\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4412 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4412 - acc: 0.7778 - val_loss: 0.5095 - val_acc: 0.7396\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4412 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7396\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4411 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4411 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4411 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4410 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4410 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4409 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4409 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4409 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7448\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4408 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4408 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4408 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4408 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4407 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4407 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4407 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4406 - acc: 0.7778 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4406 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4406 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4405 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 48us/step - loss: 0.4405 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4405 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4404 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4404 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4404 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4403 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4403 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4403 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4403 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4402 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4402 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4401 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4401 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4401 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4400 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4400 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4400 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4400 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4399 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4399 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4398 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4398 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4398 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4398 - acc: 0.7760 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4397 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4397 - acc: 0.7778 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4397 - acc: 0.7760 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4396 - acc: 0.7760 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4396 - acc: 0.7760 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4396 - acc: 0.7760 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4395 - acc: 0.7778 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4395 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4395 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4395 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4394 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4394 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4394 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4393 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4393 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4393 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4392 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4392 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4392 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4392 - acc: 0.7743 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4391 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4391 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4391 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4390 - acc: 0.7743 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4390 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4390 - acc: 0.7778 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4390 - acc: 0.7760 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4389 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4389 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4389 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4388 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4388 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4388 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4387 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4387 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4387 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 63us/step - loss: 0.4387 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4386 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4386 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4386 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4386 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4385 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4385 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4384 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4384 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4384 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4383 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4383 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4383 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4383 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4382 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4382 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4382 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4382 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4381 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4381 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4381 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4380 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4380 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4380 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4379 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4379 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4379 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4378 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4378 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4378 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4378 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4377 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4377 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4377 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4376 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4376 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4376 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4376 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4375 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4375 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4374 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4374 - acc: 0.7743 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4374 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4374 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4374 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4373 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4373 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4372 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4372 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4372 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4372 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4371 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4371 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4371 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4370 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4370 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4370 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4370 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4369 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4369 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 0.4369 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4368 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4368 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4368 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4367 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4367 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4367 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4367 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4366 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4366 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4366 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4365 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4365 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.4365 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.4364 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.4364 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.4364 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4364 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4363 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4363 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63345414],\n",
       "       [0.6494737 ],\n",
       "       [0.313152  ],\n",
       "       [0.19808899],\n",
       "       [0.17491078],\n",
       "       [0.53056824],\n",
       "       [0.02268486],\n",
       "       [0.3208175 ],\n",
       "       [0.9394373 ],\n",
       "       [0.19931854]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.755\n",
      "roc-auc is 0.814\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.5097924421230952,\n",
       "  0.5097797413667043,\n",
       "  0.5097766915957133,\n",
       "  0.5097631712754568,\n",
       "  0.5097470084826151,\n",
       "  0.5097430497407913,\n",
       "  0.5097368111213049,\n",
       "  0.5097194065650305,\n",
       "  0.5097020516792933,\n",
       "  0.5096957137187322,\n",
       "  0.5096797943115234,\n",
       "  0.509665921330452,\n",
       "  0.5096505582332611,\n",
       "  0.50965682665507,\n",
       "  0.509653573234876,\n",
       "  0.5096373359362284,\n",
       "  0.5096470564603806,\n",
       "  0.5096306751171747,\n",
       "  0.5096166481574377,\n",
       "  0.5096015681823095,\n",
       "  0.5095974653959274,\n",
       "  0.5095922499895096,\n",
       "  0.509577805797259,\n",
       "  0.5095735937356949,\n",
       "  0.5095824748277664,\n",
       "  0.5095666994651159,\n",
       "  0.5095517784357071,\n",
       "  0.5095391819874445,\n",
       "  0.5095286071300507,\n",
       "  0.5095242808262507,\n",
       "  0.5095097968975703,\n",
       "  0.5095081875721613,\n",
       "  0.5094939668973287,\n",
       "  0.5094816287358602,\n",
       "  0.509472593665123,\n",
       "  0.5094725439945856,\n",
       "  0.5094625353813171,\n",
       "  0.5094507137934366,\n",
       "  0.5094649891058604,\n",
       "  0.5094535996516546,\n",
       "  0.5094431787729263,\n",
       "  0.5094332297643026,\n",
       "  0.5094228486220042,\n",
       "  0.509413460890452,\n",
       "  0.5094064821799597,\n",
       "  0.5093972037235895,\n",
       "  0.5094026128451029,\n",
       "  0.5093936969836553,\n",
       "  0.5093977997700373,\n",
       "  0.5093897382418314,\n",
       "  0.5093831866979599,\n",
       "  0.50937087337176,\n",
       "  0.5093634178241094,\n",
       "  0.5093544473250707,\n",
       "  0.5093564291795095,\n",
       "  0.5093478212753931,\n",
       "  0.5093535532553991,\n",
       "  0.509344552954038,\n",
       "  0.5093366702397665,\n",
       "  0.5093287924925486,\n",
       "  0.5093202690283457,\n",
       "  0.50931383172671,\n",
       "  0.5093038181463877,\n",
       "  0.5093047171831131,\n",
       "  0.5093064705530802,\n",
       "  0.5092930992444357,\n",
       "  0.5092830012241999,\n",
       "  0.5092697491248449,\n",
       "  0.5092555036147436,\n",
       "  0.5092467864354452,\n",
       "  0.5092376669247946,\n",
       "  0.5092248866955439,\n",
       "  0.5092252343893051,\n",
       "  0.5092379699150721,\n",
       "  0.5092267791430155,\n",
       "  0.509219745794932,\n",
       "  0.5092240869998932,\n",
       "  0.5092130998770396,\n",
       "  0.5092043379942576,\n",
       "  0.5092052022616068,\n",
       "  0.5091999669869741,\n",
       "  0.5091926256815592,\n",
       "  0.5091820955276489,\n",
       "  0.5091741184393564,\n",
       "  0.5091663151979446,\n",
       "  0.5091727326313654,\n",
       "  0.5091674278179804,\n",
       "  0.5091718683640162,\n",
       "  0.5091633001963297,\n",
       "  0.5091563314199448,\n",
       "  0.5091490546862284,\n",
       "  0.5091434319814047,\n",
       "  0.5091370592514673,\n",
       "  0.5091302245855331,\n",
       "  0.5091238518555959,\n",
       "  0.509117916226387,\n",
       "  0.5091134111086527,\n",
       "  0.509107326467832,\n",
       "  0.5091025481621424,\n",
       "  0.509098415573438,\n",
       "  0.5090960562229156,\n",
       "  0.5090919733047485,\n",
       "  0.5090860823790232,\n",
       "  0.5090804000695547,\n",
       "  0.509078249335289,\n",
       "  0.5090751250584921,\n",
       "  0.5090703417857488,\n",
       "  0.5090669840574265,\n",
       "  0.5090661148230234,\n",
       "  0.509058266878128,\n",
       "  0.5090515414873759,\n",
       "  0.5090482731660207,\n",
       "  0.5090427994728088,\n",
       "  0.5090370227893194,\n",
       "  0.50904381275177,\n",
       "  0.5090401122967402,\n",
       "  0.5090355078379313,\n",
       "  0.5090310275554657,\n",
       "  0.5090291202068329,\n",
       "  0.5090247044960657,\n",
       "  0.5090194493532181,\n",
       "  0.5090148746967316,\n",
       "  0.5090098430713018,\n",
       "  0.5090043693780899,\n",
       "  0.509014422694842,\n",
       "  0.5090117355187734,\n",
       "  0.5090078413486481,\n",
       "  0.5090068429708481,\n",
       "  0.5090025713046392,\n",
       "  0.5089987317721049,\n",
       "  0.508998860915502,\n",
       "  0.5089957664410273,\n",
       "  0.508991539478302,\n",
       "  0.5089884599049886,\n",
       "  0.5089859614769617,\n",
       "  0.508982057372729,\n",
       "  0.508977010846138,\n",
       "  0.5089738517999649,\n",
       "  0.5089802841345469,\n",
       "  0.5089781184991201,\n",
       "  0.5089768668015798,\n",
       "  0.5089776416619619,\n",
       "  0.5089732656876246,\n",
       "  0.5089711397886276,\n",
       "  0.5089718600114187,\n",
       "  0.5089706480503082,\n",
       "  0.5089670370022455,\n",
       "  0.5089650203784307,\n",
       "  0.5089637736479441,\n",
       "  0.5089610715707144,\n",
       "  0.5089590499798456,\n",
       "  0.5089552253484726,\n",
       "  0.5089642405509949,\n",
       "  0.5089611013730367,\n",
       "  0.5089623679717382,\n",
       "  0.508962685863177,\n",
       "  0.5089635302623113,\n",
       "  0.5089604357878367,\n",
       "  0.508959506948789,\n",
       "  0.5089569489161173,\n",
       "  0.5089551905790964,\n",
       "  0.5089514454205831,\n",
       "  0.5089485297600428,\n",
       "  0.5089431504408518,\n",
       "  0.508938287695249,\n",
       "  0.508935754497846,\n",
       "  0.5089326898256937,\n",
       "  0.5089295655488968,\n",
       "  0.5089294016361237,\n",
       "  0.5089258849620819,\n",
       "  0.5089219808578491,\n",
       "  0.5089187572399775,\n",
       "  0.5089122653007507,\n",
       "  0.5089072485764822,\n",
       "  0.5089031358559927,\n",
       "  0.5089124093453089,\n",
       "  0.5089106559753418,\n",
       "  0.5089070945978165,\n",
       "  0.5089078744252523,\n",
       "  0.5089079588651657,\n",
       "  0.5089062800010046,\n",
       "  0.5089057187239329,\n",
       "  0.5089046905438105,\n",
       "  0.5089011987050375,\n",
       "  0.5088979303836823,\n",
       "  0.5088967879613241,\n",
       "  0.5088973790407181,\n",
       "  0.508895143866539,\n",
       "  0.5088891983032227,\n",
       "  0.5088978956143061,\n",
       "  0.5088969171047211,\n",
       "  0.5088921040296555,\n",
       "  0.5088860342899958,\n",
       "  0.5088794976472855,\n",
       "  0.5088758567969004,\n",
       "  0.5088726431131363,\n",
       "  0.5088674823443095,\n",
       "  0.5088662852843603,\n",
       "  0.5088621377944946,\n",
       "  0.5088589638471603],\n",
       " 'val_acc': [0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7447916666666666,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7395833333333334,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.7447916666666666,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334,\n",
       "  0.7552083333333334],\n",
       " 'loss': [0.4426186763577991,\n",
       "  0.4425550368097093,\n",
       "  0.4425552437702815,\n",
       "  0.4425152490536372,\n",
       "  0.44244873689280617,\n",
       "  0.4424010366201401,\n",
       "  0.4423684659931395,\n",
       "  0.4423466804954741,\n",
       "  0.44228555262088776,\n",
       "  0.4422495745950275,\n",
       "  0.4422323952118556,\n",
       "  0.4421706414884991,\n",
       "  0.442154956360658,\n",
       "  0.4421145286824968,\n",
       "  0.44207005202770233,\n",
       "  0.4420364747444789,\n",
       "  0.44198297295305466,\n",
       "  0.4419349912140105,\n",
       "  0.4419185866912206,\n",
       "  0.44186130662759143,\n",
       "  0.44184038539727527,\n",
       "  0.4418111526303821,\n",
       "  0.4417654739485847,\n",
       "  0.4417475395732456,\n",
       "  0.4416681230068207,\n",
       "  0.441641192469332,\n",
       "  0.44160955647627514,\n",
       "  0.4415942695405748,\n",
       "  0.44154955115583205,\n",
       "  0.44152962499194676,\n",
       "  0.4414842592345344,\n",
       "  0.44144050114684635,\n",
       "  0.4414026223950916,\n",
       "  0.4413570496771071,\n",
       "  0.4413381732172436,\n",
       "  0.4412986073229048,\n",
       "  0.4412689357995987,\n",
       "  0.4412604951196247,\n",
       "  0.44122172395388287,\n",
       "  0.44116453660859,\n",
       "  0.4411509980758031,\n",
       "  0.44110263387362164,\n",
       "  0.4410772836870617,\n",
       "  0.4410693844159444,\n",
       "  0.4409918900993135,\n",
       "  0.4409832689497206,\n",
       "  0.4409391830364863,\n",
       "  0.44090225133630967,\n",
       "  0.4408767173687617,\n",
       "  0.4408366315894657,\n",
       "  0.4408213198184967,\n",
       "  0.4408007197909885,\n",
       "  0.44077379835976493,\n",
       "  0.44072583317756653,\n",
       "  0.4406956848171022,\n",
       "  0.44070542024241555,\n",
       "  0.4406336545944214,\n",
       "  0.4406098822752635,\n",
       "  0.44059595631228554,\n",
       "  0.44051554136806065,\n",
       "  0.44050010873211753,\n",
       "  0.44048426383071476,\n",
       "  0.4404433113005426,\n",
       "  0.4404134336445067,\n",
       "  0.44038038949171704,\n",
       "  0.44033586730559665,\n",
       "  0.4403082678715388,\n",
       "  0.440289502342542,\n",
       "  0.4402576817406548,\n",
       "  0.44021285242504543,\n",
       "  0.4401833050780826,\n",
       "  0.4401422010527717,\n",
       "  0.4401206887430615,\n",
       "  0.4401022543509801,\n",
       "  0.44002341230710346,\n",
       "  0.4400065028005176,\n",
       "  0.4399808628691567,\n",
       "  0.4399714370568593,\n",
       "  0.43992390400833553,\n",
       "  0.4399186356200112,\n",
       "  0.4398460040489833,\n",
       "  0.43981507420539856,\n",
       "  0.43979236152436996,\n",
       "  0.4397587461604012,\n",
       "  0.43972787923283047,\n",
       "  0.43971192505624557,\n",
       "  0.43966640035311383,\n",
       "  0.4396445718076494,\n",
       "  0.43960385852389866,\n",
       "  0.4395780861377716,\n",
       "  0.4395444641510646,\n",
       "  0.4395129746860928,\n",
       "  0.43949655029508805,\n",
       "  0.4394673771328396,\n",
       "  0.43944096399678123,\n",
       "  0.43938904172844356,\n",
       "  0.43938007288508946,\n",
       "  0.43932777808772194,\n",
       "  0.43931449121899074,\n",
       "  0.4392664250400331,\n",
       "  0.43923279808627236,\n",
       "  0.4392138926519288,\n",
       "  0.43919085297319627,\n",
       "  0.4391953878932529,\n",
       "  0.4391198671526379,\n",
       "  0.4391143106751972,\n",
       "  0.43907784091101754,\n",
       "  0.4390309105316798,\n",
       "  0.439010136657291,\n",
       "  0.4389905085166295,\n",
       "  0.4389643006854587,\n",
       "  0.4389214681254493,\n",
       "  0.4389239052931468,\n",
       "  0.438887647456593,\n",
       "  0.4388321505652534,\n",
       "  0.4387897253036499,\n",
       "  0.4387577921152115,\n",
       "  0.4387356572681003,\n",
       "  0.438715386721823,\n",
       "  0.43869080146153766,\n",
       "  0.4386609014537599,\n",
       "  0.43863183591100907,\n",
       "  0.43857883082495797,\n",
       "  0.43855061795976424,\n",
       "  0.4385539922449324,\n",
       "  0.4384795543220308,\n",
       "  0.4384934769736396,\n",
       "  0.4384271452824275,\n",
       "  0.43838931454552543,\n",
       "  0.43837254246075946,\n",
       "  0.4383474936087926,\n",
       "  0.43833888073762256,\n",
       "  0.4382894817325804,\n",
       "  0.4382552496261067,\n",
       "  0.43824341065353817,\n",
       "  0.4381982336441676,\n",
       "  0.43817298611005145,\n",
       "  0.43815840946303475,\n",
       "  0.4381362398465474,\n",
       "  0.4380904949373669,\n",
       "  0.43805250028769177,\n",
       "  0.43802843656804824,\n",
       "  0.43799690653880435,\n",
       "  0.43797033694055343,\n",
       "  0.43794115549988216,\n",
       "  0.43790364431010353,\n",
       "  0.4378720869620641,\n",
       "  0.437848288151953,\n",
       "  0.4378166612651613,\n",
       "  0.43780400190088486,\n",
       "  0.4377810027864244,\n",
       "  0.4377432299984826,\n",
       "  0.43770072526401943,\n",
       "  0.43765932818253833,\n",
       "  0.4376426827576425,\n",
       "  0.437633764412668,\n",
       "  0.43758584558963776,\n",
       "  0.4375580698251724,\n",
       "  0.4375154624382655,\n",
       "  0.4374946289592319,\n",
       "  0.43743323617511326,\n",
       "  0.43742113643222386,\n",
       "  0.43738337192270493,\n",
       "  0.4373982747395833,\n",
       "  0.43735264407263863,\n",
       "  0.43729812734656864,\n",
       "  0.4372880823082394,\n",
       "  0.43724589546521503,\n",
       "  0.43721506827407414,\n",
       "  0.4371998922692405,\n",
       "  0.4371746910942925,\n",
       "  0.43713732063770294,\n",
       "  0.43709759579764473,\n",
       "  0.43707379036479527,\n",
       "  0.43704042666488224,\n",
       "  0.4370143910249074,\n",
       "  0.4369794875383377,\n",
       "  0.4369586739275191,\n",
       "  0.4369155317544937,\n",
       "  0.43691736294163597,\n",
       "  0.4368579073084725,\n",
       "  0.4368334064881007,\n",
       "  0.4368196858300103,\n",
       "  0.43679167164696586,\n",
       "  0.4367499119705624,\n",
       "  0.436721083190706,\n",
       "  0.4367102368010415,\n",
       "  0.4366506619585885,\n",
       "  0.43664009537961745,\n",
       "  0.43659307393762803,\n",
       "  0.43657246894306606,\n",
       "  0.43653982215457493,\n",
       "  0.43648524582386017,\n",
       "  0.4364791264136632,\n",
       "  0.4364451633559333,\n",
       "  0.4364174356063207,\n",
       "  0.4363784260219998,\n",
       "  0.43636128968662685,\n",
       "  0.4363265567355686,\n",
       "  0.43630245162381065],\n",
       " 'acc': [0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7795138888888888,\n",
       "  0.7777777777777778,\n",
       "  0.7795138888888888,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7760416666666666,\n",
       "  0.7777777777777778,\n",
       "  0.7777777777777778,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7777777777777778,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7743055555555556,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7743055555555556,\n",
       "  0.7760416666666666,\n",
       "  0.7777777777777778,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7743055555555556,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7743055555555556,\n",
       "  0.7760416666666666,\n",
       "  0.7760416666666666,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7725694444444444,\n",
       "  0.7743055555555556,\n",
       "  0.7725694444444444,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7743055555555556,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7743055555555556,\n",
       "  0.7743055555555556,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7743055555555556,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444,\n",
       "  0.7725694444444444]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f66a4028a58>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4363 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4362 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4362 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4362 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4361 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4361 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4361 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4360 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4360 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4360 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4360 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4359 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4359 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4359 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4358 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4358 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4358 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4357 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4357 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4357 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4356 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4356 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4356 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4356 - acc: 0.7708 - val_loss: 0.5087 - val_acc: 0.7552\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4356 - acc: 0.7726 - val_loss: 0.5087 - val_acc: 0.7552\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4355 - acc: 0.7708 - val_loss: 0.5087 - val_acc: 0.7552\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4355 - acc: 0.7708 - val_loss: 0.5087 - val_acc: 0.7552\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4354 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4354 - acc: 0.7708 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4354 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4353 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4353 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4353 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4353 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4352 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4352 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4352 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4352 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4351 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4351 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4351 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4350 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4350 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4350 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4350 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4349 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4349 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4348 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4348 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4348 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4348 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4348 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4347 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4347 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4347 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4346 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4346 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4346 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4346 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4345 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4345 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4345 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4344 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4344 - acc: 0.7743 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4344 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4344 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4343 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4343 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4343 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4343 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4342 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4342 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4342 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4341 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4341 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4341 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4340 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4340 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4340 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4340 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4339 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4339 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4339 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4338 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4338 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4338 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4338 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4337 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4337 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4337 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4336 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4336 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4336 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4336 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4336 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4335 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4335 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4335 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4335 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4334 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4334 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4334 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4333 - acc: 0.7726 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4333 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4333 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4332 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4332 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4332 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4332 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4332 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4331 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4331 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4331 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4330 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4330 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4330 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4330 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4330 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4329 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4329 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 62us/step - loss: 0.4329 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4328 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4328 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4328 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4328 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4328 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4327 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4327 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4327 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4327 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4326 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4326 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4326 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4325 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4326 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4325 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4325 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4324 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4324 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4324 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4324 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4324 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4323 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4323 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4323 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4322 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4322 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4322 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4322 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4321 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4321 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4321 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4321 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4320 - acc: 0.7726 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4320 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4320 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4320 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4320 - acc: 0.7743 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4318 - acc: 0.7778 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7552\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4317 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4317 - acc: 0.7760 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4317 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4317 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4317 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4316 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4316 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4316 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4316 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4315 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4315 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4315 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4314 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4314 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4314 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4314 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4313 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4313 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4313 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4312 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4312 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4312 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4312 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4311 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4311 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4311 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4311 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4310 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4310 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4310 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4309 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4309 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4309 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4309 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4308 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.4308 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4308 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4308 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4307 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4307 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4307 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4307 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4306 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4306 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4306 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4306 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4305 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4305 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4305 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4304 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4304 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4304 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4303 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4303 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4303 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4302 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4302 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4301 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4301 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4301 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4300 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4300 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4300 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4299 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4299 - acc: 0.7795 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4299 - acc: 0.7778 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4298 - acc: 0.7778 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4298 - acc: 0.7778 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4297 - acc: 0.7795 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4296 - acc: 0.7778 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4296 - acc: 0.7795 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4295 - acc: 0.7795 - val_loss: 0.5086 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4295 - acc: 0.7778 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4295 - acc: 0.7778 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4295 - acc: 0.7812 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4294 - acc: 0.7795 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4294 - acc: 0.7812 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4293 - acc: 0.7795 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4293 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4293 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4292 - acc: 0.7812 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4292 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4292 - acc: 0.7830 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4291 - acc: 0.7830 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4291 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4290 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4290 - acc: 0.7830 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4290 - acc: 0.7830 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4290 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4289 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4288 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4288 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4288 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4288 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4287 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4287 - acc: 0.7812 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4286 - acc: 0.7812 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4286 - acc: 0.7812 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4286 - acc: 0.7812 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4285 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4285 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4285 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4284 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4284 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4284 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4283 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4283 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4283 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4283 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4282 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4282 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4280 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4280 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4280 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4277 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4277 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4276 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4276 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4276 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4275 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4275 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4275 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4274 - acc: 0.7830 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4274 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4274 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4273 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4273 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4273 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4272 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4272 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4272 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7708\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4270 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4270 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4270 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4269 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4269 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4269 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4267 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4267 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4267 - acc: 0.7847 - val_loss: 0.5084 - val_acc: 0.7708\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4266 - acc: 0.7847 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4266 - acc: 0.7847 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4266 - acc: 0.7865 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4266 - acc: 0.7847 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4265 - acc: 0.7847 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4265 - acc: 0.7847 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4265 - acc: 0.7865 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4265 - acc: 0.7865 - val_loss: 0.5085 - val_acc: 0.7708\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4264 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4264 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4264 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4264 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4262 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4262 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4262 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4262 - acc: 0.7882 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4261 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4261 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4260 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 109us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7708\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7708\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7708\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7708\n",
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7708\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7708\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4254 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4253 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4253 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4252 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4252 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4252 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4252 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4252 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7708\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4251 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4251 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4250 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4250 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4250 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4247 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4246 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4246 - acc: 0.7917 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4246 - acc: 0.7917 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4240 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4240 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4240 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4238 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4238 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4238 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7708\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4234 - acc: 0.7917 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4234 - acc: 0.7917 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4234 - acc: 0.7917 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4233 - acc: 0.7917 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4233 - acc: 0.7917 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4233 - acc: 0.7917 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4233 - acc: 0.7917 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4232 - acc: 0.7917 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4232 - acc: 0.7917 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4232 - acc: 0.7917 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4232 - acc: 0.7917 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4231 - acc: 0.7917 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4231 - acc: 0.7917 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4231 - acc: 0.7917 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4231 - acc: 0.7917 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4230 - acc: 0.7917 - val_loss: 0.5105 - val_acc: 0.7760\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4230 - acc: 0.7917 - val_loss: 0.5105 - val_acc: 0.7760\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4229 - acc: 0.7917 - val_loss: 0.5105 - val_acc: 0.7760\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4229 - acc: 0.7917 - val_loss: 0.5105 - val_acc: 0.7760\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4229 - acc: 0.7917 - val_loss: 0.5106 - val_acc: 0.7760\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4229 - acc: 0.7917 - val_loss: 0.5106 - val_acc: 0.7760\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4229 - acc: 0.7917 - val_loss: 0.5106 - val_acc: 0.7760\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4228 - acc: 0.7917 - val_loss: 0.5106 - val_acc: 0.7760\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4228 - acc: 0.7917 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4228 - acc: 0.7917 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4228 - acc: 0.7917 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4227 - acc: 0.7917 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4227 - acc: 0.7917 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4227 - acc: 0.7917 - val_loss: 0.5108 - val_acc: 0.7760\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4227 - acc: 0.7917 - val_loss: 0.5108 - val_acc: 0.7760\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4226 - acc: 0.7917 - val_loss: 0.5108 - val_acc: 0.7760\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4226 - acc: 0.7917 - val_loss: 0.5108 - val_acc: 0.7760\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4226 - acc: 0.7917 - val_loss: 0.5109 - val_acc: 0.7760\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4225 - acc: 0.7917 - val_loss: 0.5109 - val_acc: 0.7760\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4225 - acc: 0.7917 - val_loss: 0.5109 - val_acc: 0.7760\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4225 - acc: 0.7917 - val_loss: 0.5109 - val_acc: 0.7760\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4225 - acc: 0.7917 - val_loss: 0.5109 - val_acc: 0.7760\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4224 - acc: 0.7917 - val_loss: 0.5110 - val_acc: 0.7760\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4224 - acc: 0.7917 - val_loss: 0.5110 - val_acc: 0.7760\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4224 - acc: 0.7917 - val_loss: 0.5110 - val_acc: 0.7760\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4224 - acc: 0.7917 - val_loss: 0.5110 - val_acc: 0.7760\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4223 - acc: 0.7917 - val_loss: 0.5111 - val_acc: 0.7760\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4223 - acc: 0.7917 - val_loss: 0.5111 - val_acc: 0.7760\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.5111 - val_acc: 0.7760\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.5111 - val_acc: 0.7760\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.5111 - val_acc: 0.7760\n",
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.5112 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4222 - acc: 0.7917 - val_loss: 0.5112 - val_acc: 0.7760\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4221 - acc: 0.7917 - val_loss: 0.5112 - val_acc: 0.7760\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4221 - acc: 0.7917 - val_loss: 0.5113 - val_acc: 0.7760\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4221 - acc: 0.7917 - val_loss: 0.5113 - val_acc: 0.7760\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4221 - acc: 0.7917 - val_loss: 0.5113 - val_acc: 0.7760\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.5113 - val_acc: 0.7760\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.5113 - val_acc: 0.7760\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4220 - acc: 0.7917 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4219 - acc: 0.7917 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4219 - acc: 0.7917 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4219 - acc: 0.7917 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4219 - acc: 0.7917 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.5115 - val_acc: 0.7760\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.5115 - val_acc: 0.7760\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.5115 - val_acc: 0.7760\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.5115 - val_acc: 0.7760\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4217 - acc: 0.7951 - val_loss: 0.5115 - val_acc: 0.7760\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4217 - acc: 0.7951 - val_loss: 0.5116 - val_acc: 0.7760\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4217 - acc: 0.7951 - val_loss: 0.5116 - val_acc: 0.7760\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4217 - acc: 0.7951 - val_loss: 0.5116 - val_acc: 0.7760\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4217 - acc: 0.7951 - val_loss: 0.5116 - val_acc: 0.7760\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4216 - acc: 0.7951 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4216 - acc: 0.7951 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4216 - acc: 0.7951 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4216 - acc: 0.7951 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4215 - acc: 0.7951 - val_loss: 0.5118 - val_acc: 0.7760\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.5118 - val_acc: 0.7760\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.5118 - val_acc: 0.7760\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4214 - acc: 0.7951 - val_loss: 0.5118 - val_acc: 0.7760\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4214 - acc: 0.7934 - val_loss: 0.5119 - val_acc: 0.7760\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4214 - acc: 0.7951 - val_loss: 0.5119 - val_acc: 0.7760\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4214 - acc: 0.7934 - val_loss: 0.5119 - val_acc: 0.7760\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4214 - acc: 0.7934 - val_loss: 0.5119 - val_acc: 0.7760\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4213 - acc: 0.7934 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4212 - acc: 0.7934 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4212 - acc: 0.7934 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4212 - acc: 0.7934 - val_loss: 0.5121 - val_acc: 0.7760\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4212 - acc: 0.7934 - val_loss: 0.5121 - val_acc: 0.7760\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4212 - acc: 0.7934 - val_loss: 0.5121 - val_acc: 0.7760\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.5121 - val_acc: 0.7760\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.5122 - val_acc: 0.7760\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.5122 - val_acc: 0.7760\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4211 - acc: 0.7934 - val_loss: 0.5122 - val_acc: 0.7760\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.5122 - val_acc: 0.7760\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7760\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7760\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4210 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7760\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7760\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7760\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4209 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.5125 - val_acc: 0.7760\n",
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4208 - acc: 0.7934 - val_loss: 0.5125 - val_acc: 0.7760\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.5125 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.5125 - val_acc: 0.7760\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4207 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.5126 - val_acc: 0.7760\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.5127 - val_acc: 0.7760\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4206 - acc: 0.7934 - val_loss: 0.5127 - val_acc: 0.7760\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.5127 - val_acc: 0.7760\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.5127 - val_acc: 0.7760\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7760\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7760\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4205 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7760\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.7760\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.5129 - val_acc: 0.7760\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4204 - acc: 0.7934 - val_loss: 0.5129 - val_acc: 0.7760\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4204 - acc: 0.7951 - val_loss: 0.5129 - val_acc: 0.7760\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4204 - acc: 0.7951 - val_loss: 0.5129 - val_acc: 0.7760\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4203 - acc: 0.7951 - val_loss: 0.5129 - val_acc: 0.7760\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4204 - acc: 0.7951 - val_loss: 0.5130 - val_acc: 0.7760\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4203 - acc: 0.7951 - val_loss: 0.5130 - val_acc: 0.7708\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4203 - acc: 0.7951 - val_loss: 0.5130 - val_acc: 0.7708\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4203 - acc: 0.7951 - val_loss: 0.5130 - val_acc: 0.7708\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4202 - acc: 0.7951 - val_loss: 0.5131 - val_acc: 0.7708\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4202 - acc: 0.7951 - val_loss: 0.5131 - val_acc: 0.7708\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4202 - acc: 0.7951 - val_loss: 0.5131 - val_acc: 0.7708\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4202 - acc: 0.7951 - val_loss: 0.5131 - val_acc: 0.7708\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4201 - acc: 0.7951 - val_loss: 0.5131 - val_acc: 0.7708\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4201 - acc: 0.7951 - val_loss: 0.5132 - val_acc: 0.7708\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4201 - acc: 0.7951 - val_loss: 0.5132 - val_acc: 0.7708\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4201 - acc: 0.7951 - val_loss: 0.5132 - val_acc: 0.7708\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4201 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4200 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4200 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4200 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4200 - acc: 0.7951 - val_loss: 0.5133 - val_acc: 0.7708\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4199 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7708\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4200 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7708\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 327us/step - loss: 0.4199 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7708\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4199 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7708\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4199 - acc: 0.7951 - val_loss: 0.5134 - val_acc: 0.7708\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4199 - acc: 0.7951 - val_loss: 0.5135 - val_acc: 0.7708\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4198 - acc: 0.7951 - val_loss: 0.5135 - val_acc: 0.7708\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4198 - acc: 0.7951 - val_loss: 0.5135 - val_acc: 0.7708\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4198 - acc: 0.7951 - val_loss: 0.5135 - val_acc: 0.7708\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4198 - acc: 0.7951 - val_loss: 0.5136 - val_acc: 0.7708\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4198 - acc: 0.7951 - val_loss: 0.5136 - val_acc: 0.7708\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4197 - acc: 0.7951 - val_loss: 0.5136 - val_acc: 0.7708\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4197 - acc: 0.7951 - val_loss: 0.5136 - val_acc: 0.7708\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4197 - acc: 0.7951 - val_loss: 0.5137 - val_acc: 0.7708\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4197 - acc: 0.7951 - val_loss: 0.5137 - val_acc: 0.7708\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4197 - acc: 0.7951 - val_loss: 0.5137 - val_acc: 0.7708\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4196 - acc: 0.7951 - val_loss: 0.5137 - val_acc: 0.7708\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4196 - acc: 0.7951 - val_loss: 0.5138 - val_acc: 0.7708\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4196 - acc: 0.7951 - val_loss: 0.5138 - val_acc: 0.7708\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4196 - acc: 0.7951 - val_loss: 0.5138 - val_acc: 0.7708\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4195 - acc: 0.7951 - val_loss: 0.5138 - val_acc: 0.7708\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4195 - acc: 0.7951 - val_loss: 0.5139 - val_acc: 0.7708\n",
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4195 - acc: 0.7951 - val_loss: 0.5139 - val_acc: 0.7708\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4195 - acc: 0.7951 - val_loss: 0.5139 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4195 - acc: 0.7951 - val_loss: 0.5139 - val_acc: 0.7708\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4195 - acc: 0.7951 - val_loss: 0.5139 - val_acc: 0.7708\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4194 - acc: 0.7951 - val_loss: 0.5140 - val_acc: 0.7708\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4194 - acc: 0.7951 - val_loss: 0.5140 - val_acc: 0.7708\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4194 - acc: 0.7951 - val_loss: 0.5140 - val_acc: 0.7708\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4194 - acc: 0.7951 - val_loss: 0.5140 - val_acc: 0.7708\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4194 - acc: 0.7951 - val_loss: 0.5141 - val_acc: 0.7708\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4193 - acc: 0.7951 - val_loss: 0.5141 - val_acc: 0.7708\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4193 - acc: 0.7951 - val_loss: 0.5141 - val_acc: 0.7708\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4193 - acc: 0.7951 - val_loss: 0.5141 - val_acc: 0.7708\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4193 - acc: 0.7951 - val_loss: 0.5141 - val_acc: 0.7708\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4192 - acc: 0.7951 - val_loss: 0.5142 - val_acc: 0.7708\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4192 - acc: 0.7951 - val_loss: 0.5142 - val_acc: 0.7708\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4192 - acc: 0.7951 - val_loss: 0.5142 - val_acc: 0.7708\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4192 - acc: 0.7951 - val_loss: 0.5142 - val_acc: 0.7708\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4192 - acc: 0.7951 - val_loss: 0.5142 - val_acc: 0.7708\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4192 - acc: 0.7951 - val_loss: 0.5143 - val_acc: 0.7708\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4191 - acc: 0.7951 - val_loss: 0.5143 - val_acc: 0.7708\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4191 - acc: 0.7951 - val_loss: 0.5143 - val_acc: 0.7656\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4191 - acc: 0.7951 - val_loss: 0.5143 - val_acc: 0.7656\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4191 - acc: 0.7951 - val_loss: 0.5143 - val_acc: 0.7656\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4190 - acc: 0.7951 - val_loss: 0.5143 - val_acc: 0.7656\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4190 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4190 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4190 - acc: 0.7969 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4190 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4190 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4189 - acc: 0.7951 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4188 - acc: 0.7951 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4188 - acc: 0.7951 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4188 - acc: 0.7951 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4188 - acc: 0.7951 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4187 - acc: 0.7951 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4187 - acc: 0.7951 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4187 - acc: 0.7951 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4187 - acc: 0.7969 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4187 - acc: 0.7951 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4185 - acc: 0.7951 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4186 - acc: 0.7951 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4185 - acc: 0.7969 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4185 - acc: 0.7951 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4185 - acc: 0.7951 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4185 - acc: 0.7951 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4184 - acc: 0.7951 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4184 - acc: 0.7951 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4184 - acc: 0.7951 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4184 - acc: 0.7951 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4184 - acc: 0.7969 - val_loss: 0.5149 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4183 - acc: 0.7969 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4183 - acc: 0.7951 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4183 - acc: 0.7951 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4183 - acc: 0.7951 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4183 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4182 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4180 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4181 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4180 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4180 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4180 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4180 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4180 - acc: 0.7951 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4179 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4179 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4179 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4179 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4178 - acc: 0.7969 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4178 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4178 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4178 - acc: 0.7951 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4178 - acc: 0.7969 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4178 - acc: 0.7951 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4178 - acc: 0.7969 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4178 - acc: 0.7969 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4178 - acc: 0.7951 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4177 - acc: 0.7969 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4177 - acc: 0.7969 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4177 - acc: 0.7969 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4177 - acc: 0.7951 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4177 - acc: 0.7951 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4176 - acc: 0.7986 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4176 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4175 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4175 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4175 - acc: 0.7951 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4175 - acc: 0.7951 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4175 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4174 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4175 - acc: 0.7969 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4174 - acc: 0.7951 - val_loss: 0.5156 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4174 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4174 - acc: 0.7986 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4174 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4174 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4174 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4173 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4174 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4173 - acc: 0.7986 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4173 - acc: 0.7969 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4173 - acc: 0.7969 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4173 - acc: 0.7986 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4173 - acc: 0.7969 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4172 - acc: 0.7951 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4172 - acc: 0.7969 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4172 - acc: 0.7986 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4172 - acc: 0.7969 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4172 - acc: 0.7969 - val_loss: 0.5158 - val_acc: 0.7656\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4172 - acc: 0.7969 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4171 - acc: 0.8003 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4171 - acc: 0.7969 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4171 - acc: 0.7986 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4171 - acc: 0.7969 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4171 - acc: 0.8003 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4171 - acc: 0.8003 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4171 - acc: 0.7986 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4170 - acc: 0.7986 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4170 - acc: 0.8003 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4170 - acc: 0.8021 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4170 - acc: 0.8021 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4170 - acc: 0.8003 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4170 - acc: 0.8003 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4170 - acc: 0.8003 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4169 - acc: 0.8003 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4169 - acc: 0.8003 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4169 - acc: 0.8021 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4169 - acc: 0.7986 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4169 - acc: 0.8003 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4169 - acc: 0.8003 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4169 - acc: 0.7986 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4168 - acc: 0.8021 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4168 - acc: 0.8021 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4168 - acc: 0.8021 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4167 - acc: 0.8003 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4167 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4167 - acc: 0.8003 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4167 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4167 - acc: 0.8021 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4167 - acc: 0.8003 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4166 - acc: 0.7986 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4166 - acc: 0.8021 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4166 - acc: 0.8021 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4166 - acc: 0.8003 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4166 - acc: 0.8021 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4165 - acc: 0.8021 - val_loss: 0.5164 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4165 - acc: 0.7986 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4165 - acc: 0.8003 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4165 - acc: 0.8021 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4165 - acc: 0.8021 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4164 - acc: 0.8021 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4164 - acc: 0.8021 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4164 - acc: 0.8003 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4164 - acc: 0.8021 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4164 - acc: 0.8038 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4163 - acc: 0.8021 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4163 - acc: 0.8021 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4163 - acc: 0.8003 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4162 - acc: 0.8021 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4163 - acc: 0.8021 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4162 - acc: 0.8038 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4162 - acc: 0.8003 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4162 - acc: 0.8021 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4162 - acc: 0.8003 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4162 - acc: 0.8021 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4162 - acc: 0.8038 - val_loss: 0.5168 - val_acc: 0.7656\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4161 - acc: 0.8003 - val_loss: 0.5168 - val_acc: 0.7656\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4161 - acc: 0.8021 - val_loss: 0.5168 - val_acc: 0.7656\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4161 - acc: 0.8038 - val_loss: 0.5168 - val_acc: 0.7656\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4161 - acc: 0.8021 - val_loss: 0.5168 - val_acc: 0.7656\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4161 - acc: 0.8021 - val_loss: 0.5168 - val_acc: 0.7656\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4161 - acc: 0.8021 - val_loss: 0.5169 - val_acc: 0.7656\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4160 - acc: 0.8021 - val_loss: 0.5169 - val_acc: 0.7656\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4160 - acc: 0.8038 - val_loss: 0.5169 - val_acc: 0.7656\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4160 - acc: 0.8021 - val_loss: 0.5169 - val_acc: 0.7656\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4160 - acc: 0.8021 - val_loss: 0.5169 - val_acc: 0.7656\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4160 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7656\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4160 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7656\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4159 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4159 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4159 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4159 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.4158 - acc: 0.8038 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4158 - acc: 0.8038 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4158 - acc: 0.8038 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4158 - acc: 0.8038 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4158 - acc: 0.8038 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4158 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4156 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4156 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4156 - acc: 0.8038 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4156 - acc: 0.8038 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4156 - acc: 0.8056 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4156 - acc: 0.8038 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4156 - acc: 0.8021 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4155 - acc: 0.8038 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4155 - acc: 0.8038 - val_loss: 0.5173 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4155 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4155 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 105us/step - loss: 0.4155 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4154 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4154 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4154 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4154 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4154 - acc: 0.8038 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4153 - acc: 0.8038 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4153 - acc: 0.8038 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4154 - acc: 0.8038 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4153 - acc: 0.8056 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4153 - acc: 0.8038 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4153 - acc: 0.8056 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4153 - acc: 0.8038 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4152 - acc: 0.8056 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4152 - acc: 0.8038 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4152 - acc: 0.8038 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4152 - acc: 0.8056 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4152 - acc: 0.8056 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4152 - acc: 0.8056 - val_loss: 0.5176 - val_acc: 0.7552\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4151 - acc: 0.8056 - val_loss: 0.5176 - val_acc: 0.7552\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4151 - acc: 0.8056 - val_loss: 0.5176 - val_acc: 0.7552\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4151 - acc: 0.8038 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4151 - acc: 0.8056 - val_loss: 0.5177 - val_acc: 0.7552\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4151 - acc: 0.8056 - val_loss: 0.5177 - val_acc: 0.7552\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4151 - acc: 0.8056 - val_loss: 0.5177 - val_acc: 0.7552\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4151 - acc: 0.8056 - val_loss: 0.5177 - val_acc: 0.7552\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4150 - acc: 0.8038 - val_loss: 0.5177 - val_acc: 0.7552\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4150 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4150 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4150 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4150 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4150 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4149 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 113us/step - loss: 0.4149 - acc: 0.8056 - val_loss: 0.5178 - val_acc: 0.7552\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4149 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4149 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4149 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4149 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5179 - val_acc: 0.7552\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4147 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4147 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4147 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4147 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4147 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4147 - acc: 0.8056 - val_loss: 0.5180 - val_acc: 0.7552\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7552\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7552\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4145 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4145 - acc: 0.8056 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4145 - acc: 0.8056 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4145 - acc: 0.8056 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4145 - acc: 0.8073 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4144 - acc: 0.8056 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4144 - acc: 0.8073 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4144 - acc: 0.8056 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4144 - acc: 0.8056 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4144 - acc: 0.8056 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4144 - acc: 0.8073 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4144 - acc: 0.8073 - val_loss: 0.5182 - val_acc: 0.7500\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4143 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4143 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4143 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4143 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4143 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4143 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4142 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4141 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4141 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4141 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4141 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4141 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4140 - acc: 0.8073 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4140 - acc: 0.8073 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4140 - acc: 0.8073 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4140 - acc: 0.8073 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4140 - acc: 0.8073 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4140 - acc: 0.8090 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4140 - acc: 0.8056 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4140 - acc: 0.8090 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4140 - acc: 0.8056 - val_loss: 0.5185 - val_acc: 0.7500\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4139 - acc: 0.8090 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4139 - acc: 0.8073 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4139 - acc: 0.8056 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4139 - acc: 0.8090 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4139 - acc: 0.8073 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4139 - acc: 0.8073 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4138 - acc: 0.8073 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4138 - acc: 0.8073 - val_loss: 0.5186 - val_acc: 0.7500\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4138 - acc: 0.8073 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4138 - acc: 0.8073 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4138 - acc: 0.8073 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4138 - acc: 0.8073 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4137 - acc: 0.8090 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4137 - acc: 0.8073 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4136 - acc: 0.8090 - val_loss: 0.5188 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4137 - acc: 0.8090 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4136 - acc: 0.8073 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4136 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4136 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4136 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4136 - acc: 0.8073 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4136 - acc: 0.8073 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4135 - acc: 0.8073 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4135 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4135 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4135 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4135 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4135 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4134 - acc: 0.8090 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4134 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4134 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4134 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4134 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4134 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4134 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4134 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4134 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4133 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4133 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4133 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4133 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4133 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4133 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4133 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4132 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4132 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4132 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4132 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4132 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4131 - acc: 0.8073 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4131 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4131 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4131 - acc: 0.8090 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4131 - acc: 0.8090 - val_loss: 0.5191 - val_acc: 0.7500\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4131 - acc: 0.8073 - val_loss: 0.5191 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f66a4063dd8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, K\n",
    "from keras.layers import Dense, Flatten, Input, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid\n",
    "model_2 = Sequential([\n",
    "    Dense(6, input_shape=(8,), activation='relu'),\n",
    "    Dense(6, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/400\n",
      "576/576 [==============================] - 0s 565us/step - loss: 0.6961 - acc: 0.5920 - val_loss: 4.2320 - val_acc: 0.5208\n",
      "Epoch 2/400\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6865 - acc: 0.6094 - val_loss: 4.0817 - val_acc: 0.5000\n",
      "Epoch 3/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6775 - acc: 0.6389 - val_loss: 3.9061 - val_acc: 0.5104\n",
      "Epoch 4/400\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6692 - acc: 0.6528 - val_loss: 3.7507 - val_acc: 0.5104\n",
      "Epoch 5/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6614 - acc: 0.6597 - val_loss: 3.6441 - val_acc: 0.5000\n",
      "Epoch 6/400\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.6537 - acc: 0.6701 - val_loss: 3.5737 - val_acc: 0.4948\n",
      "Epoch 7/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6462 - acc: 0.6753 - val_loss: 3.5116 - val_acc: 0.4792\n",
      "Epoch 8/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6391 - acc: 0.6771 - val_loss: 3.4529 - val_acc: 0.4792\n",
      "Epoch 9/400\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6327 - acc: 0.6806 - val_loss: 3.4215 - val_acc: 0.4635\n",
      "Epoch 10/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6269 - acc: 0.6858 - val_loss: 3.4274 - val_acc: 0.4427\n",
      "Epoch 11/400\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.6215 - acc: 0.6858 - val_loss: 3.4778 - val_acc: 0.4323\n",
      "Epoch 12/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6165 - acc: 0.6858 - val_loss: 3.5504 - val_acc: 0.4323\n",
      "Epoch 13/400\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6118 - acc: 0.6858 - val_loss: 3.6534 - val_acc: 0.4479\n",
      "Epoch 14/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6074 - acc: 0.6840 - val_loss: 3.7855 - val_acc: 0.4375\n",
      "Epoch 15/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6033 - acc: 0.6858 - val_loss: 3.9234 - val_acc: 0.4323\n",
      "Epoch 16/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5992 - acc: 0.6806 - val_loss: 4.0673 - val_acc: 0.4271\n",
      "Epoch 17/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5955 - acc: 0.6788 - val_loss: 4.2015 - val_acc: 0.4219\n",
      "Epoch 18/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5920 - acc: 0.6788 - val_loss: 4.3403 - val_acc: 0.4115\n",
      "Epoch 19/400\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5885 - acc: 0.6788 - val_loss: 4.4982 - val_acc: 0.4115\n",
      "Epoch 20/400\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5850 - acc: 0.6806 - val_loss: 4.6835 - val_acc: 0.4062\n",
      "Epoch 21/400\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5815 - acc: 0.6788 - val_loss: 4.8697 - val_acc: 0.4115\n",
      "Epoch 22/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5783 - acc: 0.6736 - val_loss: 5.0614 - val_acc: 0.4062\n",
      "Epoch 23/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5751 - acc: 0.6823 - val_loss: 5.2659 - val_acc: 0.3958\n",
      "Epoch 24/400\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5721 - acc: 0.6823 - val_loss: 5.4935 - val_acc: 0.3958\n",
      "Epoch 25/400\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5691 - acc: 0.6858 - val_loss: 5.7152 - val_acc: 0.3750\n",
      "Epoch 26/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5661 - acc: 0.6875 - val_loss: 5.9482 - val_acc: 0.3750\n",
      "Epoch 27/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5631 - acc: 0.6875 - val_loss: 6.1749 - val_acc: 0.3698\n",
      "Epoch 28/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5602 - acc: 0.6927 - val_loss: 6.3891 - val_acc: 0.3698\n",
      "Epoch 29/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5572 - acc: 0.6892 - val_loss: 6.5998 - val_acc: 0.3698\n",
      "Epoch 30/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5545 - acc: 0.6892 - val_loss: 6.8234 - val_acc: 0.3594\n",
      "Epoch 31/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5517 - acc: 0.6927 - val_loss: 7.0366 - val_acc: 0.3542\n",
      "Epoch 32/400\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5489 - acc: 0.6944 - val_loss: 7.2461 - val_acc: 0.3542\n",
      "Epoch 33/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5462 - acc: 0.6944 - val_loss: 7.4396 - val_acc: 0.3542\n",
      "Epoch 34/400\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5438 - acc: 0.7014 - val_loss: 7.6197 - val_acc: 0.3542\n",
      "Epoch 35/400\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5413 - acc: 0.7049 - val_loss: 7.7861 - val_acc: 0.3542\n",
      "Epoch 36/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5390 - acc: 0.7083 - val_loss: 7.9496 - val_acc: 0.3542\n",
      "Epoch 37/400\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5368 - acc: 0.7031 - val_loss: 8.1084 - val_acc: 0.3542\n",
      "Epoch 38/400\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5346 - acc: 0.7031 - val_loss: 8.2552 - val_acc: 0.3542\n",
      "Epoch 39/400\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5323 - acc: 0.7083 - val_loss: 8.4091 - val_acc: 0.3542\n",
      "Epoch 40/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5300 - acc: 0.7118 - val_loss: 8.5614 - val_acc: 0.3542\n",
      "Epoch 41/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5277 - acc: 0.7135 - val_loss: 8.7070 - val_acc: 0.3542\n",
      "Epoch 42/400\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5255 - acc: 0.7153 - val_loss: 8.8313 - val_acc: 0.3542\n",
      "Epoch 43/400\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5234 - acc: 0.7205 - val_loss: 8.9514 - val_acc: 0.3542\n",
      "Epoch 44/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5214 - acc: 0.7205 - val_loss: 9.0688 - val_acc: 0.3542\n",
      "Epoch 45/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5195 - acc: 0.7205 - val_loss: 9.1804 - val_acc: 0.3542\n",
      "Epoch 46/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5174 - acc: 0.7205 - val_loss: 9.2867 - val_acc: 0.3542\n",
      "Epoch 47/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5155 - acc: 0.7257 - val_loss: 9.3933 - val_acc: 0.3542\n",
      "Epoch 48/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5137 - acc: 0.7309 - val_loss: 9.4875 - val_acc: 0.3542\n",
      "Epoch 49/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5118 - acc: 0.7309 - val_loss: 9.5699 - val_acc: 0.3542\n",
      "Epoch 50/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5101 - acc: 0.7309 - val_loss: 9.6288 - val_acc: 0.3542\n",
      "Epoch 51/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5084 - acc: 0.7344 - val_loss: 9.6922 - val_acc: 0.3542\n",
      "Epoch 52/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5068 - acc: 0.7326 - val_loss: 9.7409 - val_acc: 0.3542\n",
      "Epoch 53/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5051 - acc: 0.7344 - val_loss: 9.7936 - val_acc: 0.3542\n",
      "Epoch 54/400\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5035 - acc: 0.7396 - val_loss: 9.8435 - val_acc: 0.3542\n",
      "Epoch 55/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5019 - acc: 0.7396 - val_loss: 9.8915 - val_acc: 0.3542\n",
      "Epoch 56/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5005 - acc: 0.7465 - val_loss: 9.9272 - val_acc: 0.3542\n",
      "Epoch 57/400\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4990 - acc: 0.7535 - val_loss: 9.9645 - val_acc: 0.3542\n",
      "Epoch 58/400\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4975 - acc: 0.7535 - val_loss: 10.0010 - val_acc: 0.3542\n",
      "Epoch 59/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4960 - acc: 0.7604 - val_loss: 10.0361 - val_acc: 0.3542\n",
      "Epoch 60/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4946 - acc: 0.7587 - val_loss: 10.0648 - val_acc: 0.3542\n",
      "Epoch 61/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 86us/step - loss: 0.4933 - acc: 0.7604 - val_loss: 10.0868 - val_acc: 0.3542\n",
      "Epoch 62/400\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4919 - acc: 0.7569 - val_loss: 10.1099 - val_acc: 0.3542\n",
      "Epoch 63/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4908 - acc: 0.7622 - val_loss: 10.1232 - val_acc: 0.3542\n",
      "Epoch 64/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4896 - acc: 0.7622 - val_loss: 10.1380 - val_acc: 0.3542\n",
      "Epoch 65/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4884 - acc: 0.7656 - val_loss: 10.1523 - val_acc: 0.3542\n",
      "Epoch 66/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4873 - acc: 0.7639 - val_loss: 10.1650 - val_acc: 0.3542\n",
      "Epoch 67/400\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4863 - acc: 0.7656 - val_loss: 10.1764 - val_acc: 0.3542\n",
      "Epoch 68/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4852 - acc: 0.7622 - val_loss: 10.1880 - val_acc: 0.3542\n",
      "Epoch 69/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4842 - acc: 0.7674 - val_loss: 10.1962 - val_acc: 0.3542\n",
      "Epoch 70/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4833 - acc: 0.7674 - val_loss: 10.2010 - val_acc: 0.3542\n",
      "Epoch 71/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4824 - acc: 0.7639 - val_loss: 10.2100 - val_acc: 0.3542\n",
      "Epoch 72/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4816 - acc: 0.7639 - val_loss: 10.2128 - val_acc: 0.3542\n",
      "Epoch 73/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4806 - acc: 0.7622 - val_loss: 10.2198 - val_acc: 0.3542\n",
      "Epoch 74/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4797 - acc: 0.7622 - val_loss: 10.2230 - val_acc: 0.3542\n",
      "Epoch 75/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4789 - acc: 0.7622 - val_loss: 10.2258 - val_acc: 0.3542\n",
      "Epoch 76/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4782 - acc: 0.7622 - val_loss: 10.2251 - val_acc: 0.3542\n",
      "Epoch 77/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4772 - acc: 0.7639 - val_loss: 10.2317 - val_acc: 0.3542\n",
      "Epoch 78/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4763 - acc: 0.7639 - val_loss: 10.2328 - val_acc: 0.3542\n",
      "Epoch 79/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4754 - acc: 0.7674 - val_loss: 10.2324 - val_acc: 0.3542\n",
      "Epoch 80/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4747 - acc: 0.7674 - val_loss: 10.2375 - val_acc: 0.3542\n",
      "Epoch 81/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4739 - acc: 0.7708 - val_loss: 10.2370 - val_acc: 0.3542\n",
      "Epoch 82/400\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4732 - acc: 0.7674 - val_loss: 10.2367 - val_acc: 0.3542\n",
      "Epoch 83/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4724 - acc: 0.7691 - val_loss: 10.2366 - val_acc: 0.3542\n",
      "Epoch 84/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4716 - acc: 0.7674 - val_loss: 10.2400 - val_acc: 0.3542\n",
      "Epoch 85/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4709 - acc: 0.7708 - val_loss: 10.2395 - val_acc: 0.3542\n",
      "Epoch 86/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4703 - acc: 0.7691 - val_loss: 10.2392 - val_acc: 0.3542\n",
      "Epoch 87/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4697 - acc: 0.7708 - val_loss: 10.2389 - val_acc: 0.3542\n",
      "Epoch 88/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4691 - acc: 0.7708 - val_loss: 10.2387 - val_acc: 0.3542\n",
      "Epoch 89/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4685 - acc: 0.7743 - val_loss: 10.2385 - val_acc: 0.3542\n",
      "Epoch 90/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4677 - acc: 0.7743 - val_loss: 10.2382 - val_acc: 0.3542\n",
      "Epoch 91/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4672 - acc: 0.7760 - val_loss: 10.2378 - val_acc: 0.3542\n",
      "Epoch 92/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4666 - acc: 0.7674 - val_loss: 10.2377 - val_acc: 0.3542\n",
      "Epoch 93/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4661 - acc: 0.7708 - val_loss: 10.2374 - val_acc: 0.3542\n",
      "Epoch 94/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4655 - acc: 0.7708 - val_loss: 10.2373 - val_acc: 0.3542\n",
      "Epoch 95/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4650 - acc: 0.7708 - val_loss: 10.2375 - val_acc: 0.3542\n",
      "Epoch 96/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4644 - acc: 0.7708 - val_loss: 10.2378 - val_acc: 0.3542\n",
      "Epoch 97/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4641 - acc: 0.7708 - val_loss: 10.2384 - val_acc: 0.3542\n",
      "Epoch 98/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4637 - acc: 0.7743 - val_loss: 10.2377 - val_acc: 0.3542\n",
      "Epoch 99/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 10.2378 - val_acc: 0.3542\n",
      "Epoch 100/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4629 - acc: 0.7795 - val_loss: 10.2386 - val_acc: 0.3542\n",
      "Epoch 101/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7760 - val_loss: 10.2382 - val_acc: 0.3542\n",
      "Epoch 102/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 10.2381 - val_acc: 0.3542\n",
      "Epoch 103/400\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4618 - acc: 0.7812 - val_loss: 10.2389 - val_acc: 0.3542\n",
      "Epoch 104/400\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 10.2388 - val_acc: 0.3542\n",
      "Epoch 105/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 10.2384 - val_acc: 0.3542\n",
      "Epoch 106/400\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4610 - acc: 0.7830 - val_loss: 10.2387 - val_acc: 0.3542\n",
      "Epoch 107/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4605 - acc: 0.7778 - val_loss: 10.2394 - val_acc: 0.3542\n",
      "Epoch 108/400\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 10.2395 - val_acc: 0.3542\n",
      "Epoch 109/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4599 - acc: 0.7812 - val_loss: 10.2400 - val_acc: 0.3542\n",
      "Epoch 110/400\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4597 - acc: 0.7812 - val_loss: 10.2404 - val_acc: 0.3542\n",
      "Epoch 111/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4593 - acc: 0.7812 - val_loss: 10.2404 - val_acc: 0.3542\n",
      "Epoch 112/400\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4593 - acc: 0.7830 - val_loss: 10.2411 - val_acc: 0.3542\n",
      "Epoch 113/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4590 - acc: 0.7795 - val_loss: 10.2413 - val_acc: 0.3542\n",
      "Epoch 114/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4587 - acc: 0.7795 - val_loss: 10.2414 - val_acc: 0.3542\n",
      "Epoch 115/400\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 10.2416 - val_acc: 0.3542\n",
      "Epoch 116/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4584 - acc: 0.7795 - val_loss: 10.2417 - val_acc: 0.3542\n",
      "Epoch 117/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4581 - acc: 0.7795 - val_loss: 10.2424 - val_acc: 0.3542\n",
      "Epoch 118/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4580 - acc: 0.7795 - val_loss: 10.2424 - val_acc: 0.3542\n",
      "Epoch 119/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4576 - acc: 0.7812 - val_loss: 10.2432 - val_acc: 0.3542\n",
      "Epoch 120/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4574 - acc: 0.7795 - val_loss: 10.2432 - val_acc: 0.3542\n",
      "Epoch 121/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 75us/step - loss: 0.4572 - acc: 0.7795 - val_loss: 10.2433 - val_acc: 0.3542\n",
      "Epoch 122/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4569 - acc: 0.7778 - val_loss: 10.2438 - val_acc: 0.3542\n",
      "Epoch 123/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4567 - acc: 0.7778 - val_loss: 10.2443 - val_acc: 0.3542\n",
      "Epoch 124/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4565 - acc: 0.7795 - val_loss: 10.2441 - val_acc: 0.3542\n",
      "Epoch 125/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4566 - acc: 0.7795 - val_loss: 10.2449 - val_acc: 0.3542\n",
      "Epoch 126/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4562 - acc: 0.7812 - val_loss: 10.2456 - val_acc: 0.3542\n",
      "Epoch 127/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4559 - acc: 0.7812 - val_loss: 10.2455 - val_acc: 0.3542\n",
      "Epoch 128/400\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4559 - acc: 0.7830 - val_loss: 10.2465 - val_acc: 0.3542\n",
      "Epoch 129/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4555 - acc: 0.7847 - val_loss: 10.2467 - val_acc: 0.3542\n",
      "Epoch 130/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4553 - acc: 0.7830 - val_loss: 10.2473 - val_acc: 0.3542\n",
      "Epoch 131/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4551 - acc: 0.7847 - val_loss: 10.2478 - val_acc: 0.3542\n",
      "Epoch 132/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4548 - acc: 0.7865 - val_loss: 10.2482 - val_acc: 0.3542\n",
      "Epoch 133/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4549 - acc: 0.7865 - val_loss: 10.2481 - val_acc: 0.3542\n",
      "Epoch 134/400\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4547 - acc: 0.7847 - val_loss: 10.2482 - val_acc: 0.3542\n",
      "Epoch 135/400\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4545 - acc: 0.7865 - val_loss: 10.2486 - val_acc: 0.3542\n",
      "Epoch 136/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4543 - acc: 0.7882 - val_loss: 10.2494 - val_acc: 0.3542\n",
      "Epoch 137/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4542 - acc: 0.7882 - val_loss: 10.2497 - val_acc: 0.3542\n",
      "Epoch 138/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4542 - acc: 0.7865 - val_loss: 10.2502 - val_acc: 0.3542\n",
      "Epoch 139/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4539 - acc: 0.7865 - val_loss: 10.2505 - val_acc: 0.3542\n",
      "Epoch 140/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4540 - acc: 0.7865 - val_loss: 10.2504 - val_acc: 0.3542\n",
      "Epoch 141/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4535 - acc: 0.7847 - val_loss: 10.2506 - val_acc: 0.3542\n",
      "Epoch 142/400\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4533 - acc: 0.7865 - val_loss: 10.2515 - val_acc: 0.3542\n",
      "Epoch 143/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4533 - acc: 0.7847 - val_loss: 10.2521 - val_acc: 0.3542\n",
      "Epoch 144/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4530 - acc: 0.7865 - val_loss: 10.2526 - val_acc: 0.3542\n",
      "Epoch 145/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4531 - acc: 0.7865 - val_loss: 10.2527 - val_acc: 0.3542\n",
      "Epoch 146/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4528 - acc: 0.7882 - val_loss: 10.2527 - val_acc: 0.3542\n",
      "Epoch 147/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4526 - acc: 0.7865 - val_loss: 10.2531 - val_acc: 0.3542\n",
      "Epoch 148/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4526 - acc: 0.7882 - val_loss: 10.2531 - val_acc: 0.3542\n",
      "Epoch 149/400\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4524 - acc: 0.7847 - val_loss: 10.2536 - val_acc: 0.3542\n",
      "Epoch 150/400\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4521 - acc: 0.7847 - val_loss: 10.2539 - val_acc: 0.3542\n",
      "Epoch 151/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4519 - acc: 0.7865 - val_loss: 10.2545 - val_acc: 0.3542\n",
      "Epoch 152/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4519 - acc: 0.7865 - val_loss: 10.2544 - val_acc: 0.3542\n",
      "Epoch 153/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4517 - acc: 0.7847 - val_loss: 10.2552 - val_acc: 0.3542\n",
      "Epoch 154/400\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4515 - acc: 0.7830 - val_loss: 10.2556 - val_acc: 0.3542\n",
      "Epoch 155/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4514 - acc: 0.7865 - val_loss: 10.2556 - val_acc: 0.3542\n",
      "Epoch 156/400\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4514 - acc: 0.7830 - val_loss: 10.2557 - val_acc: 0.3542\n",
      "Epoch 157/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4512 - acc: 0.7865 - val_loss: 10.2560 - val_acc: 0.3542\n",
      "Epoch 158/400\n",
      "576/576 [==============================] - 0s 125us/step - loss: 0.4511 - acc: 0.7830 - val_loss: 10.2564 - val_acc: 0.3542\n",
      "Epoch 159/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4508 - acc: 0.7830 - val_loss: 10.2567 - val_acc: 0.3542\n",
      "Epoch 160/400\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.4506 - acc: 0.7830 - val_loss: 10.2568 - val_acc: 0.3542\n",
      "Epoch 161/400\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4505 - acc: 0.7847 - val_loss: 10.2572 - val_acc: 0.3542\n",
      "Epoch 162/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4504 - acc: 0.7812 - val_loss: 10.2578 - val_acc: 0.3542\n",
      "Epoch 163/400\n",
      "576/576 [==============================] - 0s 124us/step - loss: 0.4503 - acc: 0.7830 - val_loss: 10.2576 - val_acc: 0.3542\n",
      "Epoch 164/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4500 - acc: 0.7812 - val_loss: 10.2565 - val_acc: 0.3542\n",
      "Epoch 165/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4499 - acc: 0.7830 - val_loss: 10.2570 - val_acc: 0.3542\n",
      "Epoch 166/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4497 - acc: 0.7812 - val_loss: 10.2569 - val_acc: 0.3542\n",
      "Epoch 167/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4496 - acc: 0.7795 - val_loss: 10.2576 - val_acc: 0.3542\n",
      "Epoch 168/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4494 - acc: 0.7830 - val_loss: 10.2575 - val_acc: 0.3542\n",
      "Epoch 169/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4494 - acc: 0.7812 - val_loss: 10.2539 - val_acc: 0.3542\n",
      "Epoch 170/400\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.4491 - acc: 0.7778 - val_loss: 10.2531 - val_acc: 0.3542\n",
      "Epoch 171/400\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4491 - acc: 0.7812 - val_loss: 10.2483 - val_acc: 0.3542\n",
      "Epoch 172/400\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4488 - acc: 0.7812 - val_loss: 10.2391 - val_acc: 0.3542\n",
      "Epoch 173/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4487 - acc: 0.7812 - val_loss: 10.2332 - val_acc: 0.3542\n",
      "Epoch 174/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4487 - acc: 0.7812 - val_loss: 10.2242 - val_acc: 0.3542\n",
      "Epoch 175/400\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4484 - acc: 0.7795 - val_loss: 10.2152 - val_acc: 0.3542\n",
      "Epoch 176/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4483 - acc: 0.7795 - val_loss: 10.2060 - val_acc: 0.3542\n",
      "Epoch 177/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4482 - acc: 0.7795 - val_loss: 10.1925 - val_acc: 0.3542\n",
      "Epoch 178/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4481 - acc: 0.7812 - val_loss: 10.1825 - val_acc: 0.3542\n",
      "Epoch 179/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4479 - acc: 0.7812 - val_loss: 10.1770 - val_acc: 0.3594\n",
      "Epoch 180/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4478 - acc: 0.7795 - val_loss: 10.1753 - val_acc: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/400\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4475 - acc: 0.7795 - val_loss: 10.1746 - val_acc: 0.3594\n",
      "Epoch 182/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4477 - acc: 0.7795 - val_loss: 10.1754 - val_acc: 0.3594\n",
      "Epoch 183/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4475 - acc: 0.7743 - val_loss: 10.1752 - val_acc: 0.3594\n",
      "Epoch 184/400\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4475 - acc: 0.7760 - val_loss: 10.1751 - val_acc: 0.3594\n",
      "Epoch 185/400\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4469 - acc: 0.7760 - val_loss: 10.1754 - val_acc: 0.3594\n",
      "Epoch 186/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4469 - acc: 0.7760 - val_loss: 10.1754 - val_acc: 0.3594\n",
      "Epoch 187/400\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4467 - acc: 0.7743 - val_loss: 10.1757 - val_acc: 0.3594\n",
      "Epoch 188/400\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4467 - acc: 0.7778 - val_loss: 10.1756 - val_acc: 0.3594\n",
      "Epoch 189/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4465 - acc: 0.7743 - val_loss: 10.1757 - val_acc: 0.3594\n",
      "Epoch 190/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4463 - acc: 0.7743 - val_loss: 10.1762 - val_acc: 0.3594\n",
      "Epoch 191/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4462 - acc: 0.7760 - val_loss: 10.1764 - val_acc: 0.3594\n",
      "Epoch 192/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4459 - acc: 0.7743 - val_loss: 10.1761 - val_acc: 0.3594\n",
      "Epoch 193/400\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4458 - acc: 0.7760 - val_loss: 10.1765 - val_acc: 0.3594\n",
      "Epoch 194/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4458 - acc: 0.7743 - val_loss: 10.1765 - val_acc: 0.3594\n",
      "Epoch 195/400\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4456 - acc: 0.7743 - val_loss: 10.1758 - val_acc: 0.3594\n",
      "Epoch 196/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4454 - acc: 0.7726 - val_loss: 10.1759 - val_acc: 0.3594\n",
      "Epoch 197/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4452 - acc: 0.7760 - val_loss: 10.1767 - val_acc: 0.3594\n",
      "Epoch 198/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4450 - acc: 0.7743 - val_loss: 10.1768 - val_acc: 0.3594\n",
      "Epoch 199/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4451 - acc: 0.7760 - val_loss: 10.1768 - val_acc: 0.3594\n",
      "Epoch 200/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4448 - acc: 0.7778 - val_loss: 10.1771 - val_acc: 0.3594\n",
      "Epoch 201/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4446 - acc: 0.7760 - val_loss: 10.1781 - val_acc: 0.3594\n",
      "Epoch 202/400\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 10.1783 - val_acc: 0.3594\n",
      "Epoch 203/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 10.1783 - val_acc: 0.3594\n",
      "Epoch 204/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4444 - acc: 0.7743 - val_loss: 10.1782 - val_acc: 0.3594\n",
      "Epoch 205/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4443 - acc: 0.7760 - val_loss: 10.1780 - val_acc: 0.3594\n",
      "Epoch 206/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4443 - acc: 0.7778 - val_loss: 10.1778 - val_acc: 0.3594\n",
      "Epoch 207/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4439 - acc: 0.7760 - val_loss: 10.1781 - val_acc: 0.3594\n",
      "Epoch 208/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4441 - acc: 0.7743 - val_loss: 10.1774 - val_acc: 0.3594\n",
      "Epoch 209/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 10.1770 - val_acc: 0.3594\n",
      "Epoch 210/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4436 - acc: 0.7760 - val_loss: 10.1771 - val_acc: 0.3594\n",
      "Epoch 211/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4437 - acc: 0.7726 - val_loss: 10.1768 - val_acc: 0.3594\n",
      "Epoch 212/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4434 - acc: 0.7760 - val_loss: 10.1770 - val_acc: 0.3594\n",
      "Epoch 213/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4434 - acc: 0.7726 - val_loss: 10.1766 - val_acc: 0.3594\n",
      "Epoch 214/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4434 - acc: 0.7760 - val_loss: 10.1771 - val_acc: 0.3594\n",
      "Epoch 215/400\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4432 - acc: 0.7778 - val_loss: 10.1768 - val_acc: 0.3594\n",
      "Epoch 216/400\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4430 - acc: 0.7795 - val_loss: 10.1765 - val_acc: 0.3594\n",
      "Epoch 217/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4430 - acc: 0.7795 - val_loss: 10.1768 - val_acc: 0.3594\n",
      "Epoch 218/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4429 - acc: 0.7743 - val_loss: 10.1767 - val_acc: 0.3594\n",
      "Epoch 219/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4427 - acc: 0.7743 - val_loss: 10.1768 - val_acc: 0.3594\n",
      "Epoch 220/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4426 - acc: 0.7760 - val_loss: 10.1777 - val_acc: 0.3594\n",
      "Epoch 221/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 10.1782 - val_acc: 0.3594\n",
      "Epoch 222/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4423 - acc: 0.7795 - val_loss: 10.1778 - val_acc: 0.3594\n",
      "Epoch 223/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4421 - acc: 0.7743 - val_loss: 10.1778 - val_acc: 0.3594\n",
      "Epoch 224/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4422 - acc: 0.7743 - val_loss: 10.1783 - val_acc: 0.3594\n",
      "Epoch 225/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4419 - acc: 0.7778 - val_loss: 10.1782 - val_acc: 0.3594\n",
      "Epoch 226/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4419 - acc: 0.7726 - val_loss: 10.1779 - val_acc: 0.3594\n",
      "Epoch 227/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4419 - acc: 0.7743 - val_loss: 10.1781 - val_acc: 0.3594\n",
      "Epoch 228/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4417 - acc: 0.7726 - val_loss: 10.1779 - val_acc: 0.3594\n",
      "Epoch 229/400\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4416 - acc: 0.7760 - val_loss: 10.1773 - val_acc: 0.3594\n",
      "Epoch 230/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4414 - acc: 0.7760 - val_loss: 10.1775 - val_acc: 0.3594\n",
      "Epoch 231/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4413 - acc: 0.7778 - val_loss: 10.1773 - val_acc: 0.3594\n",
      "Epoch 232/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4412 - acc: 0.7760 - val_loss: 10.1779 - val_acc: 0.3594\n",
      "Epoch 233/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4411 - acc: 0.7760 - val_loss: 10.1784 - val_acc: 0.3594\n",
      "Epoch 234/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4409 - acc: 0.7743 - val_loss: 10.1784 - val_acc: 0.3594\n",
      "Epoch 235/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4409 - acc: 0.7760 - val_loss: 10.1792 - val_acc: 0.3594\n",
      "Epoch 236/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4406 - acc: 0.7743 - val_loss: 10.1787 - val_acc: 0.3594\n",
      "Epoch 237/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4406 - acc: 0.7760 - val_loss: 10.1787 - val_acc: 0.3594\n",
      "Epoch 238/400\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4404 - acc: 0.7726 - val_loss: 10.1786 - val_acc: 0.3594\n",
      "Epoch 239/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4404 - acc: 0.7760 - val_loss: 10.1779 - val_acc: 0.3594\n",
      "Epoch 240/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4401 - acc: 0.7743 - val_loss: 10.1774 - val_acc: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4401 - acc: 0.7726 - val_loss: 10.1778 - val_acc: 0.3594\n",
      "Epoch 242/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4398 - acc: 0.7778 - val_loss: 10.1773 - val_acc: 0.3594\n",
      "Epoch 243/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4398 - acc: 0.7743 - val_loss: 10.1770 - val_acc: 0.3594\n",
      "Epoch 244/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4395 - acc: 0.7778 - val_loss: 10.1773 - val_acc: 0.3594\n",
      "Epoch 245/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4393 - acc: 0.7743 - val_loss: 10.1773 - val_acc: 0.3594\n",
      "Epoch 246/400\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4394 - acc: 0.7743 - val_loss: 10.1775 - val_acc: 0.3594\n",
      "Epoch 247/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4393 - acc: 0.7743 - val_loss: 10.1779 - val_acc: 0.3594\n",
      "Epoch 248/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4390 - acc: 0.7760 - val_loss: 10.1772 - val_acc: 0.3594\n",
      "Epoch 249/400\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4389 - acc: 0.7743 - val_loss: 10.1773 - val_acc: 0.3594\n",
      "Epoch 250/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4389 - acc: 0.7743 - val_loss: 10.1778 - val_acc: 0.3594\n",
      "Epoch 251/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4387 - acc: 0.7778 - val_loss: 10.1830 - val_acc: 0.3542\n",
      "Epoch 252/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4385 - acc: 0.7760 - val_loss: 10.1927 - val_acc: 0.3542\n",
      "Epoch 253/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4383 - acc: 0.7760 - val_loss: 10.2035 - val_acc: 0.3542\n",
      "Epoch 254/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4381 - acc: 0.7795 - val_loss: 10.2140 - val_acc: 0.3542\n",
      "Epoch 255/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4380 - acc: 0.7760 - val_loss: 10.2264 - val_acc: 0.3542\n",
      "Epoch 256/400\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4378 - acc: 0.7812 - val_loss: 10.2323 - val_acc: 0.3542\n",
      "Epoch 257/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4375 - acc: 0.7795 - val_loss: 10.2404 - val_acc: 0.3542\n",
      "Epoch 258/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4374 - acc: 0.7795 - val_loss: 10.2463 - val_acc: 0.3542\n",
      "Epoch 259/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4374 - acc: 0.7812 - val_loss: 10.2567 - val_acc: 0.3542\n",
      "Epoch 260/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4371 - acc: 0.7795 - val_loss: 10.2623 - val_acc: 0.3542\n",
      "Epoch 261/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4368 - acc: 0.7812 - val_loss: 10.2621 - val_acc: 0.3542\n",
      "Epoch 262/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4367 - acc: 0.7812 - val_loss: 10.2620 - val_acc: 0.3542\n",
      "Epoch 263/400\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4367 - acc: 0.7812 - val_loss: 10.2618 - val_acc: 0.3542\n",
      "Epoch 264/400\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4363 - acc: 0.7795 - val_loss: 10.2616 - val_acc: 0.3542\n",
      "Epoch 265/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4364 - acc: 0.7795 - val_loss: 10.2616 - val_acc: 0.3542\n",
      "Epoch 266/400\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4361 - acc: 0.7795 - val_loss: 10.2616 - val_acc: 0.3542\n",
      "Epoch 267/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4357 - acc: 0.7795 - val_loss: 10.2617 - val_acc: 0.3542\n",
      "Epoch 268/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4359 - acc: 0.7847 - val_loss: 10.2616 - val_acc: 0.3542\n",
      "Epoch 269/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4357 - acc: 0.7899 - val_loss: 10.2619 - val_acc: 0.3542\n",
      "Epoch 270/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4354 - acc: 0.7847 - val_loss: 10.2613 - val_acc: 0.3542\n",
      "Epoch 271/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4355 - acc: 0.7830 - val_loss: 10.2610 - val_acc: 0.3542\n",
      "Epoch 272/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4354 - acc: 0.7812 - val_loss: 10.2605 - val_acc: 0.3542\n",
      "Epoch 273/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4351 - acc: 0.7847 - val_loss: 10.2606 - val_acc: 0.3542\n",
      "Epoch 274/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4349 - acc: 0.7882 - val_loss: 10.2604 - val_acc: 0.3542\n",
      "Epoch 275/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4349 - acc: 0.7830 - val_loss: 10.2602 - val_acc: 0.3542\n",
      "Epoch 276/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 10.2606 - val_acc: 0.3542\n",
      "Epoch 277/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4344 - acc: 0.7899 - val_loss: 10.2604 - val_acc: 0.3542\n",
      "Epoch 278/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4345 - acc: 0.7847 - val_loss: 10.2604 - val_acc: 0.3542\n",
      "Epoch 279/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4342 - acc: 0.7899 - val_loss: 10.2607 - val_acc: 0.3542\n",
      "Epoch 280/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4342 - acc: 0.7882 - val_loss: 10.2603 - val_acc: 0.3542\n",
      "Epoch 281/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4341 - acc: 0.7899 - val_loss: 10.2606 - val_acc: 0.3542\n",
      "Epoch 282/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 10.2605 - val_acc: 0.3542\n",
      "Epoch 283/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 10.2603 - val_acc: 0.3542\n",
      "Epoch 284/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4337 - acc: 0.7899 - val_loss: 10.2605 - val_acc: 0.3542\n",
      "Epoch 285/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4334 - acc: 0.7882 - val_loss: 10.2605 - val_acc: 0.3542\n",
      "Epoch 286/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 10.2609 - val_acc: 0.3542\n",
      "Epoch 287/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4335 - acc: 0.7882 - val_loss: 10.2617 - val_acc: 0.3542\n",
      "Epoch 288/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4333 - acc: 0.7882 - val_loss: 10.2617 - val_acc: 0.3542\n",
      "Epoch 289/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4332 - acc: 0.7899 - val_loss: 10.2614 - val_acc: 0.3542\n",
      "Epoch 290/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4330 - acc: 0.7917 - val_loss: 10.2613 - val_acc: 0.3542\n",
      "Epoch 291/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4328 - acc: 0.7899 - val_loss: 10.2613 - val_acc: 0.3542\n",
      "Epoch 292/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4328 - acc: 0.7899 - val_loss: 10.2614 - val_acc: 0.3542\n",
      "Epoch 293/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 10.2618 - val_acc: 0.3542\n",
      "Epoch 294/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 10.2620 - val_acc: 0.3542\n",
      "Epoch 295/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 10.2619 - val_acc: 0.3542\n",
      "Epoch 296/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4324 - acc: 0.7917 - val_loss: 10.2625 - val_acc: 0.3542\n",
      "Epoch 297/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4325 - acc: 0.7917 - val_loss: 10.2629 - val_acc: 0.3542\n",
      "Epoch 298/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4324 - acc: 0.7899 - val_loss: 10.2630 - val_acc: 0.3542\n",
      "Epoch 299/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4321 - acc: 0.7917 - val_loss: 10.2637 - val_acc: 0.3542\n",
      "Epoch 300/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4322 - acc: 0.7951 - val_loss: 10.2637 - val_acc: 0.3542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4320 - acc: 0.7899 - val_loss: 10.2644 - val_acc: 0.3542\n",
      "Epoch 302/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 10.2651 - val_acc: 0.3542\n",
      "Epoch 303/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4318 - acc: 0.7969 - val_loss: 10.2652 - val_acc: 0.3542\n",
      "Epoch 304/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 10.2652 - val_acc: 0.3542\n",
      "Epoch 305/400\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4315 - acc: 0.7934 - val_loss: 10.2653 - val_acc: 0.3542\n",
      "Epoch 306/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4315 - acc: 0.7917 - val_loss: 10.2660 - val_acc: 0.3542\n",
      "Epoch 307/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4314 - acc: 0.7917 - val_loss: 10.2665 - val_acc: 0.3542\n",
      "Epoch 308/400\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4312 - acc: 0.7899 - val_loss: 10.2668 - val_acc: 0.3542\n",
      "Epoch 309/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4311 - acc: 0.7951 - val_loss: 10.2674 - val_acc: 0.3542\n",
      "Epoch 310/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4311 - acc: 0.7899 - val_loss: 10.2674 - val_acc: 0.3542\n",
      "Epoch 311/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 10.2675 - val_acc: 0.3542\n",
      "Epoch 312/400\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4306 - acc: 0.7899 - val_loss: 10.2683 - val_acc: 0.3542\n",
      "Epoch 313/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4308 - acc: 0.7951 - val_loss: 10.2684 - val_acc: 0.3542\n",
      "Epoch 314/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4305 - acc: 0.7951 - val_loss: 10.2683 - val_acc: 0.3542\n",
      "Epoch 315/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4304 - acc: 0.7951 - val_loss: 10.2689 - val_acc: 0.3542\n",
      "Epoch 316/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4303 - acc: 0.7951 - val_loss: 10.2690 - val_acc: 0.3542\n",
      "Epoch 317/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4303 - acc: 0.7951 - val_loss: 10.2689 - val_acc: 0.3542\n",
      "Epoch 318/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4300 - acc: 0.7969 - val_loss: 10.2693 - val_acc: 0.3542\n",
      "Epoch 319/400\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 10.2695 - val_acc: 0.3542\n",
      "Epoch 320/400\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4299 - acc: 0.7969 - val_loss: 10.2700 - val_acc: 0.3542\n",
      "Epoch 321/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4298 - acc: 0.7951 - val_loss: 10.2702 - val_acc: 0.3542\n",
      "Epoch 322/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4298 - acc: 0.7951 - val_loss: 10.2700 - val_acc: 0.3542\n",
      "Epoch 323/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4297 - acc: 0.7969 - val_loss: 10.2706 - val_acc: 0.3542\n",
      "Epoch 324/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4295 - acc: 0.7934 - val_loss: 10.2706 - val_acc: 0.3542\n",
      "Epoch 325/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4296 - acc: 0.7934 - val_loss: 10.2709 - val_acc: 0.3542\n",
      "Epoch 326/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4293 - acc: 0.7951 - val_loss: 10.2709 - val_acc: 0.3542\n",
      "Epoch 327/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4293 - acc: 0.7951 - val_loss: 10.2708 - val_acc: 0.3542\n",
      "Epoch 328/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4292 - acc: 0.7969 - val_loss: 10.2713 - val_acc: 0.3542\n",
      "Epoch 329/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4295 - acc: 0.7951 - val_loss: 10.2711 - val_acc: 0.3542\n",
      "Epoch 330/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4291 - acc: 0.7969 - val_loss: 10.2716 - val_acc: 0.3542\n",
      "Epoch 331/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4290 - acc: 0.7969 - val_loss: 10.2727 - val_acc: 0.3542\n",
      "Epoch 332/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4290 - acc: 0.7934 - val_loss: 10.2735 - val_acc: 0.3542\n",
      "Epoch 333/400\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4290 - acc: 0.7951 - val_loss: 10.2740 - val_acc: 0.3542\n",
      "Epoch 334/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 10.2735 - val_acc: 0.3542\n",
      "Epoch 335/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4287 - acc: 0.7951 - val_loss: 10.2733 - val_acc: 0.3542\n",
      "Epoch 336/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4285 - acc: 0.7899 - val_loss: 10.2739 - val_acc: 0.3542\n",
      "Epoch 337/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 10.2740 - val_acc: 0.3542\n",
      "Epoch 338/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4286 - acc: 0.7934 - val_loss: 10.2758 - val_acc: 0.3542\n",
      "Epoch 339/400\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 10.2741 - val_acc: 0.3542\n",
      "Epoch 340/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4282 - acc: 0.7899 - val_loss: 10.2765 - val_acc: 0.3542\n",
      "Epoch 341/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4283 - acc: 0.7969 - val_loss: 10.2814 - val_acc: 0.3490\n",
      "Epoch 342/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 10.2808 - val_acc: 0.3490\n",
      "Epoch 343/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4280 - acc: 0.7934 - val_loss: 10.2805 - val_acc: 0.3490\n",
      "Epoch 344/400\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 10.2789 - val_acc: 0.3490\n",
      "Epoch 345/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4279 - acc: 0.7969 - val_loss: 10.2832 - val_acc: 0.3490\n",
      "Epoch 346/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4277 - acc: 0.7951 - val_loss: 10.2865 - val_acc: 0.3490\n",
      "Epoch 347/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4277 - acc: 0.7951 - val_loss: 10.2881 - val_acc: 0.3490\n",
      "Epoch 348/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4276 - acc: 0.7934 - val_loss: 10.2893 - val_acc: 0.3490\n",
      "Epoch 349/400\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4275 - acc: 0.7951 - val_loss: 10.2942 - val_acc: 0.3490\n",
      "Epoch 350/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4274 - acc: 0.7917 - val_loss: 10.2897 - val_acc: 0.3490\n",
      "Epoch 351/400\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4274 - acc: 0.7951 - val_loss: 10.2916 - val_acc: 0.3490\n",
      "Epoch 352/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4272 - acc: 0.7969 - val_loss: 10.2989 - val_acc: 0.3490\n",
      "Epoch 353/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4272 - acc: 0.7934 - val_loss: 10.3027 - val_acc: 0.3490\n",
      "Epoch 354/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4273 - acc: 0.7969 - val_loss: 10.3035 - val_acc: 0.3490\n",
      "Epoch 355/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4269 - acc: 0.7934 - val_loss: 10.3012 - val_acc: 0.3490\n",
      "Epoch 356/400\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4271 - acc: 0.7969 - val_loss: 10.3079 - val_acc: 0.3490\n",
      "Epoch 357/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4270 - acc: 0.7969 - val_loss: 10.3121 - val_acc: 0.3490\n",
      "Epoch 358/400\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4268 - acc: 0.7969 - val_loss: 10.3116 - val_acc: 0.3490\n",
      "Epoch 359/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4268 - acc: 0.7969 - val_loss: 10.3180 - val_acc: 0.3490\n",
      "Epoch 360/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4267 - acc: 0.7934 - val_loss: 10.3129 - val_acc: 0.3490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/400\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4267 - acc: 0.7969 - val_loss: 10.3180 - val_acc: 0.3490\n",
      "Epoch 362/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4265 - acc: 0.7969 - val_loss: 10.3230 - val_acc: 0.3490\n",
      "Epoch 363/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4265 - acc: 0.7986 - val_loss: 10.3299 - val_acc: 0.3490\n",
      "Epoch 364/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4265 - acc: 0.7969 - val_loss: 10.3331 - val_acc: 0.3490\n",
      "Epoch 365/400\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4262 - acc: 0.7934 - val_loss: 10.3341 - val_acc: 0.3490\n",
      "Epoch 366/400\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4263 - acc: 0.7969 - val_loss: 10.3322 - val_acc: 0.3490\n",
      "Epoch 367/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4262 - acc: 0.7951 - val_loss: 10.3301 - val_acc: 0.3490\n",
      "Epoch 368/400\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4262 - acc: 0.7969 - val_loss: 10.3331 - val_acc: 0.3490\n",
      "Epoch 369/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4261 - acc: 0.7951 - val_loss: 10.3331 - val_acc: 0.3490\n",
      "Epoch 370/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4261 - acc: 0.7951 - val_loss: 10.3352 - val_acc: 0.3490\n",
      "Epoch 371/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4262 - acc: 0.7951 - val_loss: 10.3334 - val_acc: 0.3490\n",
      "Epoch 372/400\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4260 - acc: 0.7969 - val_loss: 10.3387 - val_acc: 0.3490\n",
      "Epoch 373/400\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4260 - acc: 0.7986 - val_loss: 10.3437 - val_acc: 0.3490\n",
      "Epoch 374/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4257 - acc: 0.7986 - val_loss: 10.3435 - val_acc: 0.3490\n",
      "Epoch 375/400\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4257 - acc: 0.7986 - val_loss: 10.3469 - val_acc: 0.3490\n",
      "Epoch 376/400\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4255 - acc: 0.7986 - val_loss: 10.3438 - val_acc: 0.3490\n",
      "Epoch 377/400\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4256 - acc: 0.7986 - val_loss: 10.3448 - val_acc: 0.3490\n",
      "Epoch 378/400\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4256 - acc: 0.7986 - val_loss: 10.3493 - val_acc: 0.3490\n",
      "Epoch 379/400\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4255 - acc: 0.7986 - val_loss: 10.3487 - val_acc: 0.3490\n",
      "Epoch 380/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4254 - acc: 0.7986 - val_loss: 10.3446 - val_acc: 0.3490\n",
      "Epoch 381/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4253 - acc: 0.7986 - val_loss: 10.3493 - val_acc: 0.3490\n",
      "Epoch 382/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4253 - acc: 0.7986 - val_loss: 10.3581 - val_acc: 0.3490\n",
      "Epoch 383/400\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4252 - acc: 0.7986 - val_loss: 10.3544 - val_acc: 0.3490\n",
      "Epoch 384/400\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4251 - acc: 0.8003 - val_loss: 10.3599 - val_acc: 0.3490\n",
      "Epoch 385/400\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4250 - acc: 0.7986 - val_loss: 10.3606 - val_acc: 0.3490\n",
      "Epoch 386/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4250 - acc: 0.7986 - val_loss: 10.3607 - val_acc: 0.3490\n",
      "Epoch 387/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4248 - acc: 0.7986 - val_loss: 10.3602 - val_acc: 0.3490\n",
      "Epoch 388/400\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4250 - acc: 0.7986 - val_loss: 10.3624 - val_acc: 0.3490\n",
      "Epoch 389/400\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4247 - acc: 0.7986 - val_loss: 10.3623 - val_acc: 0.3490\n",
      "Epoch 390/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4245 - acc: 0.7986 - val_loss: 10.3620 - val_acc: 0.3490\n",
      "Epoch 391/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4247 - acc: 0.7986 - val_loss: 10.3646 - val_acc: 0.3438\n",
      "Epoch 392/400\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4248 - acc: 0.7986 - val_loss: 10.3678 - val_acc: 0.3438\n",
      "Epoch 393/400\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4244 - acc: 0.7986 - val_loss: 10.3652 - val_acc: 0.3438\n",
      "Epoch 394/400\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4243 - acc: 0.7986 - val_loss: 10.3670 - val_acc: 0.3438\n",
      "Epoch 395/400\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4243 - acc: 0.8003 - val_loss: 10.3670 - val_acc: 0.3438\n",
      "Epoch 396/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4243 - acc: 0.8021 - val_loss: 10.3631 - val_acc: 0.3490\n",
      "Epoch 397/400\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4242 - acc: 0.7986 - val_loss: 10.3637 - val_acc: 0.3490\n",
      "Epoch 398/400\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4242 - acc: 0.8003 - val_loss: 10.3651 - val_acc: 0.3490\n",
      "Epoch 399/400\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4240 - acc: 0.8003 - val_loss: 10.3677 - val_acc: 0.3438\n",
      "Epoch 400/400\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4240 - acc: 0.8003 - val_loss: 10.3664 - val_acc: 0.3490\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(SGD(lr=0.01),'binary_crossentropy',metrics=['accuracy'])\n",
    "model_2_hist = model_2.fit(X_train_norm, y_train, validation_data=(X_test, y_test), epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4241 - acc: 0.8003 - val_loss: 0.5212 - val_acc: 0.7188\n",
      "Epoch 2/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4239 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7188\n",
      "Epoch 3/1500\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4238 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7188\n",
      "Epoch 4/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4239 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7188\n",
      "Epoch 5/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4238 - acc: 0.8003 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 6/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4237 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 7/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4237 - acc: 0.8003 - val_loss: 0.5211 - val_acc: 0.7240\n",
      "Epoch 8/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4236 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 9/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4235 - acc: 0.8021 - val_loss: 0.5214 - val_acc: 0.7240\n",
      "Epoch 10/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4234 - acc: 0.8003 - val_loss: 0.5214 - val_acc: 0.7240\n",
      "Epoch 11/1500\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7240\n",
      "Epoch 12/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4232 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 13/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4232 - acc: 0.8003 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 14/1500\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.4233 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 15/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4231 - acc: 0.8021 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 16/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4234 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 17/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4230 - acc: 0.8021 - val_loss: 0.5211 - val_acc: 0.7240\n",
      "Epoch 18/1500\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4231 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 19/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4231 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7240\n",
      "Epoch 20/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 21/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5214 - val_acc: 0.7240\n",
      "Epoch 22/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4227 - acc: 0.8003 - val_loss: 0.5214 - val_acc: 0.7240\n",
      "Epoch 23/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 24/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 25/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7240\n",
      "Epoch 26/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4225 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7240\n",
      "Epoch 27/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7240\n",
      "Epoch 28/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7240\n",
      "Epoch 29/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5207 - val_acc: 0.7240\n",
      "Epoch 30/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7240\n",
      "Epoch 31/1500\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.4223 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7240\n",
      "Epoch 32/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4221 - acc: 0.8021 - val_loss: 0.5211 - val_acc: 0.7240\n",
      "Epoch 33/1500\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4221 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7240\n",
      "Epoch 34/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4223 - acc: 0.8021 - val_loss: 0.5212 - val_acc: 0.7240\n",
      "Epoch 35/1500\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4219 - acc: 0.8056 - val_loss: 0.5209 - val_acc: 0.7240\n",
      "Epoch 36/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7240\n",
      "Epoch 37/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7240\n",
      "Epoch 38/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4218 - acc: 0.8003 - val_loss: 0.5206 - val_acc: 0.7240\n",
      "Epoch 39/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4215 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7240\n",
      "Epoch 40/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7240\n",
      "Epoch 41/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4216 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7240\n",
      "Epoch 42/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5207 - val_acc: 0.7240\n",
      "Epoch 43/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4214 - acc: 0.8056 - val_loss: 0.5206 - val_acc: 0.7240\n",
      "Epoch 44/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5204 - val_acc: 0.7240\n",
      "Epoch 45/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5204 - val_acc: 0.7240\n",
      "Epoch 46/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5203 - val_acc: 0.7240\n",
      "Epoch 47/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5201 - val_acc: 0.7240\n",
      "Epoch 48/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4209 - acc: 0.8021 - val_loss: 0.5200 - val_acc: 0.7188\n",
      "Epoch 49/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4206 - acc: 0.8038 - val_loss: 0.5200 - val_acc: 0.7188\n",
      "Epoch 50/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4208 - acc: 0.8038 - val_loss: 0.5203 - val_acc: 0.7188\n",
      "Epoch 51/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4206 - acc: 0.8056 - val_loss: 0.5204 - val_acc: 0.7188\n",
      "Epoch 52/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4202 - acc: 0.8021 - val_loss: 0.5202 - val_acc: 0.7188\n",
      "Epoch 53/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4204 - acc: 0.8003 - val_loss: 0.5202 - val_acc: 0.7188\n",
      "Epoch 54/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4203 - acc: 0.8038 - val_loss: 0.5201 - val_acc: 0.7188\n",
      "Epoch 55/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4201 - acc: 0.8021 - val_loss: 0.5201 - val_acc: 0.7240\n",
      "Epoch 56/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4198 - acc: 0.8003 - val_loss: 0.5199 - val_acc: 0.7188\n",
      "Epoch 57/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4201 - acc: 0.8003 - val_loss: 0.5197 - val_acc: 0.7240\n",
      "Epoch 58/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4197 - acc: 0.8003 - val_loss: 0.5197 - val_acc: 0.7240\n",
      "Epoch 59/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4198 - acc: 0.8021 - val_loss: 0.5199 - val_acc: 0.7240\n",
      "Epoch 60/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4198 - acc: 0.8021 - val_loss: 0.5200 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4196 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7240\n",
      "Epoch 62/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4195 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7240\n",
      "Epoch 63/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4194 - acc: 0.7969 - val_loss: 0.5199 - val_acc: 0.7240\n",
      "Epoch 64/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4191 - acc: 0.7986 - val_loss: 0.5197 - val_acc: 0.7240\n",
      "Epoch 65/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4191 - acc: 0.7986 - val_loss: 0.5198 - val_acc: 0.7240\n",
      "Epoch 66/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4189 - acc: 0.7969 - val_loss: 0.5198 - val_acc: 0.7292\n",
      "Epoch 67/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4187 - acc: 0.7951 - val_loss: 0.5195 - val_acc: 0.7292\n",
      "Epoch 68/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4186 - acc: 0.8003 - val_loss: 0.5197 - val_acc: 0.7292\n",
      "Epoch 69/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4185 - acc: 0.7969 - val_loss: 0.5196 - val_acc: 0.7292\n",
      "Epoch 70/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4184 - acc: 0.7969 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 71/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4184 - acc: 0.8003 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 72/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4181 - acc: 0.7969 - val_loss: 0.5193 - val_acc: 0.7292\n",
      "Epoch 73/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4183 - acc: 0.7951 - val_loss: 0.5192 - val_acc: 0.7292\n",
      "Epoch 74/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4178 - acc: 0.7969 - val_loss: 0.5192 - val_acc: 0.7292\n",
      "Epoch 75/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4178 - acc: 0.7951 - val_loss: 0.5191 - val_acc: 0.7292\n",
      "Epoch 76/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4177 - acc: 0.7986 - val_loss: 0.5190 - val_acc: 0.7292\n",
      "Epoch 77/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4178 - acc: 0.7986 - val_loss: 0.5191 - val_acc: 0.7292\n",
      "Epoch 78/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4174 - acc: 0.7986 - val_loss: 0.5193 - val_acc: 0.7292\n",
      "Epoch 79/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4176 - acc: 0.7986 - val_loss: 0.5191 - val_acc: 0.7292\n",
      "Epoch 80/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4174 - acc: 0.7986 - val_loss: 0.5191 - val_acc: 0.7240\n",
      "Epoch 81/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4172 - acc: 0.7986 - val_loss: 0.5191 - val_acc: 0.7240\n",
      "Epoch 82/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4171 - acc: 0.7986 - val_loss: 0.5193 - val_acc: 0.7240\n",
      "Epoch 83/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4171 - acc: 0.7986 - val_loss: 0.5192 - val_acc: 0.7240\n",
      "Epoch 84/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4170 - acc: 0.7951 - val_loss: 0.5192 - val_acc: 0.7240\n",
      "Epoch 85/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4166 - acc: 0.7969 - val_loss: 0.5190 - val_acc: 0.7240\n",
      "Epoch 86/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4166 - acc: 0.7934 - val_loss: 0.5190 - val_acc: 0.7240\n",
      "Epoch 87/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4166 - acc: 0.7986 - val_loss: 0.5189 - val_acc: 0.7240\n",
      "Epoch 88/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4164 - acc: 0.7934 - val_loss: 0.5190 - val_acc: 0.7240\n",
      "Epoch 89/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4163 - acc: 0.7951 - val_loss: 0.5192 - val_acc: 0.7240\n",
      "Epoch 90/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4162 - acc: 0.7951 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 91/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4161 - acc: 0.7951 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 92/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4162 - acc: 0.7951 - val_loss: 0.5193 - val_acc: 0.7240\n",
      "Epoch 93/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4158 - acc: 0.7969 - val_loss: 0.5194 - val_acc: 0.7240\n",
      "Epoch 94/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4159 - acc: 0.7969 - val_loss: 0.5197 - val_acc: 0.7292\n",
      "Epoch 95/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4158 - acc: 0.7951 - val_loss: 0.5195 - val_acc: 0.7292\n",
      "Epoch 96/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4155 - acc: 0.7951 - val_loss: 0.5196 - val_acc: 0.7292\n",
      "Epoch 97/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4154 - acc: 0.7969 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 98/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4152 - acc: 0.7969 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 99/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4152 - acc: 0.7969 - val_loss: 0.5194 - val_acc: 0.7292\n",
      "Epoch 100/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4151 - acc: 0.7969 - val_loss: 0.5196 - val_acc: 0.7292\n",
      "Epoch 101/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4151 - acc: 0.7969 - val_loss: 0.5198 - val_acc: 0.7292\n",
      "Epoch 102/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4150 - acc: 0.7951 - val_loss: 0.5198 - val_acc: 0.7344\n",
      "Epoch 103/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4147 - acc: 0.7969 - val_loss: 0.5198 - val_acc: 0.7344\n",
      "Epoch 104/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4146 - acc: 0.7969 - val_loss: 0.5198 - val_acc: 0.7344\n",
      "Epoch 105/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4146 - acc: 0.7986 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 106/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4146 - acc: 0.7986 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 107/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4144 - acc: 0.7986 - val_loss: 0.5198 - val_acc: 0.7344\n",
      "Epoch 108/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4143 - acc: 0.7986 - val_loss: 0.5198 - val_acc: 0.7344\n",
      "Epoch 109/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4143 - acc: 0.8003 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 110/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4141 - acc: 0.7986 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 111/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4140 - acc: 0.7986 - val_loss: 0.5196 - val_acc: 0.7292\n",
      "Epoch 112/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4138 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7344\n",
      "Epoch 113/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4138 - acc: 0.8003 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 114/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4138 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 115/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4137 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 116/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4135 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 117/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4133 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 118/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4133 - acc: 0.8003 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 119/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4132 - acc: 0.8003 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 120/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4131 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 121/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 58us/step - loss: 0.4131 - acc: 0.8021 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 122/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4131 - acc: 0.8021 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 123/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4127 - acc: 0.8003 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 124/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4128 - acc: 0.8021 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 125/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4128 - acc: 0.7986 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 126/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4128 - acc: 0.8038 - val_loss: 0.5201 - val_acc: 0.7292\n",
      "Epoch 127/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4125 - acc: 0.8038 - val_loss: 0.5200 - val_acc: 0.7292\n",
      "Epoch 128/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4125 - acc: 0.8056 - val_loss: 0.5201 - val_acc: 0.7292\n",
      "Epoch 129/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4124 - acc: 0.8021 - val_loss: 0.5201 - val_acc: 0.7292\n",
      "Epoch 130/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4123 - acc: 0.8038 - val_loss: 0.5204 - val_acc: 0.7292\n",
      "Epoch 131/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4123 - acc: 0.8038 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 132/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4122 - acc: 0.8073 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 133/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4123 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 134/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4122 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 135/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4121 - acc: 0.8056 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 136/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4117 - acc: 0.8021 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 137/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4119 - acc: 0.8073 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 138/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4117 - acc: 0.8073 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 139/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4116 - acc: 0.8056 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 140/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4113 - acc: 0.8056 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 141/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4113 - acc: 0.8073 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 142/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4113 - acc: 0.8090 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 143/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4110 - acc: 0.8056 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 144/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4110 - acc: 0.8073 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 145/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4111 - acc: 0.8073 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 146/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4111 - acc: 0.8056 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 147/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4109 - acc: 0.8090 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 148/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4106 - acc: 0.8090 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 149/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4106 - acc: 0.8108 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 150/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4105 - acc: 0.8090 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 151/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4108 - acc: 0.8090 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 152/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4105 - acc: 0.8090 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 153/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4103 - acc: 0.8090 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 154/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4102 - acc: 0.8108 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 155/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4101 - acc: 0.8090 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 156/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4101 - acc: 0.8108 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 157/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4101 - acc: 0.8090 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 158/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4099 - acc: 0.8090 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 159/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4099 - acc: 0.8108 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 160/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4098 - acc: 0.8108 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 161/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4097 - acc: 0.8073 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 162/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4098 - acc: 0.8108 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 163/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4095 - acc: 0.8108 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 164/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4095 - acc: 0.8090 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 165/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4097 - acc: 0.8108 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 166/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4093 - acc: 0.8090 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 167/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4092 - acc: 0.8108 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 168/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4093 - acc: 0.8108 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 169/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4092 - acc: 0.8108 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 170/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4091 - acc: 0.8108 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 171/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4090 - acc: 0.8108 - val_loss: 0.5204 - val_acc: 0.7344\n",
      "Epoch 172/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4091 - acc: 0.8108 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 173/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4090 - acc: 0.8142 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 174/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4090 - acc: 0.8090 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 175/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4090 - acc: 0.8125 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 176/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4088 - acc: 0.8125 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 177/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4086 - acc: 0.8108 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 178/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4084 - acc: 0.8125 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 179/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4084 - acc: 0.8125 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 180/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4084 - acc: 0.8142 - val_loss: 0.5199 - val_acc: 0.7344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4083 - acc: 0.8125 - val_loss: 0.5200 - val_acc: 0.7344\n",
      "Epoch 182/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4082 - acc: 0.8125 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 183/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4082 - acc: 0.8125 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 184/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4083 - acc: 0.8142 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 185/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4081 - acc: 0.8142 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 186/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4078 - acc: 0.8125 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 187/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4077 - acc: 0.8142 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 188/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4078 - acc: 0.8142 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 189/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4078 - acc: 0.8142 - val_loss: 0.5201 - val_acc: 0.7396\n",
      "Epoch 190/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4076 - acc: 0.8142 - val_loss: 0.5202 - val_acc: 0.7396\n",
      "Epoch 191/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4075 - acc: 0.8125 - val_loss: 0.5202 - val_acc: 0.7396\n",
      "Epoch 192/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4074 - acc: 0.8160 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 193/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4073 - acc: 0.8142 - val_loss: 0.5199 - val_acc: 0.7396\n",
      "Epoch 194/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4076 - acc: 0.8142 - val_loss: 0.5201 - val_acc: 0.7396\n",
      "Epoch 195/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4074 - acc: 0.8125 - val_loss: 0.5201 - val_acc: 0.7396\n",
      "Epoch 196/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4072 - acc: 0.8142 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 197/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4071 - acc: 0.8142 - val_loss: 0.5198 - val_acc: 0.7344\n",
      "Epoch 198/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4070 - acc: 0.8142 - val_loss: 0.5200 - val_acc: 0.7344\n",
      "Epoch 199/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4070 - acc: 0.8194 - val_loss: 0.5200 - val_acc: 0.7344\n",
      "Epoch 200/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4069 - acc: 0.8194 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 201/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4071 - acc: 0.8160 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 202/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4068 - acc: 0.8160 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 203/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4066 - acc: 0.8194 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 204/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4069 - acc: 0.8194 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 205/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4067 - acc: 0.8177 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 206/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4069 - acc: 0.8177 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 207/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4066 - acc: 0.8194 - val_loss: 0.5202 - val_acc: 0.7344\n",
      "Epoch 208/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4066 - acc: 0.8177 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 209/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4064 - acc: 0.8194 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 210/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4064 - acc: 0.8194 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 211/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4064 - acc: 0.8160 - val_loss: 0.5200 - val_acc: 0.7344\n",
      "Epoch 212/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4064 - acc: 0.8194 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 213/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4064 - acc: 0.8194 - val_loss: 0.5197 - val_acc: 0.7344\n",
      "Epoch 214/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4064 - acc: 0.8194 - val_loss: 0.5195 - val_acc: 0.7344\n",
      "Epoch 215/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4062 - acc: 0.8194 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 216/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4061 - acc: 0.8177 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 217/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4059 - acc: 0.8247 - val_loss: 0.5200 - val_acc: 0.7344\n",
      "Epoch 218/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4060 - acc: 0.8212 - val_loss: 0.5201 - val_acc: 0.7344\n",
      "Epoch 219/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4059 - acc: 0.8212 - val_loss: 0.5199 - val_acc: 0.7344\n",
      "Epoch 220/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4059 - acc: 0.8212 - val_loss: 0.5203 - val_acc: 0.7344\n",
      "Epoch 221/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4059 - acc: 0.8177 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 222/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4060 - acc: 0.8212 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 223/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4059 - acc: 0.8212 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 224/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4056 - acc: 0.8212 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 225/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4056 - acc: 0.8212 - val_loss: 0.5209 - val_acc: 0.7344\n",
      "Epoch 226/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4056 - acc: 0.8212 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 227/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4056 - acc: 0.8229 - val_loss: 0.5205 - val_acc: 0.7344\n",
      "Epoch 228/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4054 - acc: 0.8212 - val_loss: 0.5206 - val_acc: 0.7344\n",
      "Epoch 229/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4054 - acc: 0.8212 - val_loss: 0.5209 - val_acc: 0.7344\n",
      "Epoch 230/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4053 - acc: 0.8229 - val_loss: 0.5209 - val_acc: 0.7344\n",
      "Epoch 231/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4054 - acc: 0.8212 - val_loss: 0.5207 - val_acc: 0.7344\n",
      "Epoch 232/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4052 - acc: 0.8194 - val_loss: 0.5206 - val_acc: 0.7396\n",
      "Epoch 233/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4051 - acc: 0.8247 - val_loss: 0.5205 - val_acc: 0.7396\n",
      "Epoch 234/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4051 - acc: 0.8194 - val_loss: 0.5205 - val_acc: 0.7396\n",
      "Epoch 235/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4052 - acc: 0.8229 - val_loss: 0.5204 - val_acc: 0.7396\n",
      "Epoch 236/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4051 - acc: 0.8229 - val_loss: 0.5205 - val_acc: 0.7396\n",
      "Epoch 237/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4049 - acc: 0.8247 - val_loss: 0.5204 - val_acc: 0.7396\n",
      "Epoch 238/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4049 - acc: 0.8229 - val_loss: 0.5204 - val_acc: 0.7396\n",
      "Epoch 239/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4052 - acc: 0.8229 - val_loss: 0.5206 - val_acc: 0.7396\n",
      "Epoch 240/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4047 - acc: 0.8212 - val_loss: 0.5210 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4048 - acc: 0.8247 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 242/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4050 - acc: 0.8212 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 243/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4046 - acc: 0.8212 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 244/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4046 - acc: 0.8194 - val_loss: 0.5213 - val_acc: 0.7396\n",
      "Epoch 245/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4046 - acc: 0.8212 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 246/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4046 - acc: 0.8229 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 247/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4048 - acc: 0.8212 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 248/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4045 - acc: 0.8194 - val_loss: 0.5216 - val_acc: 0.7396\n",
      "Epoch 249/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4045 - acc: 0.8212 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 250/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4041 - acc: 0.8212 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 251/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4045 - acc: 0.8229 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 252/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4043 - acc: 0.8212 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 253/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4041 - acc: 0.8212 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 254/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4042 - acc: 0.8229 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 255/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4044 - acc: 0.8229 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 256/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4041 - acc: 0.8212 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 257/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4040 - acc: 0.8229 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 258/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4038 - acc: 0.8177 - val_loss: 0.5218 - val_acc: 0.7500\n",
      "Epoch 259/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4040 - acc: 0.8212 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 260/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4039 - acc: 0.8229 - val_loss: 0.5223 - val_acc: 0.7552\n",
      "Epoch 261/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4041 - acc: 0.8212 - val_loss: 0.5224 - val_acc: 0.7552\n",
      "Epoch 262/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4038 - acc: 0.8229 - val_loss: 0.5227 - val_acc: 0.7552\n",
      "Epoch 263/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4039 - acc: 0.8229 - val_loss: 0.5227 - val_acc: 0.7552\n",
      "Epoch 264/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4037 - acc: 0.8229 - val_loss: 0.5227 - val_acc: 0.7552\n",
      "Epoch 265/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4035 - acc: 0.8212 - val_loss: 0.5228 - val_acc: 0.7552\n",
      "Epoch 266/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4037 - acc: 0.8194 - val_loss: 0.5227 - val_acc: 0.7552\n",
      "Epoch 267/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4036 - acc: 0.8177 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 268/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4034 - acc: 0.8212 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 269/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4036 - acc: 0.8194 - val_loss: 0.5231 - val_acc: 0.7500\n",
      "Epoch 270/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4038 - acc: 0.8212 - val_loss: 0.5231 - val_acc: 0.7500\n",
      "Epoch 271/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4035 - acc: 0.8229 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 272/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4034 - acc: 0.8229 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 273/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4034 - acc: 0.8229 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 274/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4032 - acc: 0.8194 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 275/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4033 - acc: 0.8194 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 276/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4032 - acc: 0.8194 - val_loss: 0.5230 - val_acc: 0.7500\n",
      "Epoch 277/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4031 - acc: 0.8229 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 278/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4034 - acc: 0.8212 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 279/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4030 - acc: 0.8212 - val_loss: 0.5231 - val_acc: 0.7500\n",
      "Epoch 280/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4032 - acc: 0.8212 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 281/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4030 - acc: 0.8194 - val_loss: 0.5231 - val_acc: 0.7500\n",
      "Epoch 282/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4029 - acc: 0.8212 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 283/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4031 - acc: 0.8212 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 284/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4029 - acc: 0.8229 - val_loss: 0.5230 - val_acc: 0.7500\n",
      "Epoch 285/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4029 - acc: 0.8212 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 286/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4027 - acc: 0.8229 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 287/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4027 - acc: 0.8212 - val_loss: 0.5233 - val_acc: 0.7448\n",
      "Epoch 288/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4027 - acc: 0.8212 - val_loss: 0.5234 - val_acc: 0.7500\n",
      "Epoch 289/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4025 - acc: 0.8194 - val_loss: 0.5236 - val_acc: 0.7500\n",
      "Epoch 290/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4027 - acc: 0.8194 - val_loss: 0.5236 - val_acc: 0.7500\n",
      "Epoch 291/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4026 - acc: 0.8229 - val_loss: 0.5236 - val_acc: 0.7500\n",
      "Epoch 292/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4023 - acc: 0.8212 - val_loss: 0.5235 - val_acc: 0.7500\n",
      "Epoch 293/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4024 - acc: 0.8212 - val_loss: 0.5238 - val_acc: 0.7500\n",
      "Epoch 294/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4025 - acc: 0.8229 - val_loss: 0.5236 - val_acc: 0.7448\n",
      "Epoch 295/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4024 - acc: 0.8229 - val_loss: 0.5237 - val_acc: 0.7500\n",
      "Epoch 296/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4021 - acc: 0.8212 - val_loss: 0.5238 - val_acc: 0.7500\n",
      "Epoch 297/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4024 - acc: 0.8229 - val_loss: 0.5239 - val_acc: 0.7500\n",
      "Epoch 298/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4025 - acc: 0.8212 - val_loss: 0.5242 - val_acc: 0.7552\n",
      "Epoch 299/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4022 - acc: 0.8194 - val_loss: 0.5238 - val_acc: 0.7448\n",
      "Epoch 300/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4022 - acc: 0.8194 - val_loss: 0.5238 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4021 - acc: 0.8229 - val_loss: 0.5239 - val_acc: 0.7448\n",
      "Epoch 302/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4020 - acc: 0.8194 - val_loss: 0.5241 - val_acc: 0.7448\n",
      "Epoch 303/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4021 - acc: 0.8212 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 304/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4020 - acc: 0.8229 - val_loss: 0.5246 - val_acc: 0.7448\n",
      "Epoch 305/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4019 - acc: 0.8229 - val_loss: 0.5245 - val_acc: 0.7448\n",
      "Epoch 306/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4019 - acc: 0.8212 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 307/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4019 - acc: 0.8194 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 308/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4022 - acc: 0.8212 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 309/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4017 - acc: 0.8212 - val_loss: 0.5253 - val_acc: 0.7500\n",
      "Epoch 310/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4018 - acc: 0.8194 - val_loss: 0.5255 - val_acc: 0.7552\n",
      "Epoch 311/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4016 - acc: 0.8229 - val_loss: 0.5257 - val_acc: 0.7552\n",
      "Epoch 312/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4016 - acc: 0.8212 - val_loss: 0.5257 - val_acc: 0.7552\n",
      "Epoch 313/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4017 - acc: 0.8212 - val_loss: 0.5258 - val_acc: 0.7552\n",
      "Epoch 314/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4016 - acc: 0.8212 - val_loss: 0.5258 - val_acc: 0.7552\n",
      "Epoch 315/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4015 - acc: 0.8229 - val_loss: 0.5257 - val_acc: 0.7552\n",
      "Epoch 316/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4016 - acc: 0.8247 - val_loss: 0.5258 - val_acc: 0.7552\n",
      "Epoch 317/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4014 - acc: 0.8194 - val_loss: 0.5259 - val_acc: 0.7552\n",
      "Epoch 318/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4014 - acc: 0.8212 - val_loss: 0.5259 - val_acc: 0.7552\n",
      "Epoch 319/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4016 - acc: 0.8212 - val_loss: 0.5259 - val_acc: 0.7552\n",
      "Epoch 320/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4012 - acc: 0.8229 - val_loss: 0.5262 - val_acc: 0.7552\n",
      "Epoch 321/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4013 - acc: 0.8212 - val_loss: 0.5264 - val_acc: 0.7552\n",
      "Epoch 322/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4012 - acc: 0.8177 - val_loss: 0.5265 - val_acc: 0.7552\n",
      "Epoch 323/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4013 - acc: 0.8212 - val_loss: 0.5262 - val_acc: 0.7552\n",
      "Epoch 324/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4011 - acc: 0.8194 - val_loss: 0.5266 - val_acc: 0.7552\n",
      "Epoch 325/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4013 - acc: 0.8247 - val_loss: 0.5265 - val_acc: 0.7552\n",
      "Epoch 326/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4011 - acc: 0.8212 - val_loss: 0.5265 - val_acc: 0.7552\n",
      "Epoch 327/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4012 - acc: 0.8212 - val_loss: 0.5265 - val_acc: 0.7552\n",
      "Epoch 328/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4010 - acc: 0.8229 - val_loss: 0.5267 - val_acc: 0.7552\n",
      "Epoch 329/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4009 - acc: 0.8177 - val_loss: 0.5273 - val_acc: 0.7552\n",
      "Epoch 330/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4007 - acc: 0.8212 - val_loss: 0.5273 - val_acc: 0.7552\n",
      "Epoch 331/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4009 - acc: 0.8212 - val_loss: 0.5273 - val_acc: 0.7552\n",
      "Epoch 332/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4010 - acc: 0.8212 - val_loss: 0.5274 - val_acc: 0.7552\n",
      "Epoch 333/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4007 - acc: 0.8229 - val_loss: 0.5274 - val_acc: 0.7552\n",
      "Epoch 334/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4006 - acc: 0.8212 - val_loss: 0.5275 - val_acc: 0.7552\n",
      "Epoch 335/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4007 - acc: 0.8194 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 336/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4005 - acc: 0.8194 - val_loss: 0.5276 - val_acc: 0.7552\n",
      "Epoch 337/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4006 - acc: 0.8229 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 338/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4007 - acc: 0.8212 - val_loss: 0.5273 - val_acc: 0.7500\n",
      "Epoch 339/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4004 - acc: 0.8212 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 340/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4003 - acc: 0.8229 - val_loss: 0.5272 - val_acc: 0.7552\n",
      "Epoch 341/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4004 - acc: 0.8177 - val_loss: 0.5276 - val_acc: 0.7552\n",
      "Epoch 342/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4004 - acc: 0.8229 - val_loss: 0.5276 - val_acc: 0.7552\n",
      "Epoch 343/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4003 - acc: 0.8229 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 344/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4002 - acc: 0.8194 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 345/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4002 - acc: 0.8212 - val_loss: 0.5282 - val_acc: 0.7552\n",
      "Epoch 346/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4003 - acc: 0.8212 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 347/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4002 - acc: 0.8194 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 348/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4002 - acc: 0.8212 - val_loss: 0.5281 - val_acc: 0.7500\n",
      "Epoch 349/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4004 - acc: 0.8194 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 350/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4000 - acc: 0.8229 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 351/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4000 - acc: 0.8229 - val_loss: 0.5285 - val_acc: 0.7552\n",
      "Epoch 352/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4000 - acc: 0.8229 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 353/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3997 - acc: 0.8212 - val_loss: 0.5290 - val_acc: 0.7552\n",
      "Epoch 354/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3998 - acc: 0.8229 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 355/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4000 - acc: 0.8229 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 356/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3997 - acc: 0.8229 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 357/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3997 - acc: 0.8229 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 358/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3994 - acc: 0.8212 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 359/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3997 - acc: 0.8229 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 360/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3997 - acc: 0.8177 - val_loss: 0.5292 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3997 - acc: 0.8212 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 362/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3995 - acc: 0.8229 - val_loss: 0.5296 - val_acc: 0.7500\n",
      "Epoch 363/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3998 - acc: 0.8212 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 364/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.3995 - acc: 0.8229 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 365/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3997 - acc: 0.8194 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 366/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3994 - acc: 0.8212 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 367/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3992 - acc: 0.8229 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 368/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3994 - acc: 0.8194 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 369/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3993 - acc: 0.8212 - val_loss: 0.5299 - val_acc: 0.7500\n",
      "Epoch 370/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3995 - acc: 0.8229 - val_loss: 0.5298 - val_acc: 0.7500\n",
      "Epoch 371/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3993 - acc: 0.8212 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 372/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3992 - acc: 0.8247 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 373/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3991 - acc: 0.8194 - val_loss: 0.5302 - val_acc: 0.7500\n",
      "Epoch 374/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3989 - acc: 0.8212 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 375/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3993 - acc: 0.8212 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 376/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.3989 - acc: 0.8229 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 377/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3991 - acc: 0.8229 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 378/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.3989 - acc: 0.8229 - val_loss: 0.5304 - val_acc: 0.7552\n",
      "Epoch 379/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3990 - acc: 0.8194 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 380/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3989 - acc: 0.8229 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 381/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3989 - acc: 0.8212 - val_loss: 0.5307 - val_acc: 0.7500\n",
      "Epoch 382/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3988 - acc: 0.8212 - val_loss: 0.5303 - val_acc: 0.7500\n",
      "Epoch 383/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3987 - acc: 0.8194 - val_loss: 0.5303 - val_acc: 0.7500\n",
      "Epoch 384/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3988 - acc: 0.8229 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 385/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3988 - acc: 0.8194 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 386/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3987 - acc: 0.8194 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 387/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3987 - acc: 0.8194 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 388/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3985 - acc: 0.8194 - val_loss: 0.5312 - val_acc: 0.7500\n",
      "Epoch 389/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3984 - acc: 0.8229 - val_loss: 0.5314 - val_acc: 0.7552\n",
      "Epoch 390/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3986 - acc: 0.8229 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 391/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3984 - acc: 0.8194 - val_loss: 0.5314 - val_acc: 0.7552\n",
      "Epoch 392/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3986 - acc: 0.8177 - val_loss: 0.5317 - val_acc: 0.7552\n",
      "Epoch 393/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3983 - acc: 0.8194 - val_loss: 0.5314 - val_acc: 0.7552\n",
      "Epoch 394/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3985 - acc: 0.8229 - val_loss: 0.5315 - val_acc: 0.7552\n",
      "Epoch 395/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3984 - acc: 0.8229 - val_loss: 0.5316 - val_acc: 0.7552\n",
      "Epoch 396/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3985 - acc: 0.8229 - val_loss: 0.5316 - val_acc: 0.7552\n",
      "Epoch 397/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3983 - acc: 0.8194 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 398/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3981 - acc: 0.8212 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 399/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3981 - acc: 0.8229 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 400/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3981 - acc: 0.8194 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 401/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3982 - acc: 0.8212 - val_loss: 0.5315 - val_acc: 0.7552\n",
      "Epoch 402/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3981 - acc: 0.8212 - val_loss: 0.5314 - val_acc: 0.7552\n",
      "Epoch 403/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3982 - acc: 0.8247 - val_loss: 0.5314 - val_acc: 0.7552\n",
      "Epoch 404/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3983 - acc: 0.8229 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 405/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3980 - acc: 0.8212 - val_loss: 0.5314 - val_acc: 0.7552\n",
      "Epoch 406/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3979 - acc: 0.8212 - val_loss: 0.5315 - val_acc: 0.7552\n",
      "Epoch 407/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3981 - acc: 0.8194 - val_loss: 0.5320 - val_acc: 0.7552\n",
      "Epoch 408/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3981 - acc: 0.8212 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 409/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3981 - acc: 0.8212 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 410/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3979 - acc: 0.8247 - val_loss: 0.5318 - val_acc: 0.7552\n",
      "Epoch 411/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3978 - acc: 0.8247 - val_loss: 0.5319 - val_acc: 0.7552\n",
      "Epoch 412/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3978 - acc: 0.8212 - val_loss: 0.5320 - val_acc: 0.7552\n",
      "Epoch 413/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3979 - acc: 0.8229 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 414/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3977 - acc: 0.8247 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 415/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3977 - acc: 0.8247 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 416/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3979 - acc: 0.8247 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 417/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3979 - acc: 0.8212 - val_loss: 0.5323 - val_acc: 0.7552\n",
      "Epoch 418/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3976 - acc: 0.8229 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 419/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3976 - acc: 0.8247 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 420/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3977 - acc: 0.8247 - val_loss: 0.5320 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3974 - acc: 0.8212 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 422/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3977 - acc: 0.8212 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 423/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3976 - acc: 0.8194 - val_loss: 0.5327 - val_acc: 0.7552\n",
      "Epoch 424/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3976 - acc: 0.8247 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 425/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3975 - acc: 0.8212 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 426/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3975 - acc: 0.8247 - val_loss: 0.5315 - val_acc: 0.7552\n",
      "Epoch 427/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3973 - acc: 0.8229 - val_loss: 0.5318 - val_acc: 0.7552\n",
      "Epoch 428/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3973 - acc: 0.8229 - val_loss: 0.5316 - val_acc: 0.7552\n",
      "Epoch 429/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3973 - acc: 0.8247 - val_loss: 0.5317 - val_acc: 0.7552\n",
      "Epoch 430/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.3974 - acc: 0.8229 - val_loss: 0.5318 - val_acc: 0.7552\n",
      "Epoch 431/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.3971 - acc: 0.8229 - val_loss: 0.5318 - val_acc: 0.7552\n",
      "Epoch 432/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3975 - acc: 0.8229 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 433/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3972 - acc: 0.8247 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 434/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3970 - acc: 0.8229 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 435/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3970 - acc: 0.8264 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 436/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3969 - acc: 0.8247 - val_loss: 0.5320 - val_acc: 0.7552\n",
      "Epoch 437/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3969 - acc: 0.8229 - val_loss: 0.5320 - val_acc: 0.7552\n",
      "Epoch 438/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3972 - acc: 0.8247 - val_loss: 0.5323 - val_acc: 0.7552\n",
      "Epoch 439/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3969 - acc: 0.8212 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 440/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3970 - acc: 0.8264 - val_loss: 0.5320 - val_acc: 0.7552\n",
      "Epoch 441/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3966 - acc: 0.8247 - val_loss: 0.5320 - val_acc: 0.7552\n",
      "Epoch 442/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3970 - acc: 0.8247 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 443/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3967 - acc: 0.8281 - val_loss: 0.5324 - val_acc: 0.7552\n",
      "Epoch 444/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3967 - acc: 0.8229 - val_loss: 0.5329 - val_acc: 0.7552\n",
      "Epoch 445/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3967 - acc: 0.8264 - val_loss: 0.5326 - val_acc: 0.7552\n",
      "Epoch 446/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3969 - acc: 0.8247 - val_loss: 0.5326 - val_acc: 0.7552\n",
      "Epoch 447/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3966 - acc: 0.8229 - val_loss: 0.5327 - val_acc: 0.7552\n",
      "Epoch 448/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3967 - acc: 0.8247 - val_loss: 0.5327 - val_acc: 0.7552\n",
      "Epoch 449/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3965 - acc: 0.8264 - val_loss: 0.5327 - val_acc: 0.7552\n",
      "Epoch 450/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3965 - acc: 0.8247 - val_loss: 0.5329 - val_acc: 0.7552\n",
      "Epoch 451/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3966 - acc: 0.8247 - val_loss: 0.5328 - val_acc: 0.7552\n",
      "Epoch 452/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3964 - acc: 0.8264 - val_loss: 0.5326 - val_acc: 0.7552\n",
      "Epoch 453/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3966 - acc: 0.8229 - val_loss: 0.5328 - val_acc: 0.7552\n",
      "Epoch 454/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3964 - acc: 0.8281 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 455/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3965 - acc: 0.8264 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 456/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3966 - acc: 0.8247 - val_loss: 0.5336 - val_acc: 0.7552\n",
      "Epoch 457/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3966 - acc: 0.8229 - val_loss: 0.5333 - val_acc: 0.7552\n",
      "Epoch 458/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3967 - acc: 0.8212 - val_loss: 0.5337 - val_acc: 0.7552\n",
      "Epoch 459/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3966 - acc: 0.8264 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 460/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3963 - acc: 0.8281 - val_loss: 0.5333 - val_acc: 0.7552\n",
      "Epoch 461/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3963 - acc: 0.8264 - val_loss: 0.5331 - val_acc: 0.7552\n",
      "Epoch 462/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3964 - acc: 0.8264 - val_loss: 0.5332 - val_acc: 0.7552\n",
      "Epoch 463/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3963 - acc: 0.8247 - val_loss: 0.5335 - val_acc: 0.7552\n",
      "Epoch 464/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3963 - acc: 0.8264 - val_loss: 0.5336 - val_acc: 0.7552\n",
      "Epoch 465/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3964 - acc: 0.8281 - val_loss: 0.5336 - val_acc: 0.7552\n",
      "Epoch 466/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3961 - acc: 0.8247 - val_loss: 0.5338 - val_acc: 0.7552\n",
      "Epoch 467/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3962 - acc: 0.8229 - val_loss: 0.5335 - val_acc: 0.7552\n",
      "Epoch 468/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3964 - acc: 0.8264 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 469/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3962 - acc: 0.8281 - val_loss: 0.5333 - val_acc: 0.7552\n",
      "Epoch 470/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3960 - acc: 0.8264 - val_loss: 0.5338 - val_acc: 0.7552\n",
      "Epoch 471/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3960 - acc: 0.8264 - val_loss: 0.5335 - val_acc: 0.7552\n",
      "Epoch 472/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3957 - acc: 0.8247 - val_loss: 0.5335 - val_acc: 0.7552\n",
      "Epoch 473/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3961 - acc: 0.8281 - val_loss: 0.5339 - val_acc: 0.7552\n",
      "Epoch 474/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3959 - acc: 0.8281 - val_loss: 0.5339 - val_acc: 0.7552\n",
      "Epoch 475/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3959 - acc: 0.8229 - val_loss: 0.5340 - val_acc: 0.7552\n",
      "Epoch 476/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3957 - acc: 0.8281 - val_loss: 0.5338 - val_acc: 0.7552\n",
      "Epoch 477/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3957 - acc: 0.8299 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 478/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3959 - acc: 0.8299 - val_loss: 0.5335 - val_acc: 0.7552\n",
      "Epoch 479/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3958 - acc: 0.8264 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 480/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3957 - acc: 0.8281 - val_loss: 0.5332 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3957 - acc: 0.8247 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 482/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3954 - acc: 0.8247 - val_loss: 0.5337 - val_acc: 0.7552\n",
      "Epoch 483/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3959 - acc: 0.8281 - val_loss: 0.5334 - val_acc: 0.7552\n",
      "Epoch 484/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3958 - acc: 0.8281 - val_loss: 0.5338 - val_acc: 0.7552\n",
      "Epoch 485/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3954 - acc: 0.8299 - val_loss: 0.5338 - val_acc: 0.7552\n",
      "Epoch 486/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3955 - acc: 0.8316 - val_loss: 0.5340 - val_acc: 0.7552\n",
      "Epoch 487/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.3956 - acc: 0.8281 - val_loss: 0.5341 - val_acc: 0.7552\n",
      "Epoch 488/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3955 - acc: 0.8316 - val_loss: 0.5342 - val_acc: 0.7552\n",
      "Epoch 489/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3954 - acc: 0.8247 - val_loss: 0.5340 - val_acc: 0.7552\n",
      "Epoch 490/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3951 - acc: 0.8299 - val_loss: 0.5342 - val_acc: 0.7552\n",
      "Epoch 491/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3954 - acc: 0.8281 - val_loss: 0.5342 - val_acc: 0.7552\n",
      "Epoch 492/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3951 - acc: 0.8264 - val_loss: 0.5343 - val_acc: 0.7552\n",
      "Epoch 493/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3951 - acc: 0.8281 - val_loss: 0.5342 - val_acc: 0.7552\n",
      "Epoch 494/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3952 - acc: 0.8247 - val_loss: 0.5342 - val_acc: 0.7552\n",
      "Epoch 495/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3948 - acc: 0.8281 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 496/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3952 - acc: 0.8281 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 497/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3949 - acc: 0.8299 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 498/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3951 - acc: 0.8281 - val_loss: 0.5347 - val_acc: 0.7552\n",
      "Epoch 499/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3949 - acc: 0.8299 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 500/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3950 - acc: 0.8281 - val_loss: 0.5347 - val_acc: 0.7552\n",
      "Epoch 501/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3949 - acc: 0.8281 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 502/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3949 - acc: 0.8299 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 503/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3949 - acc: 0.8264 - val_loss: 0.5350 - val_acc: 0.7552\n",
      "Epoch 504/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3947 - acc: 0.8299 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 505/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3948 - acc: 0.8281 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 506/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3947 - acc: 0.8264 - val_loss: 0.5349 - val_acc: 0.7552\n",
      "Epoch 507/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3947 - acc: 0.8299 - val_loss: 0.5347 - val_acc: 0.7552\n",
      "Epoch 508/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3945 - acc: 0.8264 - val_loss: 0.5351 - val_acc: 0.7552\n",
      "Epoch 509/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3945 - acc: 0.8281 - val_loss: 0.5349 - val_acc: 0.7552\n",
      "Epoch 510/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3948 - acc: 0.8281 - val_loss: 0.5351 - val_acc: 0.7552\n",
      "Epoch 511/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3945 - acc: 0.8316 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 512/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3944 - acc: 0.8281 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 513/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3945 - acc: 0.8316 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 514/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3944 - acc: 0.8281 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 515/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3946 - acc: 0.8299 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 516/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3946 - acc: 0.8299 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 517/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3944 - acc: 0.8299 - val_loss: 0.5350 - val_acc: 0.7552\n",
      "Epoch 518/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3941 - acc: 0.8264 - val_loss: 0.5351 - val_acc: 0.7552\n",
      "Epoch 519/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3947 - acc: 0.8229 - val_loss: 0.5350 - val_acc: 0.7552\n",
      "Epoch 520/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3943 - acc: 0.8316 - val_loss: 0.5354 - val_acc: 0.7552\n",
      "Epoch 521/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3943 - acc: 0.8281 - val_loss: 0.5356 - val_acc: 0.7552\n",
      "Epoch 522/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3941 - acc: 0.8281 - val_loss: 0.5351 - val_acc: 0.7552\n",
      "Epoch 523/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3942 - acc: 0.8281 - val_loss: 0.5347 - val_acc: 0.7552\n",
      "Epoch 524/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3941 - acc: 0.8281 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 525/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3937 - acc: 0.8316 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 526/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3941 - acc: 0.8299 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 527/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3941 - acc: 0.8281 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 528/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3940 - acc: 0.8264 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 529/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.3939 - acc: 0.8316 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 530/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3937 - acc: 0.8281 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 531/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3940 - acc: 0.8299 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 532/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3938 - acc: 0.8281 - val_loss: 0.5345 - val_acc: 0.7552\n",
      "Epoch 533/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3937 - acc: 0.8299 - val_loss: 0.5343 - val_acc: 0.7552\n",
      "Epoch 534/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3935 - acc: 0.8281 - val_loss: 0.5343 - val_acc: 0.7552\n",
      "Epoch 535/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3938 - acc: 0.8316 - val_loss: 0.5344 - val_acc: 0.7552\n",
      "Epoch 536/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3935 - acc: 0.8299 - val_loss: 0.5347 - val_acc: 0.7552\n",
      "Epoch 537/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3936 - acc: 0.8316 - val_loss: 0.5349 - val_acc: 0.7552\n",
      "Epoch 538/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3936 - acc: 0.8316 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 539/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3935 - acc: 0.8316 - val_loss: 0.5352 - val_acc: 0.7552\n",
      "Epoch 540/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3935 - acc: 0.8281 - val_loss: 0.5349 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3935 - acc: 0.8333 - val_loss: 0.5353 - val_acc: 0.7552\n",
      "Epoch 542/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3936 - acc: 0.8333 - val_loss: 0.5350 - val_acc: 0.7552\n",
      "Epoch 543/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3932 - acc: 0.8281 - val_loss: 0.5352 - val_acc: 0.7552\n",
      "Epoch 544/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3931 - acc: 0.8281 - val_loss: 0.5355 - val_acc: 0.7552\n",
      "Epoch 545/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3931 - acc: 0.8299 - val_loss: 0.5358 - val_acc: 0.7552\n",
      "Epoch 546/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3934 - acc: 0.8316 - val_loss: 0.5357 - val_acc: 0.7552\n",
      "Epoch 547/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3933 - acc: 0.8316 - val_loss: 0.5359 - val_acc: 0.7552\n",
      "Epoch 548/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3931 - acc: 0.8299 - val_loss: 0.5355 - val_acc: 0.7552\n",
      "Epoch 549/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3929 - acc: 0.8299 - val_loss: 0.5359 - val_acc: 0.7552\n",
      "Epoch 550/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3932 - acc: 0.8316 - val_loss: 0.5360 - val_acc: 0.7552\n",
      "Epoch 551/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3928 - acc: 0.8316 - val_loss: 0.5355 - val_acc: 0.7552\n",
      "Epoch 552/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3929 - acc: 0.8281 - val_loss: 0.5359 - val_acc: 0.7552\n",
      "Epoch 553/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3929 - acc: 0.8299 - val_loss: 0.5358 - val_acc: 0.7552\n",
      "Epoch 554/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3931 - acc: 0.8316 - val_loss: 0.5357 - val_acc: 0.7552\n",
      "Epoch 555/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3933 - acc: 0.8316 - val_loss: 0.5357 - val_acc: 0.7552\n",
      "Epoch 556/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3928 - acc: 0.8333 - val_loss: 0.5354 - val_acc: 0.7552\n",
      "Epoch 557/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3928 - acc: 0.8281 - val_loss: 0.5358 - val_acc: 0.7552\n",
      "Epoch 558/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3932 - acc: 0.8333 - val_loss: 0.5356 - val_acc: 0.7552\n",
      "Epoch 559/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3926 - acc: 0.8316 - val_loss: 0.5360 - val_acc: 0.7552\n",
      "Epoch 560/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3930 - acc: 0.8333 - val_loss: 0.5361 - val_acc: 0.7552\n",
      "Epoch 561/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3927 - acc: 0.8299 - val_loss: 0.5364 - val_acc: 0.7552\n",
      "Epoch 562/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3927 - acc: 0.8316 - val_loss: 0.5362 - val_acc: 0.7552\n",
      "Epoch 563/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3925 - acc: 0.8333 - val_loss: 0.5359 - val_acc: 0.7552\n",
      "Epoch 564/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3927 - acc: 0.8333 - val_loss: 0.5362 - val_acc: 0.7552\n",
      "Epoch 565/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3927 - acc: 0.8299 - val_loss: 0.5360 - val_acc: 0.7552\n",
      "Epoch 566/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3924 - acc: 0.8299 - val_loss: 0.5358 - val_acc: 0.7552\n",
      "Epoch 567/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3923 - acc: 0.8333 - val_loss: 0.5363 - val_acc: 0.7552\n",
      "Epoch 568/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3927 - acc: 0.8316 - val_loss: 0.5364 - val_acc: 0.7552\n",
      "Epoch 569/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3924 - acc: 0.8316 - val_loss: 0.5364 - val_acc: 0.7552\n",
      "Epoch 570/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3924 - acc: 0.8316 - val_loss: 0.5365 - val_acc: 0.7552\n",
      "Epoch 571/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3920 - acc: 0.8316 - val_loss: 0.5360 - val_acc: 0.7552\n",
      "Epoch 572/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3923 - acc: 0.8333 - val_loss: 0.5361 - val_acc: 0.7552\n",
      "Epoch 573/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3922 - acc: 0.8351 - val_loss: 0.5363 - val_acc: 0.7552\n",
      "Epoch 574/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3925 - acc: 0.8333 - val_loss: 0.5362 - val_acc: 0.7552\n",
      "Epoch 575/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3924 - acc: 0.8316 - val_loss: 0.5361 - val_acc: 0.7552\n",
      "Epoch 576/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3922 - acc: 0.8351 - val_loss: 0.5366 - val_acc: 0.7552\n",
      "Epoch 577/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3922 - acc: 0.8299 - val_loss: 0.5374 - val_acc: 0.7552\n",
      "Epoch 578/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3920 - acc: 0.8316 - val_loss: 0.5372 - val_acc: 0.7552\n",
      "Epoch 579/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3922 - acc: 0.8316 - val_loss: 0.5371 - val_acc: 0.7552\n",
      "Epoch 580/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3918 - acc: 0.8333 - val_loss: 0.5371 - val_acc: 0.7552\n",
      "Epoch 581/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3922 - acc: 0.8351 - val_loss: 0.5375 - val_acc: 0.7552\n",
      "Epoch 582/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3919 - acc: 0.8316 - val_loss: 0.5373 - val_acc: 0.7552\n",
      "Epoch 583/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3918 - acc: 0.8316 - val_loss: 0.5379 - val_acc: 0.7552\n",
      "Epoch 584/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3918 - acc: 0.8333 - val_loss: 0.5378 - val_acc: 0.7552\n",
      "Epoch 585/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3915 - acc: 0.8333 - val_loss: 0.5374 - val_acc: 0.7552\n",
      "Epoch 586/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3918 - acc: 0.8333 - val_loss: 0.5382 - val_acc: 0.7552\n",
      "Epoch 587/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3920 - acc: 0.8351 - val_loss: 0.5383 - val_acc: 0.7552\n",
      "Epoch 588/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3920 - acc: 0.8316 - val_loss: 0.5381 - val_acc: 0.7552\n",
      "Epoch 589/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3917 - acc: 0.8351 - val_loss: 0.5377 - val_acc: 0.7552\n",
      "Epoch 590/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.3917 - acc: 0.8333 - val_loss: 0.5377 - val_acc: 0.7552\n",
      "Epoch 591/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3914 - acc: 0.8351 - val_loss: 0.5378 - val_acc: 0.7552\n",
      "Epoch 592/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3915 - acc: 0.8351 - val_loss: 0.5374 - val_acc: 0.7552\n",
      "Epoch 593/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3916 - acc: 0.8333 - val_loss: 0.5378 - val_acc: 0.7552\n",
      "Epoch 594/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3915 - acc: 0.8316 - val_loss: 0.5381 - val_acc: 0.7552\n",
      "Epoch 595/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3914 - acc: 0.8333 - val_loss: 0.5378 - val_acc: 0.7552\n",
      "Epoch 596/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3916 - acc: 0.8316 - val_loss: 0.5382 - val_acc: 0.7552\n",
      "Epoch 597/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3913 - acc: 0.8351 - val_loss: 0.5380 - val_acc: 0.7552\n",
      "Epoch 598/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3913 - acc: 0.8333 - val_loss: 0.5383 - val_acc: 0.7552\n",
      "Epoch 599/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3912 - acc: 0.8333 - val_loss: 0.5381 - val_acc: 0.7552\n",
      "Epoch 600/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3915 - acc: 0.8333 - val_loss: 0.5380 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.3913 - acc: 0.8351 - val_loss: 0.5376 - val_acc: 0.7552\n",
      "Epoch 602/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3911 - acc: 0.8351 - val_loss: 0.5376 - val_acc: 0.7552\n",
      "Epoch 603/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3916 - acc: 0.8333 - val_loss: 0.5383 - val_acc: 0.7552\n",
      "Epoch 604/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3910 - acc: 0.8351 - val_loss: 0.5382 - val_acc: 0.7552\n",
      "Epoch 605/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3911 - acc: 0.8333 - val_loss: 0.5387 - val_acc: 0.7552\n",
      "Epoch 606/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3911 - acc: 0.8333 - val_loss: 0.5383 - val_acc: 0.7552\n",
      "Epoch 607/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3910 - acc: 0.8316 - val_loss: 0.5387 - val_acc: 0.7552\n",
      "Epoch 608/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3910 - acc: 0.8333 - val_loss: 0.5388 - val_acc: 0.7552\n",
      "Epoch 609/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3908 - acc: 0.8368 - val_loss: 0.5387 - val_acc: 0.7552\n",
      "Epoch 610/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3907 - acc: 0.8368 - val_loss: 0.5388 - val_acc: 0.7552\n",
      "Epoch 611/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3908 - acc: 0.8351 - val_loss: 0.5389 - val_acc: 0.7552\n",
      "Epoch 612/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3906 - acc: 0.8368 - val_loss: 0.5390 - val_acc: 0.7552\n",
      "Epoch 613/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3907 - acc: 0.8385 - val_loss: 0.5386 - val_acc: 0.7552\n",
      "Epoch 614/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3905 - acc: 0.8351 - val_loss: 0.5390 - val_acc: 0.7552\n",
      "Epoch 615/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3904 - acc: 0.8368 - val_loss: 0.5390 - val_acc: 0.7552\n",
      "Epoch 616/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3906 - acc: 0.8351 - val_loss: 0.5390 - val_acc: 0.7552\n",
      "Epoch 617/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3904 - acc: 0.8351 - val_loss: 0.5388 - val_acc: 0.7552\n",
      "Epoch 618/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3905 - acc: 0.8316 - val_loss: 0.5391 - val_acc: 0.7552\n",
      "Epoch 619/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3905 - acc: 0.8368 - val_loss: 0.5397 - val_acc: 0.7552\n",
      "Epoch 620/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3902 - acc: 0.8385 - val_loss: 0.5391 - val_acc: 0.7552\n",
      "Epoch 621/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3905 - acc: 0.8368 - val_loss: 0.5392 - val_acc: 0.7552\n",
      "Epoch 622/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3904 - acc: 0.8385 - val_loss: 0.5397 - val_acc: 0.7552\n",
      "Epoch 623/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3906 - acc: 0.8368 - val_loss: 0.5391 - val_acc: 0.7552\n",
      "Epoch 624/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3902 - acc: 0.8351 - val_loss: 0.5391 - val_acc: 0.7552\n",
      "Epoch 625/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3902 - acc: 0.8368 - val_loss: 0.5389 - val_acc: 0.7552\n",
      "Epoch 626/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3899 - acc: 0.8385 - val_loss: 0.5389 - val_acc: 0.7500\n",
      "Epoch 627/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3902 - acc: 0.8368 - val_loss: 0.5399 - val_acc: 0.7604\n",
      "Epoch 628/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3900 - acc: 0.8385 - val_loss: 0.5395 - val_acc: 0.7552\n",
      "Epoch 629/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3899 - acc: 0.8385 - val_loss: 0.5395 - val_acc: 0.7552\n",
      "Epoch 630/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3898 - acc: 0.8403 - val_loss: 0.5394 - val_acc: 0.7552\n",
      "Epoch 631/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3901 - acc: 0.8385 - val_loss: 0.5391 - val_acc: 0.7500\n",
      "Epoch 632/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3899 - acc: 0.8368 - val_loss: 0.5393 - val_acc: 0.7552\n",
      "Epoch 633/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3897 - acc: 0.8351 - val_loss: 0.5397 - val_acc: 0.7552\n",
      "Epoch 634/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3896 - acc: 0.8385 - val_loss: 0.5398 - val_acc: 0.7552\n",
      "Epoch 635/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3898 - acc: 0.8420 - val_loss: 0.5395 - val_acc: 0.7552\n",
      "Epoch 636/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3898 - acc: 0.8385 - val_loss: 0.5395 - val_acc: 0.7552\n",
      "Epoch 637/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3897 - acc: 0.8403 - val_loss: 0.5395 - val_acc: 0.7552\n",
      "Epoch 638/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3896 - acc: 0.8368 - val_loss: 0.5401 - val_acc: 0.7552\n",
      "Epoch 639/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3898 - acc: 0.8403 - val_loss: 0.5401 - val_acc: 0.7448\n",
      "Epoch 640/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3896 - acc: 0.8403 - val_loss: 0.5401 - val_acc: 0.7552\n",
      "Epoch 641/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3894 - acc: 0.8385 - val_loss: 0.5403 - val_acc: 0.7552\n",
      "Epoch 642/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3896 - acc: 0.8385 - val_loss: 0.5405 - val_acc: 0.7552\n",
      "Epoch 643/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3893 - acc: 0.8403 - val_loss: 0.5402 - val_acc: 0.7552\n",
      "Epoch 644/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3895 - acc: 0.8403 - val_loss: 0.5403 - val_acc: 0.7552\n",
      "Epoch 645/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3896 - acc: 0.8403 - val_loss: 0.5403 - val_acc: 0.7552\n",
      "Epoch 646/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3891 - acc: 0.8420 - val_loss: 0.5408 - val_acc: 0.7552\n",
      "Epoch 647/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3892 - acc: 0.8385 - val_loss: 0.5409 - val_acc: 0.7552\n",
      "Epoch 648/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3893 - acc: 0.8403 - val_loss: 0.5405 - val_acc: 0.7552\n",
      "Epoch 649/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3892 - acc: 0.8403 - val_loss: 0.5406 - val_acc: 0.7500\n",
      "Epoch 650/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3891 - acc: 0.8385 - val_loss: 0.5405 - val_acc: 0.7552\n",
      "Epoch 651/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3891 - acc: 0.8385 - val_loss: 0.5402 - val_acc: 0.7448\n",
      "Epoch 652/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3891 - acc: 0.8403 - val_loss: 0.5406 - val_acc: 0.7552\n",
      "Epoch 653/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3889 - acc: 0.8420 - val_loss: 0.5407 - val_acc: 0.7500\n",
      "Epoch 654/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3893 - acc: 0.8420 - val_loss: 0.5406 - val_acc: 0.7552\n",
      "Epoch 655/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3891 - acc: 0.8403 - val_loss: 0.5405 - val_acc: 0.7500\n",
      "Epoch 656/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3890 - acc: 0.8420 - val_loss: 0.5405 - val_acc: 0.7500\n",
      "Epoch 657/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3890 - acc: 0.8368 - val_loss: 0.5412 - val_acc: 0.7500\n",
      "Epoch 658/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3889 - acc: 0.8420 - val_loss: 0.5410 - val_acc: 0.7500\n",
      "Epoch 659/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3891 - acc: 0.8420 - val_loss: 0.5411 - val_acc: 0.7552\n",
      "Epoch 660/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3888 - acc: 0.8403 - val_loss: 0.5409 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3886 - acc: 0.8420 - val_loss: 0.5405 - val_acc: 0.7500\n",
      "Epoch 662/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3888 - acc: 0.8420 - val_loss: 0.5401 - val_acc: 0.7448\n",
      "Epoch 663/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3885 - acc: 0.8403 - val_loss: 0.5407 - val_acc: 0.7500\n",
      "Epoch 664/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3888 - acc: 0.8385 - val_loss: 0.5411 - val_acc: 0.7552\n",
      "Epoch 665/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3890 - acc: 0.8420 - val_loss: 0.5409 - val_acc: 0.7500\n",
      "Epoch 666/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3883 - acc: 0.8403 - val_loss: 0.5410 - val_acc: 0.7500\n",
      "Epoch 667/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3886 - acc: 0.8420 - val_loss: 0.5405 - val_acc: 0.7448\n",
      "Epoch 668/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3883 - acc: 0.8438 - val_loss: 0.5401 - val_acc: 0.7448\n",
      "Epoch 669/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3885 - acc: 0.8385 - val_loss: 0.5411 - val_acc: 0.7500\n",
      "Epoch 670/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3883 - acc: 0.8385 - val_loss: 0.5413 - val_acc: 0.7500\n",
      "Epoch 671/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3885 - acc: 0.8420 - val_loss: 0.5413 - val_acc: 0.7500\n",
      "Epoch 672/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3885 - acc: 0.8420 - val_loss: 0.5409 - val_acc: 0.7500\n",
      "Epoch 673/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3881 - acc: 0.8403 - val_loss: 0.5410 - val_acc: 0.7500\n",
      "Epoch 674/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3883 - acc: 0.8420 - val_loss: 0.5411 - val_acc: 0.7500\n",
      "Epoch 675/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3884 - acc: 0.8438 - val_loss: 0.5414 - val_acc: 0.7500\n",
      "Epoch 676/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3881 - acc: 0.8420 - val_loss: 0.5411 - val_acc: 0.7500\n",
      "Epoch 677/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3880 - acc: 0.8403 - val_loss: 0.5417 - val_acc: 0.7500\n",
      "Epoch 678/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3880 - acc: 0.8420 - val_loss: 0.5413 - val_acc: 0.7500\n",
      "Epoch 679/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3882 - acc: 0.8420 - val_loss: 0.5421 - val_acc: 0.7500\n",
      "Epoch 680/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3882 - acc: 0.8403 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 681/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3877 - acc: 0.8438 - val_loss: 0.5416 - val_acc: 0.7500\n",
      "Epoch 682/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3878 - acc: 0.8438 - val_loss: 0.5414 - val_acc: 0.7448\n",
      "Epoch 683/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3877 - acc: 0.8455 - val_loss: 0.5414 - val_acc: 0.7500\n",
      "Epoch 684/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3878 - acc: 0.8438 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 685/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3877 - acc: 0.8420 - val_loss: 0.5422 - val_acc: 0.7552\n",
      "Epoch 686/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3878 - acc: 0.8438 - val_loss: 0.5422 - val_acc: 0.7500\n",
      "Epoch 687/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3877 - acc: 0.8455 - val_loss: 0.5417 - val_acc: 0.7500\n",
      "Epoch 688/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3878 - acc: 0.8420 - val_loss: 0.5421 - val_acc: 0.7500\n",
      "Epoch 689/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3874 - acc: 0.8438 - val_loss: 0.5427 - val_acc: 0.7500\n",
      "Epoch 690/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3874 - acc: 0.8438 - val_loss: 0.5423 - val_acc: 0.7500\n",
      "Epoch 691/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3876 - acc: 0.8420 - val_loss: 0.5416 - val_acc: 0.7448\n",
      "Epoch 692/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3875 - acc: 0.8455 - val_loss: 0.5420 - val_acc: 0.7500\n",
      "Epoch 693/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3877 - acc: 0.8438 - val_loss: 0.5414 - val_acc: 0.7448\n",
      "Epoch 694/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3874 - acc: 0.8438 - val_loss: 0.5413 - val_acc: 0.7500\n",
      "Epoch 695/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3874 - acc: 0.8455 - val_loss: 0.5413 - val_acc: 0.7500\n",
      "Epoch 696/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3875 - acc: 0.8403 - val_loss: 0.5417 - val_acc: 0.7500\n",
      "Epoch 697/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3874 - acc: 0.8438 - val_loss: 0.5419 - val_acc: 0.7500\n",
      "Epoch 698/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.3875 - acc: 0.8438 - val_loss: 0.5418 - val_acc: 0.7552\n",
      "Epoch 699/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3872 - acc: 0.8438 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 700/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3873 - acc: 0.8420 - val_loss: 0.5423 - val_acc: 0.7500\n",
      "Epoch 701/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3871 - acc: 0.8438 - val_loss: 0.5424 - val_acc: 0.7500\n",
      "Epoch 702/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3871 - acc: 0.8455 - val_loss: 0.5426 - val_acc: 0.7500\n",
      "Epoch 703/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3870 - acc: 0.8438 - val_loss: 0.5420 - val_acc: 0.7552\n",
      "Epoch 704/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3872 - acc: 0.8438 - val_loss: 0.5420 - val_acc: 0.7552\n",
      "Epoch 705/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3872 - acc: 0.8455 - val_loss: 0.5415 - val_acc: 0.7500\n",
      "Epoch 706/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3871 - acc: 0.8403 - val_loss: 0.5420 - val_acc: 0.7552\n",
      "Epoch 707/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3871 - acc: 0.8438 - val_loss: 0.5422 - val_acc: 0.7552\n",
      "Epoch 708/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3870 - acc: 0.8472 - val_loss: 0.5420 - val_acc: 0.7500\n",
      "Epoch 709/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3868 - acc: 0.8420 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 710/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3867 - acc: 0.8438 - val_loss: 0.5419 - val_acc: 0.7500\n",
      "Epoch 711/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3868 - acc: 0.8438 - val_loss: 0.5418 - val_acc: 0.7552\n",
      "Epoch 712/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3866 - acc: 0.8438 - val_loss: 0.5422 - val_acc: 0.7604\n",
      "Epoch 713/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3866 - acc: 0.8455 - val_loss: 0.5419 - val_acc: 0.7500\n",
      "Epoch 714/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3864 - acc: 0.8438 - val_loss: 0.5419 - val_acc: 0.7500\n",
      "Epoch 715/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3871 - acc: 0.8403 - val_loss: 0.5419 - val_acc: 0.7552\n",
      "Epoch 716/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3864 - acc: 0.8438 - val_loss: 0.5423 - val_acc: 0.7604\n",
      "Epoch 717/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.3866 - acc: 0.8438 - val_loss: 0.5422 - val_acc: 0.7604\n",
      "Epoch 718/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3865 - acc: 0.8472 - val_loss: 0.5416 - val_acc: 0.7552\n",
      "Epoch 719/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.3867 - acc: 0.8438 - val_loss: 0.5419 - val_acc: 0.7552\n",
      "Epoch 720/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3867 - acc: 0.8438 - val_loss: 0.5415 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3861 - acc: 0.8438 - val_loss: 0.5416 - val_acc: 0.7552\n",
      "Epoch 722/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3864 - acc: 0.8403 - val_loss: 0.5420 - val_acc: 0.7552\n",
      "Epoch 723/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3861 - acc: 0.8420 - val_loss: 0.5416 - val_acc: 0.7552\n",
      "Epoch 724/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3863 - acc: 0.8472 - val_loss: 0.5421 - val_acc: 0.7552\n",
      "Epoch 725/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3862 - acc: 0.8420 - val_loss: 0.5420 - val_acc: 0.7552\n",
      "Epoch 726/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3859 - acc: 0.8455 - val_loss: 0.5427 - val_acc: 0.7604\n",
      "Epoch 727/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3861 - acc: 0.8438 - val_loss: 0.5424 - val_acc: 0.7552\n",
      "Epoch 728/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3861 - acc: 0.8420 - val_loss: 0.5431 - val_acc: 0.7604\n",
      "Epoch 729/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3860 - acc: 0.8472 - val_loss: 0.5426 - val_acc: 0.7552\n",
      "Epoch 730/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3859 - acc: 0.8438 - val_loss: 0.5428 - val_acc: 0.7552\n",
      "Epoch 731/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3859 - acc: 0.8455 - val_loss: 0.5426 - val_acc: 0.7552\n",
      "Epoch 732/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3860 - acc: 0.8455 - val_loss: 0.5429 - val_acc: 0.7604\n",
      "Epoch 733/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3855 - acc: 0.8455 - val_loss: 0.5423 - val_acc: 0.7552\n",
      "Epoch 734/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3859 - acc: 0.8455 - val_loss: 0.5420 - val_acc: 0.7552\n",
      "Epoch 735/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3856 - acc: 0.8455 - val_loss: 0.5419 - val_acc: 0.7552\n",
      "Epoch 736/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3856 - acc: 0.8455 - val_loss: 0.5421 - val_acc: 0.7552\n",
      "Epoch 737/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3855 - acc: 0.8455 - val_loss: 0.5428 - val_acc: 0.7552\n",
      "Epoch 738/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3858 - acc: 0.8438 - val_loss: 0.5428 - val_acc: 0.7552\n",
      "Epoch 739/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3856 - acc: 0.8455 - val_loss: 0.5428 - val_acc: 0.7552\n",
      "Epoch 740/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3856 - acc: 0.8438 - val_loss: 0.5429 - val_acc: 0.7552\n",
      "Epoch 741/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3853 - acc: 0.8472 - val_loss: 0.5430 - val_acc: 0.7604\n",
      "Epoch 742/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3855 - acc: 0.8455 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 743/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3854 - acc: 0.8455 - val_loss: 0.5430 - val_acc: 0.7552\n",
      "Epoch 744/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3853 - acc: 0.8420 - val_loss: 0.5432 - val_acc: 0.7604\n",
      "Epoch 745/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3856 - acc: 0.8438 - val_loss: 0.5426 - val_acc: 0.7552\n",
      "Epoch 746/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3854 - acc: 0.8438 - val_loss: 0.5426 - val_acc: 0.7552\n",
      "Epoch 747/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3850 - acc: 0.8472 - val_loss: 0.5427 - val_acc: 0.7552\n",
      "Epoch 748/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3851 - acc: 0.8455 - val_loss: 0.5429 - val_acc: 0.7552\n",
      "Epoch 749/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3850 - acc: 0.8420 - val_loss: 0.5434 - val_acc: 0.7552\n",
      "Epoch 750/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3851 - acc: 0.8420 - val_loss: 0.5434 - val_acc: 0.7552\n",
      "Epoch 751/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.3848 - acc: 0.8438 - val_loss: 0.5436 - val_acc: 0.7552\n",
      "Epoch 752/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3850 - acc: 0.8472 - val_loss: 0.5438 - val_acc: 0.7552\n",
      "Epoch 753/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3849 - acc: 0.8438 - val_loss: 0.5443 - val_acc: 0.7604\n",
      "Epoch 754/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3849 - acc: 0.8455 - val_loss: 0.5444 - val_acc: 0.7604\n",
      "Epoch 755/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3846 - acc: 0.8455 - val_loss: 0.5438 - val_acc: 0.7552\n",
      "Epoch 756/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3851 - acc: 0.8455 - val_loss: 0.5434 - val_acc: 0.7552\n",
      "Epoch 757/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3845 - acc: 0.8420 - val_loss: 0.5440 - val_acc: 0.7552\n",
      "Epoch 758/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3846 - acc: 0.8455 - val_loss: 0.5431 - val_acc: 0.7552\n",
      "Epoch 759/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3847 - acc: 0.8455 - val_loss: 0.5431 - val_acc: 0.7552\n",
      "Epoch 760/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3848 - acc: 0.8438 - val_loss: 0.5443 - val_acc: 0.7552\n",
      "Epoch 761/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3848 - acc: 0.8472 - val_loss: 0.5432 - val_acc: 0.7552\n",
      "Epoch 762/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3846 - acc: 0.8438 - val_loss: 0.5441 - val_acc: 0.7552\n",
      "Epoch 763/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3847 - acc: 0.8420 - val_loss: 0.5442 - val_acc: 0.7552\n",
      "Epoch 764/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3845 - acc: 0.8438 - val_loss: 0.5439 - val_acc: 0.7552\n",
      "Epoch 765/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3841 - acc: 0.8455 - val_loss: 0.5435 - val_acc: 0.7552\n",
      "Epoch 766/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3844 - acc: 0.8455 - val_loss: 0.5431 - val_acc: 0.7552\n",
      "Epoch 767/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3843 - acc: 0.8438 - val_loss: 0.5433 - val_acc: 0.7552\n",
      "Epoch 768/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3846 - acc: 0.8438 - val_loss: 0.5435 - val_acc: 0.7552\n",
      "Epoch 769/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3844 - acc: 0.8403 - val_loss: 0.5445 - val_acc: 0.7552\n",
      "Epoch 770/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3845 - acc: 0.8455 - val_loss: 0.5438 - val_acc: 0.7552\n",
      "Epoch 771/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3841 - acc: 0.8438 - val_loss: 0.5439 - val_acc: 0.7552\n",
      "Epoch 772/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3840 - acc: 0.8438 - val_loss: 0.5438 - val_acc: 0.7552\n",
      "Epoch 773/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4321 - acc: 0.875 - 0s 51us/step - loss: 0.3842 - acc: 0.8438 - val_loss: 0.5437 - val_acc: 0.7552\n",
      "Epoch 774/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3842 - acc: 0.8455 - val_loss: 0.5438 - val_acc: 0.7552\n",
      "Epoch 775/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3841 - acc: 0.8455 - val_loss: 0.5439 - val_acc: 0.7552\n",
      "Epoch 776/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3839 - acc: 0.8438 - val_loss: 0.5431 - val_acc: 0.7552\n",
      "Epoch 777/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3838 - acc: 0.8420 - val_loss: 0.5434 - val_acc: 0.7552\n",
      "Epoch 778/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3839 - acc: 0.8438 - val_loss: 0.5434 - val_acc: 0.7604\n",
      "Epoch 779/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3838 - acc: 0.8438 - val_loss: 0.5433 - val_acc: 0.7552\n",
      "Epoch 780/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 43us/step - loss: 0.3841 - acc: 0.8455 - val_loss: 0.5439 - val_acc: 0.7604\n",
      "Epoch 781/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3837 - acc: 0.8472 - val_loss: 0.5441 - val_acc: 0.7604\n",
      "Epoch 782/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3837 - acc: 0.8455 - val_loss: 0.5442 - val_acc: 0.7604\n",
      "Epoch 783/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3837 - acc: 0.8455 - val_loss: 0.5439 - val_acc: 0.7552\n",
      "Epoch 784/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3838 - acc: 0.8420 - val_loss: 0.5441 - val_acc: 0.7552\n",
      "Epoch 785/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.3837 - acc: 0.8420 - val_loss: 0.5437 - val_acc: 0.7552\n",
      "Epoch 786/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3834 - acc: 0.8438 - val_loss: 0.5430 - val_acc: 0.7552\n",
      "Epoch 787/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3836 - acc: 0.8438 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 788/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3835 - acc: 0.8403 - val_loss: 0.5435 - val_acc: 0.7604\n",
      "Epoch 789/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3836 - acc: 0.8455 - val_loss: 0.5431 - val_acc: 0.7552\n",
      "Epoch 790/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3836 - acc: 0.8455 - val_loss: 0.5434 - val_acc: 0.7604\n",
      "Epoch 791/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3831 - acc: 0.8420 - val_loss: 0.5443 - val_acc: 0.7604\n",
      "Epoch 792/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3831 - acc: 0.8472 - val_loss: 0.5434 - val_acc: 0.7552\n",
      "Epoch 793/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3832 - acc: 0.8420 - val_loss: 0.5438 - val_acc: 0.7604\n",
      "Epoch 794/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3833 - acc: 0.8438 - val_loss: 0.5434 - val_acc: 0.7604\n",
      "Epoch 795/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3830 - acc: 0.8455 - val_loss: 0.5435 - val_acc: 0.7604\n",
      "Epoch 796/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3834 - acc: 0.8420 - val_loss: 0.5438 - val_acc: 0.7604\n",
      "Epoch 797/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3832 - acc: 0.8455 - val_loss: 0.5435 - val_acc: 0.7604\n",
      "Epoch 798/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3830 - acc: 0.8455 - val_loss: 0.5435 - val_acc: 0.7604\n",
      "Epoch 799/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3832 - acc: 0.8472 - val_loss: 0.5432 - val_acc: 0.7604\n",
      "Epoch 800/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3831 - acc: 0.8438 - val_loss: 0.5439 - val_acc: 0.7604\n",
      "Epoch 801/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3833 - acc: 0.8472 - val_loss: 0.5434 - val_acc: 0.7604\n",
      "Epoch 802/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3829 - acc: 0.8438 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 803/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3829 - acc: 0.8455 - val_loss: 0.5430 - val_acc: 0.7604\n",
      "Epoch 804/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3829 - acc: 0.8438 - val_loss: 0.5438 - val_acc: 0.7604\n",
      "Epoch 805/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3827 - acc: 0.8455 - val_loss: 0.5434 - val_acc: 0.7604\n",
      "Epoch 806/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3829 - acc: 0.8438 - val_loss: 0.5427 - val_acc: 0.7552\n",
      "Epoch 807/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3824 - acc: 0.8438 - val_loss: 0.5437 - val_acc: 0.7604\n",
      "Epoch 808/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3829 - acc: 0.8403 - val_loss: 0.5439 - val_acc: 0.7604\n",
      "Epoch 809/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3826 - acc: 0.8455 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 810/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3825 - acc: 0.8438 - val_loss: 0.5430 - val_acc: 0.7604\n",
      "Epoch 811/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3827 - acc: 0.8472 - val_loss: 0.5430 - val_acc: 0.7656\n",
      "Epoch 812/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3827 - acc: 0.8455 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 813/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3827 - acc: 0.8438 - val_loss: 0.5430 - val_acc: 0.7604\n",
      "Epoch 814/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3826 - acc: 0.8455 - val_loss: 0.5434 - val_acc: 0.7604\n",
      "Epoch 815/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3823 - acc: 0.8472 - val_loss: 0.5430 - val_acc: 0.7604\n",
      "Epoch 816/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3822 - acc: 0.8420 - val_loss: 0.5432 - val_acc: 0.7656\n",
      "Epoch 817/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3825 - acc: 0.8420 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 818/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3821 - acc: 0.8420 - val_loss: 0.5440 - val_acc: 0.7604\n",
      "Epoch 819/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3822 - acc: 0.8438 - val_loss: 0.5432 - val_acc: 0.7604\n",
      "Epoch 820/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3824 - acc: 0.8420 - val_loss: 0.5438 - val_acc: 0.7604\n",
      "Epoch 821/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3821 - acc: 0.8438 - val_loss: 0.5438 - val_acc: 0.7656\n",
      "Epoch 822/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3822 - acc: 0.8420 - val_loss: 0.5439 - val_acc: 0.7604\n",
      "Epoch 823/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3823 - acc: 0.8438 - val_loss: 0.5434 - val_acc: 0.7656\n",
      "Epoch 824/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3821 - acc: 0.8420 - val_loss: 0.5436 - val_acc: 0.7656\n",
      "Epoch 825/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3821 - acc: 0.8472 - val_loss: 0.5431 - val_acc: 0.7656\n",
      "Epoch 826/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3820 - acc: 0.8403 - val_loss: 0.5438 - val_acc: 0.7656\n",
      "Epoch 827/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.3820 - acc: 0.8438 - val_loss: 0.5432 - val_acc: 0.7604\n",
      "Epoch 828/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3817 - acc: 0.8438 - val_loss: 0.5440 - val_acc: 0.7656\n",
      "Epoch 829/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.3817 - acc: 0.8438 - val_loss: 0.5438 - val_acc: 0.7656\n",
      "Epoch 830/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3819 - acc: 0.8438 - val_loss: 0.5429 - val_acc: 0.7604\n",
      "Epoch 831/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3815 - acc: 0.8438 - val_loss: 0.5425 - val_acc: 0.7604\n",
      "Epoch 832/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3817 - acc: 0.8438 - val_loss: 0.5437 - val_acc: 0.7656\n",
      "Epoch 833/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3816 - acc: 0.8403 - val_loss: 0.5433 - val_acc: 0.7656\n",
      "Epoch 834/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3819 - acc: 0.8420 - val_loss: 0.5431 - val_acc: 0.7656\n",
      "Epoch 835/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3819 - acc: 0.8455 - val_loss: 0.5425 - val_acc: 0.7604\n",
      "Epoch 836/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3819 - acc: 0.8420 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 837/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3814 - acc: 0.8438 - val_loss: 0.5436 - val_acc: 0.7656\n",
      "Epoch 838/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3814 - acc: 0.8438 - val_loss: 0.5436 - val_acc: 0.7656\n",
      "Epoch 839/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3814 - acc: 0.8455 - val_loss: 0.5440 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 840/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3817 - acc: 0.8420 - val_loss: 0.5448 - val_acc: 0.7604\n",
      "Epoch 841/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3815 - acc: 0.8420 - val_loss: 0.5444 - val_acc: 0.7656\n",
      "Epoch 842/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3812 - acc: 0.8455 - val_loss: 0.5436 - val_acc: 0.7656\n",
      "Epoch 843/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3816 - acc: 0.8403 - val_loss: 0.5437 - val_acc: 0.7656\n",
      "Epoch 844/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3817 - acc: 0.8420 - val_loss: 0.5431 - val_acc: 0.7656\n",
      "Epoch 845/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3814 - acc: 0.8455 - val_loss: 0.5437 - val_acc: 0.7656\n",
      "Epoch 846/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3814 - acc: 0.8438 - val_loss: 0.5442 - val_acc: 0.7656\n",
      "Epoch 847/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3814 - acc: 0.8420 - val_loss: 0.5431 - val_acc: 0.7656\n",
      "Epoch 848/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3813 - acc: 0.8403 - val_loss: 0.5433 - val_acc: 0.7604\n",
      "Epoch 849/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.3813 - acc: 0.8420 - val_loss: 0.5434 - val_acc: 0.7656\n",
      "Epoch 850/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.3811 - acc: 0.8438 - val_loss: 0.5433 - val_acc: 0.7656\n",
      "Epoch 851/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3809 - acc: 0.8420 - val_loss: 0.5443 - val_acc: 0.7708\n",
      "Epoch 852/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3813 - acc: 0.8420 - val_loss: 0.5435 - val_acc: 0.7604\n",
      "Epoch 853/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3812 - acc: 0.8420 - val_loss: 0.5444 - val_acc: 0.7656\n",
      "Epoch 854/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3815 - acc: 0.8403 - val_loss: 0.5441 - val_acc: 0.7656\n",
      "Epoch 855/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3813 - acc: 0.8455 - val_loss: 0.5437 - val_acc: 0.7656\n",
      "Epoch 856/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3807 - acc: 0.8420 - val_loss: 0.5434 - val_acc: 0.7656\n",
      "Epoch 857/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3810 - acc: 0.8438 - val_loss: 0.5429 - val_acc: 0.7656\n",
      "Epoch 858/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3811 - acc: 0.8420 - val_loss: 0.5433 - val_acc: 0.7656\n",
      "Epoch 859/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3808 - acc: 0.8438 - val_loss: 0.5432 - val_acc: 0.7656\n",
      "Epoch 860/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3809 - acc: 0.8438 - val_loss: 0.5431 - val_acc: 0.7656\n",
      "Epoch 861/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3807 - acc: 0.8420 - val_loss: 0.5434 - val_acc: 0.7656\n",
      "Epoch 862/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.3807 - acc: 0.8403 - val_loss: 0.5436 - val_acc: 0.7604\n",
      "Epoch 863/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3808 - acc: 0.8420 - val_loss: 0.5440 - val_acc: 0.7604\n",
      "Epoch 864/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3810 - acc: 0.8403 - val_loss: 0.5439 - val_acc: 0.7552\n",
      "Epoch 865/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3807 - acc: 0.8420 - val_loss: 0.5442 - val_acc: 0.7604\n",
      "Epoch 866/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3808 - acc: 0.8438 - val_loss: 0.5441 - val_acc: 0.7604\n",
      "Epoch 867/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3809 - acc: 0.8403 - val_loss: 0.5443 - val_acc: 0.7604\n",
      "Epoch 868/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3807 - acc: 0.8420 - val_loss: 0.5443 - val_acc: 0.7656\n",
      "Epoch 869/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3805 - acc: 0.8420 - val_loss: 0.5445 - val_acc: 0.7604\n",
      "Epoch 870/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3805 - acc: 0.8438 - val_loss: 0.5444 - val_acc: 0.7604\n",
      "Epoch 871/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3806 - acc: 0.8438 - val_loss: 0.5442 - val_acc: 0.7552\n",
      "Epoch 872/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3804 - acc: 0.8420 - val_loss: 0.5442 - val_acc: 0.7604\n",
      "Epoch 873/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3805 - acc: 0.8420 - val_loss: 0.5439 - val_acc: 0.7552\n",
      "Epoch 874/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3803 - acc: 0.8420 - val_loss: 0.5443 - val_acc: 0.7552\n",
      "Epoch 875/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3806 - acc: 0.8420 - val_loss: 0.5443 - val_acc: 0.7552\n",
      "Epoch 876/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3805 - acc: 0.8420 - val_loss: 0.5447 - val_acc: 0.7552\n",
      "Epoch 877/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3801 - acc: 0.8438 - val_loss: 0.5444 - val_acc: 0.7552\n",
      "Epoch 878/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3802 - acc: 0.8420 - val_loss: 0.5447 - val_acc: 0.7552\n",
      "Epoch 879/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3806 - acc: 0.8438 - val_loss: 0.5449 - val_acc: 0.7552\n",
      "Epoch 880/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3802 - acc: 0.8420 - val_loss: 0.5447 - val_acc: 0.7552\n",
      "Epoch 881/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3803 - acc: 0.8420 - val_loss: 0.5448 - val_acc: 0.7552\n",
      "Epoch 882/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3801 - acc: 0.8420 - val_loss: 0.5449 - val_acc: 0.7552\n",
      "Epoch 883/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3805 - acc: 0.8420 - val_loss: 0.5453 - val_acc: 0.7552\n",
      "Epoch 884/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3802 - acc: 0.8438 - val_loss: 0.5449 - val_acc: 0.7656\n",
      "Epoch 885/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3802 - acc: 0.8438 - val_loss: 0.5445 - val_acc: 0.7552\n",
      "Epoch 886/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3802 - acc: 0.8420 - val_loss: 0.5447 - val_acc: 0.7656\n",
      "Epoch 887/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3801 - acc: 0.8403 - val_loss: 0.5447 - val_acc: 0.7552\n",
      "Epoch 888/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3799 - acc: 0.8403 - val_loss: 0.5452 - val_acc: 0.7552\n",
      "Epoch 889/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3802 - acc: 0.8455 - val_loss: 0.5450 - val_acc: 0.7552\n",
      "Epoch 890/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3799 - acc: 0.8438 - val_loss: 0.5449 - val_acc: 0.7552\n",
      "Epoch 891/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3799 - acc: 0.8420 - val_loss: 0.5451 - val_acc: 0.7552\n",
      "Epoch 892/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3799 - acc: 0.8438 - val_loss: 0.5447 - val_acc: 0.7552\n",
      "Epoch 893/1500\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.3797 - acc: 0.8403 - val_loss: 0.5456 - val_acc: 0.7604\n",
      "Epoch 894/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3799 - acc: 0.8420 - val_loss: 0.5447 - val_acc: 0.7552\n",
      "Epoch 895/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3797 - acc: 0.8438 - val_loss: 0.5442 - val_acc: 0.7604\n",
      "Epoch 896/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3800 - acc: 0.8438 - val_loss: 0.5444 - val_acc: 0.7604\n",
      "Epoch 897/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3797 - acc: 0.8403 - val_loss: 0.5451 - val_acc: 0.7604\n",
      "Epoch 898/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3800 - acc: 0.8420 - val_loss: 0.5451 - val_acc: 0.7552\n",
      "Epoch 899/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3799 - acc: 0.8420 - val_loss: 0.5446 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3796 - acc: 0.8420 - val_loss: 0.5458 - val_acc: 0.7656\n",
      "Epoch 901/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3798 - acc: 0.8438 - val_loss: 0.5463 - val_acc: 0.7552\n",
      "Epoch 902/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3796 - acc: 0.8420 - val_loss: 0.5453 - val_acc: 0.7552\n",
      "Epoch 903/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3797 - acc: 0.8403 - val_loss: 0.5450 - val_acc: 0.7552\n",
      "Epoch 904/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3799 - acc: 0.8438 - val_loss: 0.5459 - val_acc: 0.7656\n",
      "Epoch 905/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3797 - acc: 0.8420 - val_loss: 0.5456 - val_acc: 0.7552\n",
      "Epoch 906/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3799 - acc: 0.8420 - val_loss: 0.5462 - val_acc: 0.7552\n",
      "Epoch 907/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3795 - acc: 0.8420 - val_loss: 0.5465 - val_acc: 0.7552\n",
      "Epoch 908/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3795 - acc: 0.8438 - val_loss: 0.5457 - val_acc: 0.7604\n",
      "Epoch 909/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3795 - acc: 0.8420 - val_loss: 0.5469 - val_acc: 0.7656\n",
      "Epoch 910/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3795 - acc: 0.8438 - val_loss: 0.5462 - val_acc: 0.7656\n",
      "Epoch 911/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.3795 - acc: 0.8438 - val_loss: 0.5461 - val_acc: 0.7656\n",
      "Epoch 912/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3799 - acc: 0.8420 - val_loss: 0.5463 - val_acc: 0.7604\n",
      "Epoch 913/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3793 - acc: 0.8420 - val_loss: 0.5456 - val_acc: 0.7604\n",
      "Epoch 914/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3794 - acc: 0.8438 - val_loss: 0.5456 - val_acc: 0.7604\n",
      "Epoch 915/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3793 - acc: 0.8403 - val_loss: 0.5466 - val_acc: 0.7604\n",
      "Epoch 916/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3797 - acc: 0.8403 - val_loss: 0.5471 - val_acc: 0.7552\n",
      "Epoch 917/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3795 - acc: 0.8438 - val_loss: 0.5464 - val_acc: 0.7604\n",
      "Epoch 918/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3795 - acc: 0.8438 - val_loss: 0.5466 - val_acc: 0.7604\n",
      "Epoch 919/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3793 - acc: 0.8420 - val_loss: 0.5459 - val_acc: 0.7604\n",
      "Epoch 920/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3793 - acc: 0.8438 - val_loss: 0.5470 - val_acc: 0.7604\n",
      "Epoch 921/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3792 - acc: 0.8385 - val_loss: 0.5478 - val_acc: 0.7552\n",
      "Epoch 922/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3794 - acc: 0.8403 - val_loss: 0.5468 - val_acc: 0.7604\n",
      "Epoch 923/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3792 - acc: 0.8420 - val_loss: 0.5464 - val_acc: 0.7604\n",
      "Epoch 924/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3790 - acc: 0.8438 - val_loss: 0.5459 - val_acc: 0.7604\n",
      "Epoch 925/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3793 - acc: 0.8420 - val_loss: 0.5466 - val_acc: 0.7656\n",
      "Epoch 926/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3788 - acc: 0.8420 - val_loss: 0.5460 - val_acc: 0.7552\n",
      "Epoch 927/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3793 - acc: 0.8420 - val_loss: 0.5459 - val_acc: 0.7604\n",
      "Epoch 928/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3791 - acc: 0.8438 - val_loss: 0.5467 - val_acc: 0.7604\n",
      "Epoch 929/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3791 - acc: 0.8403 - val_loss: 0.5475 - val_acc: 0.7604\n",
      "Epoch 930/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3790 - acc: 0.8438 - val_loss: 0.5473 - val_acc: 0.7604\n",
      "Epoch 931/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3792 - acc: 0.8420 - val_loss: 0.5467 - val_acc: 0.7604\n",
      "Epoch 932/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3791 - acc: 0.8420 - val_loss: 0.5465 - val_acc: 0.7604\n",
      "Epoch 933/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3793 - acc: 0.8455 - val_loss: 0.5460 - val_acc: 0.7552\n",
      "Epoch 934/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3791 - acc: 0.8455 - val_loss: 0.5469 - val_acc: 0.7604\n",
      "Epoch 935/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3790 - acc: 0.8438 - val_loss: 0.5477 - val_acc: 0.7604\n",
      "Epoch 936/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3792 - acc: 0.8403 - val_loss: 0.5469 - val_acc: 0.7604\n",
      "Epoch 937/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3791 - acc: 0.8403 - val_loss: 0.5468 - val_acc: 0.7552\n",
      "Epoch 938/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3788 - acc: 0.8420 - val_loss: 0.5470 - val_acc: 0.7656\n",
      "Epoch 939/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3790 - acc: 0.8403 - val_loss: 0.5469 - val_acc: 0.7604\n",
      "Epoch 940/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3790 - acc: 0.8438 - val_loss: 0.5468 - val_acc: 0.7656\n",
      "Epoch 941/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3788 - acc: 0.8420 - val_loss: 0.5461 - val_acc: 0.7552\n",
      "Epoch 942/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3787 - acc: 0.8403 - val_loss: 0.5456 - val_acc: 0.7552\n",
      "Epoch 943/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3789 - acc: 0.8420 - val_loss: 0.5460 - val_acc: 0.7552\n",
      "Epoch 944/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3789 - acc: 0.8420 - val_loss: 0.5465 - val_acc: 0.7552\n",
      "Epoch 945/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3786 - acc: 0.8420 - val_loss: 0.5465 - val_acc: 0.7552\n",
      "Epoch 946/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3788 - acc: 0.8438 - val_loss: 0.5470 - val_acc: 0.7604\n",
      "Epoch 947/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3786 - acc: 0.8455 - val_loss: 0.5480 - val_acc: 0.7656\n",
      "Epoch 948/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3789 - acc: 0.8403 - val_loss: 0.5486 - val_acc: 0.7604\n",
      "Epoch 949/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3786 - acc: 0.8420 - val_loss: 0.5473 - val_acc: 0.7604\n",
      "Epoch 950/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3788 - acc: 0.8403 - val_loss: 0.5475 - val_acc: 0.7604\n",
      "Epoch 951/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3787 - acc: 0.8403 - val_loss: 0.5477 - val_acc: 0.7604\n",
      "Epoch 952/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3788 - acc: 0.8420 - val_loss: 0.5478 - val_acc: 0.7604\n",
      "Epoch 953/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3789 - acc: 0.8420 - val_loss: 0.5468 - val_acc: 0.7552\n",
      "Epoch 954/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3786 - acc: 0.8420 - val_loss: 0.5477 - val_acc: 0.7552\n",
      "Epoch 955/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3787 - acc: 0.8438 - val_loss: 0.5479 - val_acc: 0.7604\n",
      "Epoch 956/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3785 - acc: 0.8420 - val_loss: 0.5481 - val_acc: 0.7604\n",
      "Epoch 957/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3785 - acc: 0.8438 - val_loss: 0.5471 - val_acc: 0.7552\n",
      "Epoch 958/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3786 - acc: 0.8403 - val_loss: 0.5485 - val_acc: 0.7604\n",
      "Epoch 959/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3783 - acc: 0.8438 - val_loss: 0.5479 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 960/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3783 - acc: 0.8438 - val_loss: 0.5475 - val_acc: 0.7552\n",
      "Epoch 961/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3785 - acc: 0.8403 - val_loss: 0.5478 - val_acc: 0.7604\n",
      "Epoch 962/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3782 - acc: 0.8420 - val_loss: 0.5486 - val_acc: 0.7604\n",
      "Epoch 963/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3784 - acc: 0.8420 - val_loss: 0.5489 - val_acc: 0.7604\n",
      "Epoch 964/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3783 - acc: 0.8438 - val_loss: 0.5486 - val_acc: 0.7604\n",
      "Epoch 965/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3785 - acc: 0.8438 - val_loss: 0.5483 - val_acc: 0.7604\n",
      "Epoch 966/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3782 - acc: 0.8438 - val_loss: 0.5481 - val_acc: 0.7604\n",
      "Epoch 967/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3782 - acc: 0.8438 - val_loss: 0.5476 - val_acc: 0.7604\n",
      "Epoch 968/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3780 - acc: 0.8403 - val_loss: 0.5476 - val_acc: 0.7604\n",
      "Epoch 969/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3782 - acc: 0.8438 - val_loss: 0.5484 - val_acc: 0.7604\n",
      "Epoch 970/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3784 - acc: 0.8438 - val_loss: 0.5475 - val_acc: 0.7552\n",
      "Epoch 971/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3782 - acc: 0.8403 - val_loss: 0.5474 - val_acc: 0.7552\n",
      "Epoch 972/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3782 - acc: 0.8420 - val_loss: 0.5484 - val_acc: 0.7604\n",
      "Epoch 973/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3784 - acc: 0.8403 - val_loss: 0.5491 - val_acc: 0.7604\n",
      "Epoch 974/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3783 - acc: 0.8455 - val_loss: 0.5491 - val_acc: 0.7604\n",
      "Epoch 975/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3781 - acc: 0.8420 - val_loss: 0.5489 - val_acc: 0.7604\n",
      "Epoch 976/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3778 - acc: 0.8438 - val_loss: 0.5479 - val_acc: 0.7552\n",
      "Epoch 977/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3784 - acc: 0.8420 - val_loss: 0.5483 - val_acc: 0.7604\n",
      "Epoch 978/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3786 - acc: 0.8403 - val_loss: 0.5483 - val_acc: 0.7656\n",
      "Epoch 979/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3781 - acc: 0.8420 - val_loss: 0.5483 - val_acc: 0.7552\n",
      "Epoch 980/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3781 - acc: 0.8403 - val_loss: 0.5490 - val_acc: 0.7552\n",
      "Epoch 981/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3782 - acc: 0.8420 - val_loss: 0.5496 - val_acc: 0.7604\n",
      "Epoch 982/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.3778 - acc: 0.8420 - val_loss: 0.5491 - val_acc: 0.7604\n",
      "Epoch 983/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3778 - acc: 0.8420 - val_loss: 0.5488 - val_acc: 0.7604\n",
      "Epoch 984/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3779 - acc: 0.8403 - val_loss: 0.5486 - val_acc: 0.7604\n",
      "Epoch 985/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3780 - acc: 0.8420 - val_loss: 0.5489 - val_acc: 0.7604\n",
      "Epoch 986/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3777 - acc: 0.8420 - val_loss: 0.5487 - val_acc: 0.7604\n",
      "Epoch 987/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3776 - acc: 0.8385 - val_loss: 0.5493 - val_acc: 0.7604\n",
      "Epoch 988/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3778 - acc: 0.8438 - val_loss: 0.5490 - val_acc: 0.7604\n",
      "Epoch 989/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3777 - acc: 0.8455 - val_loss: 0.5486 - val_acc: 0.7552\n",
      "Epoch 990/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3778 - acc: 0.8438 - val_loss: 0.5493 - val_acc: 0.7604\n",
      "Epoch 991/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3775 - acc: 0.8420 - val_loss: 0.5492 - val_acc: 0.7604\n",
      "Epoch 992/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3775 - acc: 0.8438 - val_loss: 0.5487 - val_acc: 0.7552\n",
      "Epoch 993/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3774 - acc: 0.8403 - val_loss: 0.5484 - val_acc: 0.7552\n",
      "Epoch 994/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3778 - acc: 0.8420 - val_loss: 0.5489 - val_acc: 0.7552\n",
      "Epoch 995/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3776 - acc: 0.8420 - val_loss: 0.5495 - val_acc: 0.7552\n",
      "Epoch 996/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3776 - acc: 0.8438 - val_loss: 0.5510 - val_acc: 0.7604\n",
      "Epoch 997/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3774 - acc: 0.8438 - val_loss: 0.5501 - val_acc: 0.7604\n",
      "Epoch 998/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3780 - acc: 0.8438 - val_loss: 0.5495 - val_acc: 0.7552\n",
      "Epoch 999/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3773 - acc: 0.8438 - val_loss: 0.5501 - val_acc: 0.7604\n",
      "Epoch 1000/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3772 - acc: 0.8455 - val_loss: 0.5500 - val_acc: 0.7604\n",
      "Epoch 1001/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3775 - acc: 0.8403 - val_loss: 0.5500 - val_acc: 0.7552\n",
      "Epoch 1002/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3777 - acc: 0.8420 - val_loss: 0.5504 - val_acc: 0.7604\n",
      "Epoch 1003/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3774 - acc: 0.8403 - val_loss: 0.5506 - val_acc: 0.7604\n",
      "Epoch 1004/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3777 - acc: 0.8420 - val_loss: 0.5495 - val_acc: 0.7552\n",
      "Epoch 1005/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3775 - acc: 0.8403 - val_loss: 0.5504 - val_acc: 0.7604\n",
      "Epoch 1006/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3777 - acc: 0.8420 - val_loss: 0.5504 - val_acc: 0.7604\n",
      "Epoch 1007/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3771 - acc: 0.8438 - val_loss: 0.5502 - val_acc: 0.7552\n",
      "Epoch 1008/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3774 - acc: 0.8438 - val_loss: 0.5508 - val_acc: 0.7604\n",
      "Epoch 1009/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3772 - acc: 0.8420 - val_loss: 0.5504 - val_acc: 0.7552\n",
      "Epoch 1010/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3769 - acc: 0.8455 - val_loss: 0.5504 - val_acc: 0.7552\n",
      "Epoch 1011/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3775 - acc: 0.8385 - val_loss: 0.5517 - val_acc: 0.7604\n",
      "Epoch 1012/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3771 - acc: 0.8438 - val_loss: 0.5514 - val_acc: 0.7604\n",
      "Epoch 1013/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3772 - acc: 0.8403 - val_loss: 0.5524 - val_acc: 0.7604\n",
      "Epoch 1014/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3775 - acc: 0.8438 - val_loss: 0.5528 - val_acc: 0.7604\n",
      "Epoch 1015/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3769 - acc: 0.8403 - val_loss: 0.5526 - val_acc: 0.7604\n",
      "Epoch 1016/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3771 - acc: 0.8455 - val_loss: 0.5522 - val_acc: 0.7604\n",
      "Epoch 1017/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3770 - acc: 0.8403 - val_loss: 0.5518 - val_acc: 0.7604\n",
      "Epoch 1018/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3767 - acc: 0.8403 - val_loss: 0.5527 - val_acc: 0.7604\n",
      "Epoch 1019/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 67us/step - loss: 0.3769 - acc: 0.8472 - val_loss: 0.5524 - val_acc: 0.7604\n",
      "Epoch 1020/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3770 - acc: 0.8438 - val_loss: 0.5520 - val_acc: 0.7604\n",
      "Epoch 1021/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3769 - acc: 0.8438 - val_loss: 0.5521 - val_acc: 0.7604\n",
      "Epoch 1022/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3769 - acc: 0.8438 - val_loss: 0.5517 - val_acc: 0.7604\n",
      "Epoch 1023/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3772 - acc: 0.8403 - val_loss: 0.5521 - val_acc: 0.7604\n",
      "Epoch 1024/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3766 - acc: 0.8420 - val_loss: 0.5522 - val_acc: 0.7604\n",
      "Epoch 1025/1500\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.3766 - acc: 0.8455 - val_loss: 0.5529 - val_acc: 0.7604\n",
      "Epoch 1026/1500\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.3766 - acc: 0.8403 - val_loss: 0.5530 - val_acc: 0.7552\n",
      "Epoch 1027/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.3767 - acc: 0.8472 - val_loss: 0.5524 - val_acc: 0.7604\n",
      "Epoch 1028/1500\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.3763 - acc: 0.8438 - val_loss: 0.5526 - val_acc: 0.7604\n",
      "Epoch 1029/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.3768 - acc: 0.8472 - val_loss: 0.5527 - val_acc: 0.7604\n",
      "Epoch 1030/1500\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.3768 - acc: 0.8472 - val_loss: 0.5530 - val_acc: 0.7552\n",
      "Epoch 1031/1500\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.3768 - acc: 0.8472 - val_loss: 0.5534 - val_acc: 0.7552\n",
      "Epoch 1032/1500\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.3764 - acc: 0.8455 - val_loss: 0.5525 - val_acc: 0.7552\n",
      "Epoch 1033/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3765 - acc: 0.8420 - val_loss: 0.5526 - val_acc: 0.7656\n",
      "Epoch 1034/1500\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.3774 - acc: 0.8420 - val_loss: 0.5530 - val_acc: 0.7656\n",
      "Epoch 1035/1500\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.3763 - acc: 0.8403 - val_loss: 0.5520 - val_acc: 0.7604\n",
      "Epoch 1036/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3764 - acc: 0.8438 - val_loss: 0.5522 - val_acc: 0.7604\n",
      "Epoch 1037/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3766 - acc: 0.8438 - val_loss: 0.5525 - val_acc: 0.7604\n",
      "Epoch 1038/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3765 - acc: 0.8438 - val_loss: 0.5534 - val_acc: 0.7604\n",
      "Epoch 1039/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3765 - acc: 0.8403 - val_loss: 0.5539 - val_acc: 0.7604\n",
      "Epoch 1040/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3762 - acc: 0.8420 - val_loss: 0.5540 - val_acc: 0.7604\n",
      "Epoch 1041/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3764 - acc: 0.8385 - val_loss: 0.5543 - val_acc: 0.7604\n",
      "Epoch 1042/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3769 - acc: 0.8438 - val_loss: 0.5531 - val_acc: 0.7604\n",
      "Epoch 1043/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3759 - acc: 0.8403 - val_loss: 0.5535 - val_acc: 0.7656\n",
      "Epoch 1044/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3762 - acc: 0.8420 - val_loss: 0.5534 - val_acc: 0.7656\n",
      "Epoch 1045/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3761 - acc: 0.8403 - val_loss: 0.5535 - val_acc: 0.7656\n",
      "Epoch 1046/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3764 - acc: 0.8385 - val_loss: 0.5544 - val_acc: 0.7656\n",
      "Epoch 1047/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3763 - acc: 0.8403 - val_loss: 0.5534 - val_acc: 0.7604\n",
      "Epoch 1048/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3765 - acc: 0.8420 - val_loss: 0.5531 - val_acc: 0.7604\n",
      "Epoch 1049/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3761 - acc: 0.8455 - val_loss: 0.5538 - val_acc: 0.7604\n",
      "Epoch 1050/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3763 - acc: 0.8420 - val_loss: 0.5536 - val_acc: 0.7604\n",
      "Epoch 1051/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3761 - acc: 0.8403 - val_loss: 0.5532 - val_acc: 0.7604\n",
      "Epoch 1052/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3760 - acc: 0.8472 - val_loss: 0.5531 - val_acc: 0.7552\n",
      "Epoch 1053/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3761 - acc: 0.8438 - val_loss: 0.5538 - val_acc: 0.7604\n",
      "Epoch 1054/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3762 - acc: 0.8420 - val_loss: 0.5535 - val_acc: 0.7604\n",
      "Epoch 1055/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3759 - acc: 0.8403 - val_loss: 0.5529 - val_acc: 0.7604\n",
      "Epoch 1056/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3757 - acc: 0.8438 - val_loss: 0.5538 - val_acc: 0.7604\n",
      "Epoch 1057/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3762 - acc: 0.8420 - val_loss: 0.5539 - val_acc: 0.7604\n",
      "Epoch 1058/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3760 - acc: 0.8351 - val_loss: 0.5542 - val_acc: 0.7604\n",
      "Epoch 1059/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3760 - acc: 0.8455 - val_loss: 0.5549 - val_acc: 0.7656\n",
      "Epoch 1060/1500\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.3757 - acc: 0.8438 - val_loss: 0.5551 - val_acc: 0.7656\n",
      "Epoch 1061/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3759 - acc: 0.8455 - val_loss: 0.5552 - val_acc: 0.7656\n",
      "Epoch 1062/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3760 - acc: 0.8455 - val_loss: 0.5551 - val_acc: 0.7656\n",
      "Epoch 1063/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3758 - acc: 0.8385 - val_loss: 0.5557 - val_acc: 0.7656\n",
      "Epoch 1064/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3759 - acc: 0.8438 - val_loss: 0.5557 - val_acc: 0.7656\n",
      "Epoch 1065/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3757 - acc: 0.8403 - val_loss: 0.5555 - val_acc: 0.7656\n",
      "Epoch 1066/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3760 - acc: 0.8455 - val_loss: 0.5543 - val_acc: 0.7604\n",
      "Epoch 1067/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3757 - acc: 0.8455 - val_loss: 0.5552 - val_acc: 0.7656\n",
      "Epoch 1068/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3755 - acc: 0.8420 - val_loss: 0.5553 - val_acc: 0.7656\n",
      "Epoch 1069/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3753 - acc: 0.8420 - val_loss: 0.5558 - val_acc: 0.7656\n",
      "Epoch 1070/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3754 - acc: 0.8455 - val_loss: 0.5543 - val_acc: 0.7604\n",
      "Epoch 1071/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.3757 - acc: 0.8403 - val_loss: 0.5546 - val_acc: 0.7604\n",
      "Epoch 1072/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3760 - acc: 0.8420 - val_loss: 0.5556 - val_acc: 0.7604\n",
      "Epoch 1073/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3755 - acc: 0.8403 - val_loss: 0.5556 - val_acc: 0.7552\n",
      "Epoch 1074/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3755 - acc: 0.8438 - val_loss: 0.5558 - val_acc: 0.7604\n",
      "Epoch 1075/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3752 - acc: 0.8455 - val_loss: 0.5553 - val_acc: 0.7604\n",
      "Epoch 1076/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3757 - acc: 0.8455 - val_loss: 0.5557 - val_acc: 0.7656\n",
      "Epoch 1077/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3753 - acc: 0.8420 - val_loss: 0.5549 - val_acc: 0.7604\n",
      "Epoch 1078/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 87us/step - loss: 0.3756 - acc: 0.8420 - val_loss: 0.5557 - val_acc: 0.7656\n",
      "Epoch 1079/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3755 - acc: 0.8420 - val_loss: 0.5553 - val_acc: 0.7604\n",
      "Epoch 1080/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3753 - acc: 0.8403 - val_loss: 0.5558 - val_acc: 0.7552\n",
      "Epoch 1081/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3753 - acc: 0.8420 - val_loss: 0.5554 - val_acc: 0.7552\n",
      "Epoch 1082/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3753 - acc: 0.8438 - val_loss: 0.5564 - val_acc: 0.7604\n",
      "Epoch 1083/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3755 - acc: 0.8438 - val_loss: 0.5558 - val_acc: 0.7604\n",
      "Epoch 1084/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3753 - acc: 0.8403 - val_loss: 0.5556 - val_acc: 0.7552\n",
      "Epoch 1085/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3751 - acc: 0.8455 - val_loss: 0.5554 - val_acc: 0.7552\n",
      "Epoch 1086/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3752 - acc: 0.8420 - val_loss: 0.5558 - val_acc: 0.7552\n",
      "Epoch 1087/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3752 - acc: 0.8385 - val_loss: 0.5571 - val_acc: 0.7604\n",
      "Epoch 1088/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3752 - acc: 0.8438 - val_loss: 0.5568 - val_acc: 0.7604\n",
      "Epoch 1089/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3751 - acc: 0.8455 - val_loss: 0.5566 - val_acc: 0.7604\n",
      "Epoch 1090/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3756 - acc: 0.8455 - val_loss: 0.5564 - val_acc: 0.7656\n",
      "Epoch 1091/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3750 - acc: 0.8420 - val_loss: 0.5564 - val_acc: 0.7500\n",
      "Epoch 1092/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3750 - acc: 0.8420 - val_loss: 0.5568 - val_acc: 0.7604\n",
      "Epoch 1093/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3748 - acc: 0.8455 - val_loss: 0.5563 - val_acc: 0.7604\n",
      "Epoch 1094/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3753 - acc: 0.8455 - val_loss: 0.5566 - val_acc: 0.7604\n",
      "Epoch 1095/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3751 - acc: 0.8472 - val_loss: 0.5570 - val_acc: 0.7604\n",
      "Epoch 1096/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3750 - acc: 0.8420 - val_loss: 0.5563 - val_acc: 0.7552\n",
      "Epoch 1097/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3752 - acc: 0.8420 - val_loss: 0.5573 - val_acc: 0.7604\n",
      "Epoch 1098/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3750 - acc: 0.8455 - val_loss: 0.5571 - val_acc: 0.7604\n",
      "Epoch 1099/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3750 - acc: 0.8472 - val_loss: 0.5566 - val_acc: 0.7500\n",
      "Epoch 1100/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3747 - acc: 0.8438 - val_loss: 0.5569 - val_acc: 0.7500\n",
      "Epoch 1101/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3748 - acc: 0.8438 - val_loss: 0.5573 - val_acc: 0.7500\n",
      "Epoch 1102/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3753 - acc: 0.8420 - val_loss: 0.5577 - val_acc: 0.7604\n",
      "Epoch 1103/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3749 - acc: 0.8455 - val_loss: 0.5575 - val_acc: 0.7552\n",
      "Epoch 1104/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3751 - acc: 0.8455 - val_loss: 0.5571 - val_acc: 0.7552\n",
      "Epoch 1105/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3749 - acc: 0.8403 - val_loss: 0.5582 - val_acc: 0.7552\n",
      "Epoch 1106/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3748 - acc: 0.8420 - val_loss: 0.5583 - val_acc: 0.7552\n",
      "Epoch 1107/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3751 - acc: 0.8438 - val_loss: 0.5580 - val_acc: 0.7552\n",
      "Epoch 1108/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3748 - acc: 0.8455 - val_loss: 0.5575 - val_acc: 0.7500\n",
      "Epoch 1109/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3745 - acc: 0.8420 - val_loss: 0.5587 - val_acc: 0.7552\n",
      "Epoch 1110/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3750 - acc: 0.8438 - val_loss: 0.5591 - val_acc: 0.7552\n",
      "Epoch 1111/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3746 - acc: 0.8455 - val_loss: 0.5581 - val_acc: 0.7552\n",
      "Epoch 1112/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3750 - acc: 0.8420 - val_loss: 0.5581 - val_acc: 0.7552\n",
      "Epoch 1113/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3748 - acc: 0.8455 - val_loss: 0.5575 - val_acc: 0.7552\n",
      "Epoch 1114/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3745 - acc: 0.8438 - val_loss: 0.5577 - val_acc: 0.7500\n",
      "Epoch 1115/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3747 - acc: 0.8438 - val_loss: 0.5583 - val_acc: 0.7500\n",
      "Epoch 1116/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3746 - acc: 0.8438 - val_loss: 0.5587 - val_acc: 0.7552\n",
      "Epoch 1117/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3745 - acc: 0.8455 - val_loss: 0.5582 - val_acc: 0.7500\n",
      "Epoch 1118/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3746 - acc: 0.8455 - val_loss: 0.5581 - val_acc: 0.7500\n",
      "Epoch 1119/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3744 - acc: 0.8472 - val_loss: 0.5581 - val_acc: 0.7500\n",
      "Epoch 1120/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3744 - acc: 0.8490 - val_loss: 0.5579 - val_acc: 0.7500\n",
      "Epoch 1121/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3747 - acc: 0.8438 - val_loss: 0.5574 - val_acc: 0.7552\n",
      "Epoch 1122/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3745 - acc: 0.8420 - val_loss: 0.5588 - val_acc: 0.7604\n",
      "Epoch 1123/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3743 - acc: 0.8438 - val_loss: 0.5591 - val_acc: 0.7604\n",
      "Epoch 1124/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3744 - acc: 0.8472 - val_loss: 0.5580 - val_acc: 0.7552\n",
      "Epoch 1125/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3745 - acc: 0.8403 - val_loss: 0.5587 - val_acc: 0.7500\n",
      "Epoch 1126/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3748 - acc: 0.8455 - val_loss: 0.5586 - val_acc: 0.7500\n",
      "Epoch 1127/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3743 - acc: 0.8455 - val_loss: 0.5587 - val_acc: 0.7500\n",
      "Epoch 1128/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3746 - acc: 0.8472 - val_loss: 0.5587 - val_acc: 0.7500\n",
      "Epoch 1129/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3744 - acc: 0.8438 - val_loss: 0.5590 - val_acc: 0.7500\n",
      "Epoch 1130/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3741 - acc: 0.8455 - val_loss: 0.5591 - val_acc: 0.7500\n",
      "Epoch 1131/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3743 - acc: 0.8438 - val_loss: 0.5593 - val_acc: 0.7552\n",
      "Epoch 1132/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.3743 - acc: 0.8455 - val_loss: 0.5579 - val_acc: 0.7500\n",
      "Epoch 1133/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3743 - acc: 0.8403 - val_loss: 0.5595 - val_acc: 0.7552\n",
      "Epoch 1134/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3741 - acc: 0.8490 - val_loss: 0.5591 - val_acc: 0.7552\n",
      "Epoch 1135/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3743 - acc: 0.8420 - val_loss: 0.5597 - val_acc: 0.7552\n",
      "Epoch 1136/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3742 - acc: 0.8455 - val_loss: 0.5593 - val_acc: 0.7500\n",
      "Epoch 1137/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 62us/step - loss: 0.3742 - acc: 0.8438 - val_loss: 0.5596 - val_acc: 0.7500\n",
      "Epoch 1138/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3746 - acc: 0.8472 - val_loss: 0.5589 - val_acc: 0.7500\n",
      "Epoch 1139/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3743 - acc: 0.8455 - val_loss: 0.5597 - val_acc: 0.7500\n",
      "Epoch 1140/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3741 - acc: 0.8438 - val_loss: 0.5607 - val_acc: 0.7552\n",
      "Epoch 1141/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3739 - acc: 0.8455 - val_loss: 0.5609 - val_acc: 0.7552\n",
      "Epoch 1142/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3741 - acc: 0.8472 - val_loss: 0.5606 - val_acc: 0.7552\n",
      "Epoch 1143/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3737 - acc: 0.8472 - val_loss: 0.5604 - val_acc: 0.7552\n",
      "Epoch 1144/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.3738 - acc: 0.8472 - val_loss: 0.5599 - val_acc: 0.7500\n",
      "Epoch 1145/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3740 - acc: 0.8455 - val_loss: 0.5591 - val_acc: 0.7448\n",
      "Epoch 1146/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3741 - acc: 0.8472 - val_loss: 0.5588 - val_acc: 0.7448\n",
      "Epoch 1147/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3741 - acc: 0.8420 - val_loss: 0.5600 - val_acc: 0.7500\n",
      "Epoch 1148/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3736 - acc: 0.8490 - val_loss: 0.5592 - val_acc: 0.7500\n",
      "Epoch 1149/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3742 - acc: 0.8472 - val_loss: 0.5596 - val_acc: 0.7500\n",
      "Epoch 1150/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3741 - acc: 0.8438 - val_loss: 0.5595 - val_acc: 0.7500\n",
      "Epoch 1151/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3735 - acc: 0.8490 - val_loss: 0.5595 - val_acc: 0.7448\n",
      "Epoch 1152/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3741 - acc: 0.8438 - val_loss: 0.5605 - val_acc: 0.7500\n",
      "Epoch 1153/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3735 - acc: 0.8472 - val_loss: 0.5605 - val_acc: 0.7552\n",
      "Epoch 1154/1500\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.3733 - acc: 0.8438 - val_loss: 0.5601 - val_acc: 0.7500\n",
      "Epoch 1155/1500\n",
      "576/576 [==============================] - 0s 145us/step - loss: 0.3736 - acc: 0.8455 - val_loss: 0.5602 - val_acc: 0.7552\n",
      "Epoch 1156/1500\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.3733 - acc: 0.8438 - val_loss: 0.5592 - val_acc: 0.7500\n",
      "Epoch 1157/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3742 - acc: 0.8420 - val_loss: 0.5590 - val_acc: 0.7552\n",
      "Epoch 1158/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3733 - acc: 0.8420 - val_loss: 0.5603 - val_acc: 0.7552\n",
      "Epoch 1159/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3735 - acc: 0.8455 - val_loss: 0.5599 - val_acc: 0.7552\n",
      "Epoch 1160/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3735 - acc: 0.8420 - val_loss: 0.5604 - val_acc: 0.7552\n",
      "Epoch 1161/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3736 - acc: 0.8455 - val_loss: 0.5611 - val_acc: 0.7604\n",
      "Epoch 1162/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3735 - acc: 0.8472 - val_loss: 0.5600 - val_acc: 0.7500\n",
      "Epoch 1163/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3734 - acc: 0.8438 - val_loss: 0.5604 - val_acc: 0.7552\n",
      "Epoch 1164/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3735 - acc: 0.8455 - val_loss: 0.5608 - val_acc: 0.7604\n",
      "Epoch 1165/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3734 - acc: 0.8455 - val_loss: 0.5607 - val_acc: 0.7500\n",
      "Epoch 1166/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3733 - acc: 0.8455 - val_loss: 0.5615 - val_acc: 0.7656\n",
      "Epoch 1167/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.3734 - acc: 0.8455 - val_loss: 0.5608 - val_acc: 0.7552\n",
      "Epoch 1168/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3735 - acc: 0.8438 - val_loss: 0.5612 - val_acc: 0.7604\n",
      "Epoch 1169/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3734 - acc: 0.8472 - val_loss: 0.5605 - val_acc: 0.7552\n",
      "Epoch 1170/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3731 - acc: 0.8472 - val_loss: 0.5600 - val_acc: 0.7500\n",
      "Epoch 1171/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3733 - acc: 0.8438 - val_loss: 0.5603 - val_acc: 0.7500\n",
      "Epoch 1172/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3734 - acc: 0.8438 - val_loss: 0.5601 - val_acc: 0.7500\n",
      "Epoch 1173/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3734 - acc: 0.8438 - val_loss: 0.5611 - val_acc: 0.7604\n",
      "Epoch 1174/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3735 - acc: 0.8438 - val_loss: 0.5613 - val_acc: 0.7604\n",
      "Epoch 1175/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3732 - acc: 0.8455 - val_loss: 0.5611 - val_acc: 0.7552\n",
      "Epoch 1176/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3729 - acc: 0.8455 - val_loss: 0.5616 - val_acc: 0.7604\n",
      "Epoch 1177/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3731 - acc: 0.8490 - val_loss: 0.5602 - val_acc: 0.7500\n",
      "Epoch 1178/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3733 - acc: 0.8472 - val_loss: 0.5596 - val_acc: 0.7500\n",
      "Epoch 1179/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3734 - acc: 0.8420 - val_loss: 0.5607 - val_acc: 0.7500\n",
      "Epoch 1180/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3734 - acc: 0.8420 - val_loss: 0.5608 - val_acc: 0.7552\n",
      "Epoch 1181/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3730 - acc: 0.8438 - val_loss: 0.5608 - val_acc: 0.7552\n",
      "Epoch 1182/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3734 - acc: 0.8455 - val_loss: 0.5603 - val_acc: 0.7500\n",
      "Epoch 1183/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3730 - acc: 0.8455 - val_loss: 0.5603 - val_acc: 0.7448\n",
      "Epoch 1184/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3733 - acc: 0.8455 - val_loss: 0.5602 - val_acc: 0.7552\n",
      "Epoch 1185/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3732 - acc: 0.8420 - val_loss: 0.5589 - val_acc: 0.7500\n",
      "Epoch 1186/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3731 - acc: 0.8438 - val_loss: 0.5599 - val_acc: 0.7500\n",
      "Epoch 1187/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3727 - acc: 0.8455 - val_loss: 0.5596 - val_acc: 0.7500\n",
      "Epoch 1188/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3732 - acc: 0.8438 - val_loss: 0.5601 - val_acc: 0.7500\n",
      "Epoch 1189/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3728 - acc: 0.8455 - val_loss: 0.5594 - val_acc: 0.7500\n",
      "Epoch 1190/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3735 - acc: 0.8438 - val_loss: 0.5594 - val_acc: 0.7500\n",
      "Epoch 1191/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3728 - acc: 0.8420 - val_loss: 0.5601 - val_acc: 0.7448\n",
      "Epoch 1192/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3729 - acc: 0.8455 - val_loss: 0.5590 - val_acc: 0.7500\n",
      "Epoch 1193/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3728 - acc: 0.8438 - val_loss: 0.5601 - val_acc: 0.7396\n",
      "Epoch 1194/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3731 - acc: 0.8438 - val_loss: 0.5600 - val_acc: 0.7448\n",
      "Epoch 1195/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3728 - acc: 0.8438 - val_loss: 0.5598 - val_acc: 0.7396\n",
      "Epoch 1196/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 68us/step - loss: 0.3730 - acc: 0.8438 - val_loss: 0.5605 - val_acc: 0.7448\n",
      "Epoch 1197/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3728 - acc: 0.8490 - val_loss: 0.5596 - val_acc: 0.7396\n",
      "Epoch 1198/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3728 - acc: 0.8438 - val_loss: 0.5608 - val_acc: 0.7500\n",
      "Epoch 1199/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3726 - acc: 0.8455 - val_loss: 0.5597 - val_acc: 0.7500\n",
      "Epoch 1200/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3730 - acc: 0.8455 - val_loss: 0.5610 - val_acc: 0.7500\n",
      "Epoch 1201/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3726 - acc: 0.8455 - val_loss: 0.5613 - val_acc: 0.7500\n",
      "Epoch 1202/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3727 - acc: 0.8403 - val_loss: 0.5614 - val_acc: 0.7552\n",
      "Epoch 1203/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3726 - acc: 0.8438 - val_loss: 0.5611 - val_acc: 0.7448\n",
      "Epoch 1204/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3727 - acc: 0.8438 - val_loss: 0.5600 - val_acc: 0.7448\n",
      "Epoch 1205/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3726 - acc: 0.8438 - val_loss: 0.5594 - val_acc: 0.7500\n",
      "Epoch 1206/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3728 - acc: 0.8403 - val_loss: 0.5604 - val_acc: 0.7448\n",
      "Epoch 1207/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3727 - acc: 0.8455 - val_loss: 0.5613 - val_acc: 0.7500\n",
      "Epoch 1208/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3727 - acc: 0.8455 - val_loss: 0.5600 - val_acc: 0.7396\n",
      "Epoch 1209/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3725 - acc: 0.8455 - val_loss: 0.5600 - val_acc: 0.7396\n",
      "Epoch 1210/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3729 - acc: 0.8420 - val_loss: 0.5597 - val_acc: 0.7500\n",
      "Epoch 1211/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3723 - acc: 0.8455 - val_loss: 0.5605 - val_acc: 0.7500\n",
      "Epoch 1212/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3727 - acc: 0.8438 - val_loss: 0.5601 - val_acc: 0.7500\n",
      "Epoch 1213/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3726 - acc: 0.8455 - val_loss: 0.5608 - val_acc: 0.7448\n",
      "Epoch 1214/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3731 - acc: 0.8420 - val_loss: 0.5603 - val_acc: 0.7448\n",
      "Epoch 1215/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3726 - acc: 0.8438 - val_loss: 0.5605 - val_acc: 0.7500\n",
      "Epoch 1216/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3726 - acc: 0.8403 - val_loss: 0.5596 - val_acc: 0.7396\n",
      "Epoch 1217/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3725 - acc: 0.8438 - val_loss: 0.5610 - val_acc: 0.7500\n",
      "Epoch 1218/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.3725 - acc: 0.8455 - val_loss: 0.5609 - val_acc: 0.7500\n",
      "Epoch 1219/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3722 - acc: 0.8490 - val_loss: 0.5611 - val_acc: 0.7448\n",
      "Epoch 1220/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3722 - acc: 0.8420 - val_loss: 0.5615 - val_acc: 0.7448\n",
      "Epoch 1221/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3724 - acc: 0.8455 - val_loss: 0.5606 - val_acc: 0.7552\n",
      "Epoch 1222/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3723 - acc: 0.8403 - val_loss: 0.5606 - val_acc: 0.7500\n",
      "Epoch 1223/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3727 - acc: 0.8403 - val_loss: 0.5606 - val_acc: 0.7552\n",
      "Epoch 1224/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3723 - acc: 0.8455 - val_loss: 0.5603 - val_acc: 0.7500\n",
      "Epoch 1225/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3725 - acc: 0.8420 - val_loss: 0.5607 - val_acc: 0.7500\n",
      "Epoch 1226/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3722 - acc: 0.8438 - val_loss: 0.5604 - val_acc: 0.7500\n",
      "Epoch 1227/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3727 - acc: 0.8438 - val_loss: 0.5603 - val_acc: 0.7448\n",
      "Epoch 1228/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3725 - acc: 0.8420 - val_loss: 0.5614 - val_acc: 0.7552\n",
      "Epoch 1229/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3726 - acc: 0.8438 - val_loss: 0.5614 - val_acc: 0.7552\n",
      "Epoch 1230/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3721 - acc: 0.8455 - val_loss: 0.5607 - val_acc: 0.7448\n",
      "Epoch 1231/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3723 - acc: 0.8420 - val_loss: 0.5607 - val_acc: 0.7552\n",
      "Epoch 1232/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3723 - acc: 0.8438 - val_loss: 0.5599 - val_acc: 0.7448\n",
      "Epoch 1233/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3726 - acc: 0.8385 - val_loss: 0.5604 - val_acc: 0.7448\n",
      "Epoch 1234/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3724 - acc: 0.8438 - val_loss: 0.5623 - val_acc: 0.7604\n",
      "Epoch 1235/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3720 - acc: 0.8455 - val_loss: 0.5611 - val_acc: 0.7500\n",
      "Epoch 1236/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3729 - acc: 0.8472 - val_loss: 0.5608 - val_acc: 0.7448\n",
      "Epoch 1237/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3719 - acc: 0.8403 - val_loss: 0.5600 - val_acc: 0.7448\n",
      "Epoch 1238/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3723 - acc: 0.8420 - val_loss: 0.5607 - val_acc: 0.7448\n",
      "Epoch 1239/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3722 - acc: 0.8455 - val_loss: 0.5598 - val_acc: 0.7396\n",
      "Epoch 1240/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3726 - acc: 0.8438 - val_loss: 0.5600 - val_acc: 0.7448\n",
      "Epoch 1241/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3721 - acc: 0.8385 - val_loss: 0.5615 - val_acc: 0.7500\n",
      "Epoch 1242/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3718 - acc: 0.8438 - val_loss: 0.5603 - val_acc: 0.7500\n",
      "Epoch 1243/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3721 - acc: 0.8438 - val_loss: 0.5611 - val_acc: 0.7448\n",
      "Epoch 1244/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3721 - acc: 0.8403 - val_loss: 0.5598 - val_acc: 0.7448\n",
      "Epoch 1245/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3720 - acc: 0.8403 - val_loss: 0.5605 - val_acc: 0.7448\n",
      "Epoch 1246/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.3720 - acc: 0.8403 - val_loss: 0.5603 - val_acc: 0.7448\n",
      "Epoch 1247/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3722 - acc: 0.8403 - val_loss: 0.5613 - val_acc: 0.7500\n",
      "Epoch 1248/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3720 - acc: 0.8490 - val_loss: 0.5595 - val_acc: 0.7396\n",
      "Epoch 1249/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3719 - acc: 0.8403 - val_loss: 0.5601 - val_acc: 0.7500\n",
      "Epoch 1250/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3722 - acc: 0.8420 - val_loss: 0.5599 - val_acc: 0.7396\n",
      "Epoch 1251/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3727 - acc: 0.8420 - val_loss: 0.5611 - val_acc: 0.7448\n",
      "Epoch 1252/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3722 - acc: 0.8403 - val_loss: 0.5615 - val_acc: 0.7448\n",
      "Epoch 1253/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3718 - acc: 0.8438 - val_loss: 0.5620 - val_acc: 0.7552\n",
      "Epoch 1254/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3719 - acc: 0.8403 - val_loss: 0.5614 - val_acc: 0.7500\n",
      "Epoch 1255/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.3716 - acc: 0.8472 - val_loss: 0.5609 - val_acc: 0.7500\n",
      "Epoch 1256/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3721 - acc: 0.8438 - val_loss: 0.5606 - val_acc: 0.7448\n",
      "Epoch 1257/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3722 - acc: 0.8438 - val_loss: 0.5601 - val_acc: 0.7396\n",
      "Epoch 1258/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3721 - acc: 0.8368 - val_loss: 0.5611 - val_acc: 0.7500\n",
      "Epoch 1259/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3720 - acc: 0.8438 - val_loss: 0.5598 - val_acc: 0.7448\n",
      "Epoch 1260/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3720 - acc: 0.8385 - val_loss: 0.5603 - val_acc: 0.7396\n",
      "Epoch 1261/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3717 - acc: 0.8403 - val_loss: 0.5607 - val_acc: 0.7448\n",
      "Epoch 1262/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3718 - acc: 0.8403 - val_loss: 0.5600 - val_acc: 0.7396\n",
      "Epoch 1263/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3720 - acc: 0.8403 - val_loss: 0.5606 - val_acc: 0.7500\n",
      "Epoch 1264/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.3719 - acc: 0.8403 - val_loss: 0.5614 - val_acc: 0.7500\n",
      "Epoch 1265/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3718 - acc: 0.8420 - val_loss: 0.5619 - val_acc: 0.7500\n",
      "Epoch 1266/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3720 - acc: 0.8403 - val_loss: 0.5618 - val_acc: 0.7448\n",
      "Epoch 1267/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.3720 - acc: 0.8438 - val_loss: 0.5618 - val_acc: 0.7500\n",
      "Epoch 1268/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3719 - acc: 0.8438 - val_loss: 0.5600 - val_acc: 0.7500\n",
      "Epoch 1269/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3717 - acc: 0.8420 - val_loss: 0.5601 - val_acc: 0.7448\n",
      "Epoch 1270/1500\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.3720 - acc: 0.8403 - val_loss: 0.5614 - val_acc: 0.7500\n",
      "Epoch 1271/1500\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.3716 - acc: 0.8420 - val_loss: 0.5607 - val_acc: 0.7500\n",
      "Epoch 1272/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3718 - acc: 0.8385 - val_loss: 0.5612 - val_acc: 0.7500\n",
      "Epoch 1273/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3715 - acc: 0.8403 - val_loss: 0.5609 - val_acc: 0.7500\n",
      "Epoch 1274/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3715 - acc: 0.8403 - val_loss: 0.5613 - val_acc: 0.7500\n",
      "Epoch 1275/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3718 - acc: 0.8403 - val_loss: 0.5615 - val_acc: 0.7500\n",
      "Epoch 1276/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3716 - acc: 0.8403 - val_loss: 0.5611 - val_acc: 0.7500\n",
      "Epoch 1277/1500\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.3720 - acc: 0.8420 - val_loss: 0.5608 - val_acc: 0.7396\n",
      "Epoch 1278/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3716 - acc: 0.8403 - val_loss: 0.5615 - val_acc: 0.7396\n",
      "Epoch 1279/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3716 - acc: 0.8403 - val_loss: 0.5614 - val_acc: 0.7396\n",
      "Epoch 1280/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.3716 - acc: 0.8403 - val_loss: 0.5614 - val_acc: 0.7500\n",
      "Epoch 1281/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3714 - acc: 0.8420 - val_loss: 0.5615 - val_acc: 0.7396\n",
      "Epoch 1282/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3718 - acc: 0.8438 - val_loss: 0.5610 - val_acc: 0.7500\n",
      "Epoch 1283/1500\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.3713 - acc: 0.8368 - val_loss: 0.5623 - val_acc: 0.7500\n",
      "Epoch 1284/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3715 - acc: 0.8420 - val_loss: 0.5611 - val_acc: 0.7500\n",
      "Epoch 1285/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3716 - acc: 0.8420 - val_loss: 0.5600 - val_acc: 0.7552\n",
      "Epoch 1286/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3715 - acc: 0.8351 - val_loss: 0.5618 - val_acc: 0.7500\n",
      "Epoch 1287/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3713 - acc: 0.8420 - val_loss: 0.5626 - val_acc: 0.7500\n",
      "Epoch 1288/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3715 - acc: 0.8472 - val_loss: 0.5616 - val_acc: 0.7500\n",
      "Epoch 1289/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3714 - acc: 0.8403 - val_loss: 0.5619 - val_acc: 0.7396\n",
      "Epoch 1290/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3721 - acc: 0.8420 - val_loss: 0.5616 - val_acc: 0.7500\n",
      "Epoch 1291/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3715 - acc: 0.8385 - val_loss: 0.5618 - val_acc: 0.7500\n",
      "Epoch 1292/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3714 - acc: 0.8403 - val_loss: 0.5629 - val_acc: 0.7500\n",
      "Epoch 1293/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3718 - acc: 0.8403 - val_loss: 0.5621 - val_acc: 0.7500\n",
      "Epoch 1294/1500\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.3713 - acc: 0.8420 - val_loss: 0.5623 - val_acc: 0.7500\n",
      "Epoch 1295/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3713 - acc: 0.8403 - val_loss: 0.5626 - val_acc: 0.7500\n",
      "Epoch 1296/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3711 - acc: 0.8403 - val_loss: 0.5626 - val_acc: 0.7500\n",
      "Epoch 1297/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3713 - acc: 0.8403 - val_loss: 0.5623 - val_acc: 0.7448\n",
      "Epoch 1298/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3714 - acc: 0.8420 - val_loss: 0.5616 - val_acc: 0.7448\n",
      "Epoch 1299/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3709 - acc: 0.8403 - val_loss: 0.5619 - val_acc: 0.7396\n",
      "Epoch 1300/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3712 - acc: 0.8403 - val_loss: 0.5623 - val_acc: 0.7448\n",
      "Epoch 1301/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3711 - acc: 0.8420 - val_loss: 0.5627 - val_acc: 0.7500\n",
      "Epoch 1302/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3714 - acc: 0.8438 - val_loss: 0.5611 - val_acc: 0.7604\n",
      "Epoch 1303/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3715 - acc: 0.8385 - val_loss: 0.5622 - val_acc: 0.7552\n",
      "Epoch 1304/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3712 - acc: 0.8385 - val_loss: 0.5624 - val_acc: 0.7448\n",
      "Epoch 1305/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3710 - acc: 0.8403 - val_loss: 0.5627 - val_acc: 0.7448\n",
      "Epoch 1306/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3713 - acc: 0.8403 - val_loss: 0.5629 - val_acc: 0.7396\n",
      "Epoch 1307/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3719 - acc: 0.8403 - val_loss: 0.5635 - val_acc: 0.7396\n",
      "Epoch 1308/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3710 - acc: 0.8403 - val_loss: 0.5643 - val_acc: 0.7552\n",
      "Epoch 1309/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3714 - acc: 0.8455 - val_loss: 0.5634 - val_acc: 0.7396\n",
      "Epoch 1310/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3714 - acc: 0.8438 - val_loss: 0.5623 - val_acc: 0.7500\n",
      "Epoch 1311/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3711 - acc: 0.8403 - val_loss: 0.5621 - val_acc: 0.7500\n",
      "Epoch 1312/1500\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.3711 - acc: 0.8420 - val_loss: 0.5621 - val_acc: 0.7500\n",
      "Epoch 1313/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3713 - acc: 0.8385 - val_loss: 0.5630 - val_acc: 0.7500\n",
      "Epoch 1314/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 53us/step - loss: 0.3711 - acc: 0.8403 - val_loss: 0.5630 - val_acc: 0.7552\n",
      "Epoch 1315/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.3713 - acc: 0.8403 - val_loss: 0.5631 - val_acc: 0.7448\n",
      "Epoch 1316/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3709 - acc: 0.8420 - val_loss: 0.5631 - val_acc: 0.7604\n",
      "Epoch 1317/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3713 - acc: 0.8438 - val_loss: 0.5623 - val_acc: 0.7604\n",
      "Epoch 1318/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3708 - acc: 0.8420 - val_loss: 0.5621 - val_acc: 0.7604\n",
      "Epoch 1319/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3707 - acc: 0.8420 - val_loss: 0.5638 - val_acc: 0.7552\n",
      "Epoch 1320/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3706 - acc: 0.8420 - val_loss: 0.5637 - val_acc: 0.7552\n",
      "Epoch 1321/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3710 - acc: 0.8385 - val_loss: 0.5630 - val_acc: 0.7552\n",
      "Epoch 1322/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3712 - acc: 0.8403 - val_loss: 0.5631 - val_acc: 0.7552\n",
      "Epoch 1323/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3710 - acc: 0.8368 - val_loss: 0.5640 - val_acc: 0.7500\n",
      "Epoch 1324/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3707 - acc: 0.8385 - val_loss: 0.5634 - val_acc: 0.7552\n",
      "Epoch 1325/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3709 - acc: 0.8385 - val_loss: 0.5642 - val_acc: 0.7448\n",
      "Epoch 1326/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3706 - acc: 0.8403 - val_loss: 0.5645 - val_acc: 0.7396\n",
      "Epoch 1327/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.3711 - acc: 0.8385 - val_loss: 0.5649 - val_acc: 0.7396\n",
      "Epoch 1328/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3708 - acc: 0.8403 - val_loss: 0.5646 - val_acc: 0.7500\n",
      "Epoch 1329/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3708 - acc: 0.8420 - val_loss: 0.5633 - val_acc: 0.7500\n",
      "Epoch 1330/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3710 - acc: 0.8385 - val_loss: 0.5633 - val_acc: 0.7604\n",
      "Epoch 1331/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3715 - acc: 0.8385 - val_loss: 0.5631 - val_acc: 0.7604\n",
      "Epoch 1332/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3708 - acc: 0.8420 - val_loss: 0.5631 - val_acc: 0.7552\n",
      "Epoch 1333/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3705 - acc: 0.8368 - val_loss: 0.5634 - val_acc: 0.7604\n",
      "Epoch 1334/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3705 - acc: 0.8403 - val_loss: 0.5629 - val_acc: 0.7552\n",
      "Epoch 1335/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3710 - acc: 0.8351 - val_loss: 0.5634 - val_acc: 0.7552\n",
      "Epoch 1336/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3705 - acc: 0.8368 - val_loss: 0.5643 - val_acc: 0.7552\n",
      "Epoch 1337/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3705 - acc: 0.8420 - val_loss: 0.5630 - val_acc: 0.7500\n",
      "Epoch 1338/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3706 - acc: 0.8403 - val_loss: 0.5626 - val_acc: 0.7552\n",
      "Epoch 1339/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3707 - acc: 0.8385 - val_loss: 0.5620 - val_acc: 0.7552\n",
      "Epoch 1340/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3706 - acc: 0.8403 - val_loss: 0.5623 - val_acc: 0.7500\n",
      "Epoch 1341/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3712 - acc: 0.8403 - val_loss: 0.5622 - val_acc: 0.7500\n",
      "Epoch 1342/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3710 - acc: 0.8368 - val_loss: 0.5627 - val_acc: 0.7552\n",
      "Epoch 1343/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3704 - acc: 0.8385 - val_loss: 0.5639 - val_acc: 0.7552\n",
      "Epoch 1344/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3704 - acc: 0.8403 - val_loss: 0.5631 - val_acc: 0.7604\n",
      "Epoch 1345/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3709 - acc: 0.8385 - val_loss: 0.5637 - val_acc: 0.7604\n",
      "Epoch 1346/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3703 - acc: 0.8420 - val_loss: 0.5626 - val_acc: 0.7604\n",
      "Epoch 1347/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3704 - acc: 0.8403 - val_loss: 0.5648 - val_acc: 0.7552\n",
      "Epoch 1348/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3704 - acc: 0.8455 - val_loss: 0.5629 - val_acc: 0.7500\n",
      "Epoch 1349/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3702 - acc: 0.8403 - val_loss: 0.5646 - val_acc: 0.7552\n",
      "Epoch 1350/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3703 - acc: 0.8420 - val_loss: 0.5632 - val_acc: 0.7500\n",
      "Epoch 1351/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3703 - acc: 0.8368 - val_loss: 0.5641 - val_acc: 0.7552\n",
      "Epoch 1352/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3703 - acc: 0.8385 - val_loss: 0.5642 - val_acc: 0.7604\n",
      "Epoch 1353/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3704 - acc: 0.8368 - val_loss: 0.5653 - val_acc: 0.7552\n",
      "Epoch 1354/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3702 - acc: 0.8385 - val_loss: 0.5648 - val_acc: 0.7500\n",
      "Epoch 1355/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3705 - acc: 0.8385 - val_loss: 0.5638 - val_acc: 0.7552\n",
      "Epoch 1356/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3705 - acc: 0.8403 - val_loss: 0.5626 - val_acc: 0.7500\n",
      "Epoch 1357/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3705 - acc: 0.8333 - val_loss: 0.5645 - val_acc: 0.7552\n",
      "Epoch 1358/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3704 - acc: 0.8403 - val_loss: 0.5647 - val_acc: 0.7552\n",
      "Epoch 1359/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3701 - acc: 0.8403 - val_loss: 0.5643 - val_acc: 0.7500\n",
      "Epoch 1360/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3704 - acc: 0.8385 - val_loss: 0.5646 - val_acc: 0.7552\n",
      "Epoch 1361/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3705 - acc: 0.8368 - val_loss: 0.5648 - val_acc: 0.7552\n",
      "Epoch 1362/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3707 - acc: 0.8351 - val_loss: 0.5651 - val_acc: 0.7552\n",
      "Epoch 1363/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3705 - acc: 0.8368 - val_loss: 0.5646 - val_acc: 0.7552\n",
      "Epoch 1364/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3702 - acc: 0.8420 - val_loss: 0.5635 - val_acc: 0.7552\n",
      "Epoch 1365/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3703 - acc: 0.8351 - val_loss: 0.5638 - val_acc: 0.7448\n",
      "Epoch 1366/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3705 - acc: 0.8385 - val_loss: 0.5634 - val_acc: 0.7500\n",
      "Epoch 1367/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3699 - acc: 0.8385 - val_loss: 0.5634 - val_acc: 0.7552\n",
      "Epoch 1368/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3700 - acc: 0.8351 - val_loss: 0.5648 - val_acc: 0.7552\n",
      "Epoch 1369/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3705 - acc: 0.8368 - val_loss: 0.5653 - val_acc: 0.7552\n",
      "Epoch 1370/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3703 - acc: 0.8403 - val_loss: 0.5645 - val_acc: 0.7604\n",
      "Epoch 1371/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3699 - acc: 0.8368 - val_loss: 0.5651 - val_acc: 0.7552\n",
      "Epoch 1372/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3702 - acc: 0.8420 - val_loss: 0.5652 - val_acc: 0.7604\n",
      "Epoch 1373/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 59us/step - loss: 0.3699 - acc: 0.8385 - val_loss: 0.5638 - val_acc: 0.7552\n",
      "Epoch 1374/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3701 - acc: 0.8368 - val_loss: 0.5654 - val_acc: 0.7552\n",
      "Epoch 1375/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3704 - acc: 0.8385 - val_loss: 0.5646 - val_acc: 0.7552\n",
      "Epoch 1376/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3700 - acc: 0.8403 - val_loss: 0.5651 - val_acc: 0.7552\n",
      "Epoch 1377/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3702 - acc: 0.8385 - val_loss: 0.5646 - val_acc: 0.7552\n",
      "Epoch 1378/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3700 - acc: 0.8368 - val_loss: 0.5644 - val_acc: 0.7500\n",
      "Epoch 1379/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3700 - acc: 0.8368 - val_loss: 0.5645 - val_acc: 0.7552\n",
      "Epoch 1380/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3702 - acc: 0.8368 - val_loss: 0.5654 - val_acc: 0.7500\n",
      "Epoch 1381/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3698 - acc: 0.8385 - val_loss: 0.5660 - val_acc: 0.7552\n",
      "Epoch 1382/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3705 - acc: 0.8403 - val_loss: 0.5648 - val_acc: 0.7552\n",
      "Epoch 1383/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3705 - acc: 0.8385 - val_loss: 0.5652 - val_acc: 0.7552\n",
      "Epoch 1384/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3699 - acc: 0.8403 - val_loss: 0.5651 - val_acc: 0.7552\n",
      "Epoch 1385/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.3699 - acc: 0.8385 - val_loss: 0.5651 - val_acc: 0.7552\n",
      "Epoch 1386/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3701 - acc: 0.8351 - val_loss: 0.5660 - val_acc: 0.7500\n",
      "Epoch 1387/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3701 - acc: 0.8403 - val_loss: 0.5661 - val_acc: 0.7500\n",
      "Epoch 1388/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.3698 - acc: 0.8403 - val_loss: 0.5670 - val_acc: 0.7500\n",
      "Epoch 1389/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3700 - acc: 0.8385 - val_loss: 0.5666 - val_acc: 0.7500\n",
      "Epoch 1390/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3699 - acc: 0.8385 - val_loss: 0.5656 - val_acc: 0.7500\n",
      "Epoch 1391/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3700 - acc: 0.8368 - val_loss: 0.5658 - val_acc: 0.7552\n",
      "Epoch 1392/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3698 - acc: 0.8368 - val_loss: 0.5647 - val_acc: 0.7500\n",
      "Epoch 1393/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3701 - acc: 0.8351 - val_loss: 0.5651 - val_acc: 0.7500\n",
      "Epoch 1394/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3698 - acc: 0.8385 - val_loss: 0.5662 - val_acc: 0.7604\n",
      "Epoch 1395/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3698 - acc: 0.8351 - val_loss: 0.5668 - val_acc: 0.7500\n",
      "Epoch 1396/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3698 - acc: 0.8420 - val_loss: 0.5659 - val_acc: 0.7448\n",
      "Epoch 1397/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3701 - acc: 0.8368 - val_loss: 0.5666 - val_acc: 0.7500\n",
      "Epoch 1398/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3698 - acc: 0.8368 - val_loss: 0.5669 - val_acc: 0.7500\n",
      "Epoch 1399/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3701 - acc: 0.8368 - val_loss: 0.5663 - val_acc: 0.7500\n",
      "Epoch 1400/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3696 - acc: 0.8385 - val_loss: 0.5667 - val_acc: 0.7552\n",
      "Epoch 1401/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3695 - acc: 0.8351 - val_loss: 0.5678 - val_acc: 0.7552\n",
      "Epoch 1402/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3697 - acc: 0.8385 - val_loss: 0.5679 - val_acc: 0.7500\n",
      "Epoch 1403/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3695 - acc: 0.8403 - val_loss: 0.5666 - val_acc: 0.7448\n",
      "Epoch 1404/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3696 - acc: 0.8385 - val_loss: 0.5661 - val_acc: 0.7448\n",
      "Epoch 1405/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3695 - acc: 0.8368 - val_loss: 0.5656 - val_acc: 0.7448\n",
      "Epoch 1406/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3696 - acc: 0.8385 - val_loss: 0.5656 - val_acc: 0.7448\n",
      "Epoch 1407/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3698 - acc: 0.8368 - val_loss: 0.5656 - val_acc: 0.7396\n",
      "Epoch 1408/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3698 - acc: 0.8333 - val_loss: 0.5660 - val_acc: 0.7448\n",
      "Epoch 1409/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3697 - acc: 0.8368 - val_loss: 0.5668 - val_acc: 0.7448\n",
      "Epoch 1410/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3695 - acc: 0.8368 - val_loss: 0.5660 - val_acc: 0.7448\n",
      "Epoch 1411/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3695 - acc: 0.8368 - val_loss: 0.5660 - val_acc: 0.7500\n",
      "Epoch 1412/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3701 - acc: 0.8368 - val_loss: 0.5657 - val_acc: 0.7448\n",
      "Epoch 1413/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3692 - acc: 0.8385 - val_loss: 0.5646 - val_acc: 0.7448\n",
      "Epoch 1414/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3696 - acc: 0.8351 - val_loss: 0.5662 - val_acc: 0.7448\n",
      "Epoch 1415/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3694 - acc: 0.8368 - val_loss: 0.5653 - val_acc: 0.7500\n",
      "Epoch 1416/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3697 - acc: 0.8333 - val_loss: 0.5663 - val_acc: 0.7448\n",
      "Epoch 1417/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3702 - acc: 0.8385 - val_loss: 0.5662 - val_acc: 0.7448\n",
      "Epoch 1418/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3698 - acc: 0.8368 - val_loss: 0.5663 - val_acc: 0.7448\n",
      "Epoch 1419/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3693 - acc: 0.8368 - val_loss: 0.5662 - val_acc: 0.7448\n",
      "Epoch 1420/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3694 - acc: 0.8368 - val_loss: 0.5658 - val_acc: 0.7448\n",
      "Epoch 1421/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3693 - acc: 0.8385 - val_loss: 0.5674 - val_acc: 0.7448\n",
      "Epoch 1422/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3693 - acc: 0.8403 - val_loss: 0.5661 - val_acc: 0.7448\n",
      "Epoch 1423/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3697 - acc: 0.8368 - val_loss: 0.5665 - val_acc: 0.7448\n",
      "Epoch 1424/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3693 - acc: 0.8351 - val_loss: 0.5685 - val_acc: 0.7500\n",
      "Epoch 1425/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3691 - acc: 0.8385 - val_loss: 0.5675 - val_acc: 0.7448\n",
      "Epoch 1426/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.3692 - acc: 0.8368 - val_loss: 0.5680 - val_acc: 0.7500\n",
      "Epoch 1427/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3689 - acc: 0.8385 - val_loss: 0.5673 - val_acc: 0.7448\n",
      "Epoch 1428/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3692 - acc: 0.8368 - val_loss: 0.5676 - val_acc: 0.7448\n",
      "Epoch 1429/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3695 - acc: 0.8368 - val_loss: 0.5661 - val_acc: 0.7500\n",
      "Epoch 1430/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3691 - acc: 0.8368 - val_loss: 0.5676 - val_acc: 0.7448\n",
      "Epoch 1431/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3694 - acc: 0.8368 - val_loss: 0.5673 - val_acc: 0.7448\n",
      "Epoch 1432/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.3694 - acc: 0.8351 - val_loss: 0.5682 - val_acc: 0.7448\n",
      "Epoch 1433/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.3690 - acc: 0.8368 - val_loss: 0.5682 - val_acc: 0.7448\n",
      "Epoch 1434/1500\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.3694 - acc: 0.8385 - val_loss: 0.5686 - val_acc: 0.7448\n",
      "Epoch 1435/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3696 - acc: 0.8385 - val_loss: 0.5675 - val_acc: 0.7448\n",
      "Epoch 1436/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3696 - acc: 0.8368 - val_loss: 0.5670 - val_acc: 0.7448\n",
      "Epoch 1437/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.3691 - acc: 0.8351 - val_loss: 0.5679 - val_acc: 0.7448\n",
      "Epoch 1438/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3691 - acc: 0.8385 - val_loss: 0.5676 - val_acc: 0.7448\n",
      "Epoch 1439/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.3689 - acc: 0.8351 - val_loss: 0.5666 - val_acc: 0.7448\n",
      "Epoch 1440/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.3692 - acc: 0.8368 - val_loss: 0.5669 - val_acc: 0.7448\n",
      "Epoch 1441/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.3691 - acc: 0.8368 - val_loss: 0.5669 - val_acc: 0.7500\n",
      "Epoch 1442/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.3686 - acc: 0.8403 - val_loss: 0.5689 - val_acc: 0.7448\n",
      "Epoch 1443/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3693 - acc: 0.8385 - val_loss: 0.5671 - val_acc: 0.7448\n",
      "Epoch 1444/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.3691 - acc: 0.8351 - val_loss: 0.5688 - val_acc: 0.7448\n",
      "Epoch 1445/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.3691 - acc: 0.8368 - val_loss: 0.5687 - val_acc: 0.7500\n",
      "Epoch 1446/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3690 - acc: 0.8385 - val_loss: 0.5666 - val_acc: 0.7448\n",
      "Epoch 1447/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3690 - acc: 0.8385 - val_loss: 0.5661 - val_acc: 0.7448\n",
      "Epoch 1448/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3688 - acc: 0.8351 - val_loss: 0.5668 - val_acc: 0.7448\n",
      "Epoch 1449/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.3686 - acc: 0.8351 - val_loss: 0.5672 - val_acc: 0.7448\n",
      "Epoch 1450/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3687 - acc: 0.8368 - val_loss: 0.5680 - val_acc: 0.7500\n",
      "Epoch 1451/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3690 - acc: 0.8351 - val_loss: 0.5671 - val_acc: 0.7500\n",
      "Epoch 1452/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3688 - acc: 0.8368 - val_loss: 0.5679 - val_acc: 0.7500\n",
      "Epoch 1453/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3699 - acc: 0.8351 - val_loss: 0.5673 - val_acc: 0.7500\n",
      "Epoch 1454/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.3694 - acc: 0.8368 - val_loss: 0.5690 - val_acc: 0.7448\n",
      "Epoch 1455/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3690 - acc: 0.8351 - val_loss: 0.5697 - val_acc: 0.7500\n",
      "Epoch 1456/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3689 - acc: 0.8351 - val_loss: 0.5694 - val_acc: 0.7500\n",
      "Epoch 1457/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.3692 - acc: 0.8385 - val_loss: 0.5681 - val_acc: 0.7448\n",
      "Epoch 1458/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3689 - acc: 0.8368 - val_loss: 0.5679 - val_acc: 0.7448\n",
      "Epoch 1459/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.3690 - acc: 0.8368 - val_loss: 0.5674 - val_acc: 0.7448\n",
      "Epoch 1460/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5124 - acc: 0.812 - 0s 53us/step - loss: 0.3688 - acc: 0.8368 - val_loss: 0.5680 - val_acc: 0.7448\n",
      "Epoch 1461/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3685 - acc: 0.8368 - val_loss: 0.5688 - val_acc: 0.7448\n",
      "Epoch 1462/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3691 - acc: 0.8368 - val_loss: 0.5686 - val_acc: 0.7500\n",
      "Epoch 1463/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.3688 - acc: 0.8351 - val_loss: 0.5690 - val_acc: 0.7500\n",
      "Epoch 1464/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3686 - acc: 0.8368 - val_loss: 0.5682 - val_acc: 0.7448\n",
      "Epoch 1465/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.3691 - acc: 0.8368 - val_loss: 0.5678 - val_acc: 0.7500\n",
      "Epoch 1466/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3685 - acc: 0.8368 - val_loss: 0.5681 - val_acc: 0.7448\n",
      "Epoch 1467/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3692 - acc: 0.8351 - val_loss: 0.5682 - val_acc: 0.7448\n",
      "Epoch 1468/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.3689 - acc: 0.8385 - val_loss: 0.5673 - val_acc: 0.7448\n",
      "Epoch 1469/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.3692 - acc: 0.8368 - val_loss: 0.5677 - val_acc: 0.7500\n",
      "Epoch 1470/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3687 - acc: 0.8351 - val_loss: 0.5685 - val_acc: 0.7500\n",
      "Epoch 1471/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3684 - acc: 0.8351 - val_loss: 0.5694 - val_acc: 0.7500\n",
      "Epoch 1472/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3684 - acc: 0.8368 - val_loss: 0.5680 - val_acc: 0.7448\n",
      "Epoch 1473/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3688 - acc: 0.8368 - val_loss: 0.5682 - val_acc: 0.7448\n",
      "Epoch 1474/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3688 - acc: 0.8368 - val_loss: 0.5685 - val_acc: 0.7448\n",
      "Epoch 1475/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3690 - acc: 0.8385 - val_loss: 0.5670 - val_acc: 0.7448\n",
      "Epoch 1476/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.3688 - acc: 0.8333 - val_loss: 0.5686 - val_acc: 0.7448\n",
      "Epoch 1477/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.3691 - acc: 0.8368 - val_loss: 0.5688 - val_acc: 0.7448\n",
      "Epoch 1478/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.3686 - acc: 0.8351 - val_loss: 0.5684 - val_acc: 0.7500\n",
      "Epoch 1479/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3684 - acc: 0.8333 - val_loss: 0.5682 - val_acc: 0.7500\n",
      "Epoch 1480/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3686 - acc: 0.8333 - val_loss: 0.5689 - val_acc: 0.7500\n",
      "Epoch 1481/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.3686 - acc: 0.8368 - val_loss: 0.5682 - val_acc: 0.7500\n",
      "Epoch 1482/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.3685 - acc: 0.8333 - val_loss: 0.5689 - val_acc: 0.7500\n",
      "Epoch 1483/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3686 - acc: 0.8351 - val_loss: 0.5683 - val_acc: 0.7500\n",
      "Epoch 1484/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.3692 - acc: 0.8368 - val_loss: 0.5690 - val_acc: 0.7500\n",
      "Epoch 1485/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.3683 - acc: 0.8368 - val_loss: 0.5699 - val_acc: 0.7500\n",
      "Epoch 1486/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.3684 - acc: 0.8368 - val_loss: 0.5690 - val_acc: 0.7448\n",
      "Epoch 1487/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3687 - acc: 0.8403 - val_loss: 0.5687 - val_acc: 0.7500\n",
      "Epoch 1488/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.3685 - acc: 0.8351 - val_loss: 0.5693 - val_acc: 0.7448\n",
      "Epoch 1489/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3688 - acc: 0.8351 - val_loss: 0.5699 - val_acc: 0.7448\n",
      "Epoch 1490/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.3686 - acc: 0.8333 - val_loss: 0.5686 - val_acc: 0.7448\n",
      "Epoch 1491/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 77us/step - loss: 0.3687 - acc: 0.8385 - val_loss: 0.5677 - val_acc: 0.7448\n",
      "Epoch 1492/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.3681 - acc: 0.8385 - val_loss: 0.5702 - val_acc: 0.7448\n",
      "Epoch 1493/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.3682 - acc: 0.8385 - val_loss: 0.5715 - val_acc: 0.7448\n",
      "Epoch 1494/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.3687 - acc: 0.8351 - val_loss: 0.5703 - val_acc: 0.7448\n",
      "Epoch 1495/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.3682 - acc: 0.8368 - val_loss: 0.5696 - val_acc: 0.7448\n",
      "Epoch 1496/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.3685 - acc: 0.8351 - val_loss: 0.5694 - val_acc: 0.7500\n",
      "Epoch 1497/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.3686 - acc: 0.8351 - val_loss: 0.5680 - val_acc: 0.7500\n",
      "Epoch 1498/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.3681 - acc: 0.8385 - val_loss: 0.5691 - val_acc: 0.7500\n",
      "Epoch 1499/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.3686 - acc: 0.8368 - val_loss: 0.5690 - val_acc: 0.7500\n",
      "Epoch 1500/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.3683 - acc: 0.8333 - val_loss: 0.5687 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model_2_hist_run_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the graph\n",
    "y_pred_classes_test2 = model_2.predict_classes(X_test_norm)\n",
    "y_pred_proba_test2 = model_2.predict(X_test_norm)\n",
    "\n",
    "y_pred_classes_train2 = model_2.predict_classes(X_train_norm)\n",
    "y_pred_proba_train2 = model_2.predict(X_train_norm)\n",
    "\n",
    "plot_roc(y_test, y_pred_proba_test2, 'NN2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_train, y_pred_proba_train2, 'NN-Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type your code here to plot the loss accuracy and ROC curve\n",
    "model_2_hist_run_2.history.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f668d79d4a8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(model_2_hist_run_2.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16,7))\n",
    "ax.plot(range(n), model_2_hist_run_2.history['loss'], 'r', marker='.', label='Train Loss')\n",
    "ax.plot(range(n), model_2_hist_run_2.history['val_loss'], 'b', marker='.', label='Validation Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
